{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EEG.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "HwbqZXezfbQU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#utils.py\n",
        "import os\n",
        "import yaml\n",
        "import torch\n",
        "import random\n",
        "import inspect\n",
        "import numpy as np\n",
        "from typing import Dict\n",
        "from pickle import load, dump\n",
        "from functools import partial\n",
        "from fractions import Fraction\n",
        "import torch.nn.functional as F\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "EPSILON = 1e-8\n",
        "half_tensor = None\n",
        "\n",
        "\n",
        "def generate_samples(generator, gen_input):\n",
        "    return generator(gen_input)[0]['x'].data.cpu().numpy()\n",
        "\n",
        "\n",
        "def save_pkl(file_name, obj):\n",
        "    with open(file_name, 'wb') as f:\n",
        "        dump(obj, f, protocol=4)\n",
        "\n",
        "\n",
        "def load_pkl(file_name):\n",
        "    if not os.path.exists(file_name):\n",
        "        return None\n",
        "    with open(file_name, 'rb') as f:\n",
        "        return load(f)\n",
        "\n",
        "\n",
        "def get_half(num_latents, latent_size):\n",
        "    global half_tensor\n",
        "    if half_tensor is None or half_tensor.size() != (num_latents, latent_size):\n",
        "        half_tensor = torch.ones(num_latents, latent_size) * 0.5\n",
        "    return half_tensor\n",
        "\n",
        "\n",
        "def random_latents(num_latents, latent_size, z_distribution='normal'):\n",
        "    if z_distribution == 'normal':\n",
        "        return torch.randn(num_latents, latent_size)\n",
        "    elif z_distribution == 'censored':\n",
        "        return F.relu(torch.randn(num_latents, latent_size))\n",
        "    elif z_distribution == 'bernoulli':\n",
        "        return torch.bernoulli(get_half(num_latents, latent_size))\n",
        "    else:\n",
        "        raise ValueError()\n",
        "\n",
        "\n",
        "def mkdir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "\n",
        "def create_result_subdir(results_dir, experiment_name, dir_pattern='{new_num:03}-{exp_name}'):\n",
        "    if not os.path.exists(results_dir):\n",
        "        os.makedirs(results_dir)\n",
        "    fnames = os.listdir(results_dir)\n",
        "    max_num = max(map(int, filter(lambda x: all(y.isdigit() for y in x), (x.split('-')[0] for x in fnames))), default=0)\n",
        "    path = os.path.join(results_dir, dir_pattern.format(new_num=max_num + 1, exp_name=experiment_name))\n",
        "    os.makedirs(path, exist_ok=False)\n",
        "    return path\n",
        "\n",
        "\n",
        "def num_params(net):\n",
        "    model_parameters = trainable_params(net)\n",
        "    return sum([np.prod(p.size()) for p in model_parameters])\n",
        "\n",
        "\n",
        "def generic_arg_parse(x, hinttype=None):\n",
        "    if hinttype is int or hinttype is float or hinttype is str:\n",
        "        return hinttype(x)\n",
        "    try:\n",
        "        for _ in range(2):\n",
        "            x = x.strip('\\'').strip(\"\\\"\")\n",
        "        __special_tmp = eval(x, {}, {})\n",
        "    except:  # the string contained some name - probably path, treat as string\n",
        "        __special_tmp = x  # treat as string\n",
        "    return __special_tmp\n",
        "\n",
        "\n",
        "def create_params(classes, excludes=None, overrides=None):\n",
        "    params = {}\n",
        "    if not excludes:\n",
        "        excludes = {}\n",
        "    if not overrides:\n",
        "        overrides = {}\n",
        "    for cls in classes:\n",
        "        nm = cls.__name__\n",
        "        params[nm] = {\n",
        "            k: (v.default if nm not in overrides or k not in overrides[nm] else overrides[nm][k])\n",
        "            for k, v in dict(inspect.signature(cls.__init__).parameters).items()\n",
        "            if v.default != inspect._empty and (nm not in excludes or k not in excludes[nm])\n",
        "        }\n",
        "    return params\n",
        "\n",
        "\n",
        "def get_structured_params(params):\n",
        "    new_params = {}\n",
        "    for p in params:\n",
        "        if '.' in p:\n",
        "            [cls, attr] = p.split('.', 1)\n",
        "            if cls not in new_params:\n",
        "                new_params[cls] = {}\n",
        "            new_params[cls][attr] = params[p]\n",
        "        else:\n",
        "            new_params[p] = params[p]\n",
        "    return new_params\n",
        "\n",
        "\n",
        "def cudize(thing):\n",
        "    if thing is None:\n",
        "        return None\n",
        "    has_cuda = torch.cuda.is_available()\n",
        "    if not has_cuda:\n",
        "        return thing\n",
        "    if isinstance(thing, (list, tuple)):\n",
        "        return [item.cuda() for item in thing]\n",
        "    if isinstance(thing, dict):\n",
        "        return {k: v.cuda() for k, v in thing.items()}\n",
        "    return thing.cuda()\n",
        "\n",
        "\n",
        "def trainable_params(model):\n",
        "    return filter(lambda p: p.requires_grad, model.parameters())\n",
        "\n",
        "\n",
        "def pixel_norm(h):\n",
        "    mean = torch.mean(h * h, dim=1, keepdim=True)\n",
        "    dom = torch.rsqrt(mean + EPSILON)\n",
        "    return h * dom\n",
        "\n",
        "\n",
        "def simple_argparser(default_params):\n",
        "    parser = ArgumentParser()\n",
        "    for k in default_params:\n",
        "        parser.add_argument('--{}'.format(k), type=partial(generic_arg_parse, hinttype=type(default_params[k])))\n",
        "    parser.set_defaults(**default_params)\n",
        "    return get_structured_params(vars(parser.parse_args()))\n",
        "\n",
        "\n",
        "def enable_benchmark():\n",
        "    torch.backends.cudnn.benchmark = True  # for fast training(if network input size is almost constant)\n",
        "\n",
        "\n",
        "def load_model(model_path, return_all=False):\n",
        "    state = torch.load(model_path, map_location='cpu')\n",
        "    if not return_all:\n",
        "        return state['model']\n",
        "    return state['model'], state['optimizer'], state['cur_nimg']\n",
        "\n",
        "\n",
        "def parse_config(default_params, need_arg_classes, exclude_adam=True, read_cli=True):\n",
        "    parser = ArgumentParser()\n",
        "    if exclude_adam:\n",
        "        excludes = {'Adam': {'lr', 'amsgrad'}}\n",
        "        default_overrides = {'Adam': {'betas': (0.0, 0.99)}}\n",
        "        auto_args = create_params(need_arg_classes, excludes, default_overrides)\n",
        "    else:\n",
        "        auto_args = create_params(need_arg_classes)\n",
        "    for k in default_params:\n",
        "        parser.add_argument('--{}'.format(k), type=partial(generic_arg_parse, hinttype=type(default_params[k])))\n",
        "    for cls in auto_args:\n",
        "        group = parser.add_argument_group(cls, 'Arguments for initialization of class {}'.format(cls))\n",
        "        for k in auto_args[cls]:\n",
        "            name = '{}.{}'.format(cls, k)\n",
        "            group.add_argument('--{}'.format(name), type=generic_arg_parse)\n",
        "            default_params[name] = auto_args[cls][k]\n",
        "    parser.set_defaults(**default_params)\n",
        "    if read_cli:\n",
        "        params = vars(parser.parse_args())\n",
        "    else:\n",
        "        params = default_params\n",
        "    if params['config_file']:\n",
        "        print('loading config_file')\n",
        "        with open(params['config_file']) as f:\n",
        "            params = _update_params(params, yaml.load(f))\n",
        "    params = get_structured_params(params)\n",
        "    random.seed(params['random_seed'])\n",
        "    np.random.seed(params['random_seed'])\n",
        "    torch.manual_seed(params['random_seed'])\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.set_device(params['cuda_device'])\n",
        "        torch.cuda.manual_seed_all(params['random_seed'])\n",
        "        enable_benchmark()\n",
        "    return params\n",
        "\n",
        "\n",
        "def _update_params(params: Dict, given_conf: Dict):\n",
        "    for k, v in given_conf.items():\n",
        "        if isinstance(v, dict):\n",
        "            for kk, vv in v.items():\n",
        "                params['{}.{}'.format(k, kk)] = vv\n",
        "        else:\n",
        "            params[k] = v\n",
        "    return params\n",
        "\n",
        "\n",
        "def upsample_signal(signal, upsample_factor):\n",
        "    return F.interpolate(signal, scale_factor=upsample_factor, mode='linear', align_corners=False)\n",
        "\n",
        "\n",
        "def downsample_signal(signal, downsample_factor):\n",
        "    return F.avg_pool1d(signal, downsample_factor, downsample_factor, 0, False, True)\n",
        "\n",
        "\n",
        "def expand3d(signal):\n",
        "    orig_dim = signal.dim()\n",
        "    if orig_dim == 2:\n",
        "        return signal[None]\n",
        "    if orig_dim == 1:\n",
        "        return signal[None, None]\n",
        "    return signal\n",
        "\n",
        "\n",
        "def resample_signal(signal, signal_freq, desired_freq, pytorch=False):\n",
        "    if isinstance(signal, np.ndarray):\n",
        "        new_signal = torch.from_numpy(signal)\n",
        "    else:\n",
        "        new_signal = signal\n",
        "    orig_dim = new_signal.dim()\n",
        "    if orig_dim == 2:\n",
        "        new_signal = new_signal[None]\n",
        "    if orig_dim == 1:\n",
        "        new_signal = new_signal[None, None]\n",
        "    if pytorch and (orig_dim < 3 or signal.shape[2] == 1):\n",
        "        ratio = desired_freq / signal_freq\n",
        "        assert ratio == int(ratio)\n",
        "        return new_signal.repeat(1, 1, int(ratio))\n",
        "    if isinstance(desired_freq, float):\n",
        "        if desired_freq == int(desired_freq) and signal_freq == int(signal_freq):\n",
        "            desired_freq = int(desired_freq)\n",
        "            signal_freq = int(signal_freq)\n",
        "        else:\n",
        "            desired_freq = desired_freq / signal_freq\n",
        "            signal_freq = None\n",
        "    ratio = Fraction(desired_freq, signal_freq)\n",
        "    if ratio.numerator != 1:\n",
        "        new_signal = upsample_signal(new_signal, ratio.numerator)\n",
        "    if ratio.denominator != 1:\n",
        "        new_signal = downsample_signal(new_signal, ratio.denominator)\n",
        "    if not pytorch:\n",
        "        if orig_dim == 2:\n",
        "            return new_signal[0].numpy()\n",
        "        return new_signal[0, 0].numpy()\n",
        "    return new_signal\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uIZvF_9-fwUL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#dataset.py\n",
        "import os\n",
        "import glob\n",
        "from random import shuffle\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from scipy.io import loadmat\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataloader import default_collate\n",
        "\n",
        "DATASET_VERSION = 6\n",
        "\n",
        "'''\n",
        "!pip install gdown\n",
        "!gdown --id 1rVey-8ZN1mAYcCISie5kGuZ_2WNWZ8Jb\n",
        "!gdown --id 1QMKt1c6v-Pa6sGXDia4x_OwNkSNSocCW\n",
        "!gdown --id 1vMVSmtdUzIt7MfDnNzB5CO8u8Xl19k5s\n",
        "!gdown --id 1GpBX7pkmUHu6JP63WH9FcvHp9Wxz7ZUi\n",
        "'''\n",
        "\n",
        "\n",
        "# bio_sampling_freq: 1 -> 4 -> 8 -> 16 -> 24 -> 32 -> 40 -> 60 -> 100\n",
        "class EEGDataset(Dataset):\n",
        "    # for 60(sampling), starting from 1 hz(sampling) [32 samples at the beginning]\n",
        "    progression_scale_up = [4, 2, 2, 3, 4, 5, 3]\n",
        "    progression_scale_down = [1, 1, 1, 2, 3, 4, 2]\n",
        "\n",
        "    # for 60(sampling), starting from 0.25 hz(sampling) [8 samples at the beginning]\n",
        "    # progression_scale_up   = [2, 2] + progression_scale_up\n",
        "    # progression_scale_down = [1, 1] + progression_scale_down\n",
        "\n",
        "    # for 100(sampling), starting from 1 hz(sampling) [32 samples at the beginning]\n",
        "    # progression_scale_up   = progression_scale_up + [5]\n",
        "    # progression_scale_down = progression_scale_down + [3]\n",
        "\n",
        "    # for 100(sampling), starting from 0.25 hz(sampling) [8 samples at the beginning]\n",
        "    # progression_scale_up   = [2, 2] + progression_scale_up + [5]\n",
        "    # progression_scale_down = [1, 1] + progression_scale_down + [3]\n",
        "\n",
        "    picked_channels = None\n",
        "\n",
        "    # picked_channels = [3, 5, 9, 15, 16]\n",
        "    # picked_channels = [3, 5, 9, 12, 13, 14, 15, 16]\n",
        "    # picked_channels = [3, 5, 8, 9, 10, 13, 15, 16]\n",
        "\n",
        "    def __init__(self, train_files, norms, given_data, validation_ratio: float = 0.1, dir_path: str = './',\n",
        "                 data_sampling_freq: float = 80, start_sampling_freq: float = 1, end_sampling_freq: float = 60,\n",
        "                 start_seq_len: int = 32, stride: float = 0.5, num_channels: int = 5, number_of_files: int = 100000,\n",
        "                 per_user_normalization: bool = True, per_channel_normalization: bool = False):\n",
        "        super().__init__()\n",
        "        self.model_depth = 0\n",
        "        self.alpha = 1.0\n",
        "        self.dir_path = dir_path\n",
        "        self.end_sampling_freq = end_sampling_freq\n",
        "        seq_len = start_seq_len * end_sampling_freq / start_sampling_freq\n",
        "        assert seq_len == int(seq_len), 'seq_len must be an int'\n",
        "        seq_len = int(seq_len)\n",
        "        self.seq_len = seq_len\n",
        "        self.initial_kernel_size = start_seq_len\n",
        "        self.stride = int(seq_len * stride)\n",
        "        self.per_user_normalization = per_user_normalization\n",
        "        self.per_channel_normalization = per_channel_normalization\n",
        "        self.max_dataset_depth = len(self.progression_scale_up)\n",
        "        self.norms = norms\n",
        "        self.num_channels = num_channels if self.picked_channels is None else len(self.picked_channels)\n",
        "        if given_data is not None:\n",
        "            self.sizes = given_data[0]['sizes']\n",
        "            self.files = given_data[0]['files']\n",
        "            self.norms = given_data[0]['norms']\n",
        "            self.data_pointers = given_data[0]['pointers']\n",
        "            self.datas = [given_data[1]['arr_{}'.format(i)] for i in range(len(given_data[1].keys()))]\n",
        "            return\n",
        "        all_files = glob.glob(os.path.join(dir_path, '*_1.txt'))[:number_of_files]\n",
        "        is_matlab = len(all_files) == 0\n",
        "        if is_matlab:\n",
        "            all_files = glob.glob(os.path.join(dir_path, '*.mat'))[:number_of_files]\n",
        "        files = len(all_files)\n",
        "        files = [i for i in range(files)]\n",
        "        if train_files is None:\n",
        "            shuffle(files)\n",
        "            files = files[:int(len(all_files) * (1.0 - validation_ratio))]\n",
        "        else:\n",
        "            files = list(set(files) - set(train_files))\n",
        "        self.files = files\n",
        "        sizes = []\n",
        "        num_points = []\n",
        "        self.datas = []\n",
        "        for i in tqdm(files):\n",
        "            is_ok = True\n",
        "            if is_matlab:\n",
        "                try:\n",
        "                    tmp = loadmat(all_files[i])['eeg_signal']\n",
        "                    tmp = resample_signal(tmp, data_sampling_freq, end_sampling_freq)\n",
        "                    size = int(np.ceil((tmp.shape[1] - seq_len + 1) / self.stride))\n",
        "                except:\n",
        "                    size = 0\n",
        "                if size <= 0:\n",
        "                    is_ok = False\n",
        "                else:\n",
        "                    sizes.append(size)\n",
        "                    num_points.append((sizes[-1] - 1) * self.stride + seq_len)\n",
        "                    if self.picked_channels is None:\n",
        "                        self.datas.append(tmp[:num_channels, :num_points[-1]])\n",
        "                    else:\n",
        "                        self.datas.append(tmp[self.picked_channels, :num_points[-1]])\n",
        "            else:\n",
        "                for_range = range(num_channels) if self.picked_channels is None else self.picked_channels\n",
        "                for kk, j in enumerate(for_range):\n",
        "                    with open('{}_{}.txt'.format(all_files[i][:-6], j + 1)) as f:\n",
        "                        tmp = list(map(float, f.read().split()))\n",
        "                        tmp = np.array(tmp, dtype=np.float32)\n",
        "                        tmp = resample_signal(tmp, data_sampling_freq, end_sampling_freq)\n",
        "                        if kk == 0:\n",
        "                            size = int(np.ceil((len(tmp) - seq_len + 1) / self.stride))\n",
        "                            if size <= 0:\n",
        "                                is_ok = False\n",
        "                                break\n",
        "                            sizes.append(size)\n",
        "                            num_points.append((sizes[-1] - 1) * self.stride + seq_len)\n",
        "                            self.datas.append(np.zeros((num_channels, num_points[-1]), dtype=np.float32))\n",
        "                        tmp = tmp[:num_points[-1]]\n",
        "                        self.datas[-1][j, :] = tmp\n",
        "            if is_ok and per_user_normalization:\n",
        "                self.datas[-1], is_ok = self.normalize(self.datas[-1], self.per_channel_normalization)\n",
        "                if not is_ok:\n",
        "                    del sizes[-1]\n",
        "                    del num_points[-1]\n",
        "                    del self.datas[-1]\n",
        "        self.sizes = sizes\n",
        "        self.data_pointers = [(i, j) for i, s in enumerate(self.sizes) for j in range(s)]\n",
        "        if not per_user_normalization:\n",
        "            self.normalize_all()\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, validation_ratio: float, dir_path: str, number_of_files: int,\n",
        "                    data_sampling_freq: float, start_sampling_freq: float, end_sampling_freq: float,\n",
        "                    start_seq_len: int, stride: float, num_channels: int,\n",
        "                    per_user_normalization: bool, per_channel_normalization: bool):\n",
        "        assert end_sampling_freq <= data_sampling_freq\n",
        "        mode = per_user_normalization * 2 + per_channel_normalization * 1\n",
        "        train_files = None\n",
        "        train_norms = None\n",
        "        datasets = [None, None]\n",
        "        for index, split in enumerate(('train', 'val')):\n",
        "            target_location = os.path.join(dir_path, '{}%_{}c_{}m_{}s_{}v_{}ss_{}es_{}l_{}n_{}.npz'\n",
        "                                           .format(validation_ratio, num_channels, mode, stride,\n",
        "                                                   DATASET_VERSION, start_sampling_freq, end_sampling_freq,\n",
        "                                                   start_seq_len, number_of_files, split))\n",
        "            given_data = None\n",
        "            if os.path.exists(target_location):\n",
        "                print('loading {} dataset from file'.format(split))\n",
        "                if split == 'val' and validation_ratio == 0.0:\n",
        "                    print('creating {} dataset from scratch'.format(split))\n",
        "                else:\n",
        "                    given_data = (load_pkl(target_location + '.pkl'), np.load(target_location))\n",
        "            else:\n",
        "                print('creating {} dataset from scratch'.format(split))\n",
        "            dataset = cls(train_files, train_norms, given_data, validation_ratio, dir_path, data_sampling_freq,\n",
        "                          start_sampling_freq, end_sampling_freq, start_seq_len, stride, num_channels,\n",
        "                          number_of_files, per_user_normalization, per_channel_normalization)\n",
        "            if train_files is None:\n",
        "                train_files = dataset.files\n",
        "                train_norms = dataset.norms\n",
        "            if given_data is None:\n",
        "                np.savez_compressed(target_location, *dataset.datas)\n",
        "                save_pkl(target_location + '.pkl', {'sizes': dataset.sizes, 'pointers': dataset.data_pointers,\n",
        "                                                    'norms': dataset.norms, 'files': dataset.files})\n",
        "            datasets[index] = dataset\n",
        "        return datasets[0], datasets[1]\n",
        "\n",
        "    def normalize_all(self):\n",
        "        num_files = len(self.datas)\n",
        "        if self.norms is None:\n",
        "            all_max = np.max(\n",
        "                np.array([data.max(axis=1 if self.per_channel_normalization else None) for data in self.datas]), axis=0)\n",
        "            all_min = np.min(\n",
        "                np.array([data.min(axis=1 if self.per_channel_normalization else None) for data in self.datas]), axis=0)\n",
        "            self.norms = (all_max, all_min)\n",
        "        else:\n",
        "            all_max, all_min = self.norms\n",
        "        is_ok = True\n",
        "        for i in range(num_files):\n",
        "            self.datas[i], is_ok = self.normalize(self.datas[i], self.per_channel_normalization, all_max, all_min)\n",
        "        if not is_ok:\n",
        "            raise ValueError('data is constant!')\n",
        "\n",
        "    @staticmethod\n",
        "    def normalize(arr, per_channel, arr_max=None, arr_min=None):\n",
        "        if arr_max is None:\n",
        "            arr_max = arr.max(axis=1 if per_channel else None)\n",
        "        if arr_min is None:\n",
        "            arr_min = arr.min(axis=1 if per_channel else None)\n",
        "        is_ok = arr_max != arr_min\n",
        "        if per_channel:\n",
        "            is_ok = is_ok.all()\n",
        "        return ((arr - arr_min) / ((arr_max - arr_min) if is_ok else 1.0)) * 2.0 - 1.0, is_ok\n",
        "\n",
        "    @property\n",
        "    def shape(self):\n",
        "        return len(self), self.num_channels, self.seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_pointers)\n",
        "\n",
        "    def load_file(self, item):\n",
        "        i, k = self.data_pointers[item]\n",
        "        res = self.datas[i][:, k * self.stride:k * self.stride + self.seq_len]\n",
        "        return res\n",
        "\n",
        "    def resample_data(self, data, index, forward=True, alpha_fade=False):\n",
        "        up_scale = self.progression_scale_up[index - (1 if alpha_fade else 0)]\n",
        "        down_scale = self.progression_scale_down[index - (1 if alpha_fade else 0)]\n",
        "        if forward:\n",
        "            return resample_signal(data, down_scale, up_scale, True)\n",
        "        return resample_signal(data, up_scale, down_scale, True)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        with torch.no_grad():\n",
        "            datapoint = torch.from_numpy(self.load_file(item).astype(np.float32)).unsqueeze(0)\n",
        "            target_depth = self.model_depth\n",
        "            if self.max_dataset_depth != target_depth:\n",
        "                datapoint = self.create_datapoint_from_depth(datapoint, target_depth)\n",
        "        return {'x': self.alpha_fade(datapoint).squeeze(0)}\n",
        "\n",
        "    def create_datapoint_from_depth(self, datapoint, target_depth):\n",
        "        depth_diff = (self.max_dataset_depth - target_depth)\n",
        "        for index in reversed(list(range(len(self.progression_scale_up)))[-depth_diff:]):\n",
        "            datapoint = self.resample_data(datapoint, index, False)\n",
        "        return datapoint\n",
        "\n",
        "    def alpha_fade(self, datapoint):\n",
        "        if self.alpha == 1:\n",
        "            return datapoint\n",
        "        t = self.resample_data(datapoint, self.model_depth, False, alpha_fade=True)\n",
        "        t = self.resample_data(t, self.model_depth, True, alpha_fade=True)\n",
        "        return datapoint + (t - datapoint) * (1 - self.alpha)\n",
        "\n",
        "\n",
        "def get_collate_real(max_sampling_freq, max_len):\n",
        "    def collate_real(batch):\n",
        "        return cudize(default_collate(batch))\n",
        "\n",
        "    return collate_real\n",
        "\n",
        "\n",
        "def get_collate_fake(latent_size, z_distribution, collate_real):\n",
        "    def collate_fake(batch):\n",
        "        batch = collate_real(batch)  # extract condition(features)\n",
        "        batch['z'] = random_latents(batch['x'].size(0), latent_size, z_distribution)\n",
        "        del batch['x']\n",
        "        return batch\n",
        "\n",
        "    return collate_fake\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yV4pcrAAf_aE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#layers.py\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.nn.init import calculate_gain\n",
        "from torch.nn.utils import spectral_norm\n",
        "\n",
        "\n",
        "class PixelNorm(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return pixel_norm(x)\n",
        "\n",
        "\n",
        "class ScaledTanh(nn.Tanh):\n",
        "    def __init__(self, scale=0.5):\n",
        "        super().__init__()\n",
        "        self.scale = scale\n",
        "\n",
        "    def forward(self, x):\n",
        "        return super().forward(x * self.scale)\n",
        "\n",
        "\n",
        "class GDropLayer(nn.Module):\n",
        "    def __init__(self, strength=0.2, axes=(0, 1)):\n",
        "        super().__init__()\n",
        "        self.strength = strength\n",
        "        self.axes = [axes] if isinstance(axes, int) else list(axes)\n",
        "\n",
        "    def forward(self, x, deterministic=False):\n",
        "        if deterministic or not self.strength:\n",
        "            return x\n",
        "        rnd_shape = [s if axis in self.axes else 1 for axis, s in enumerate(x.size())]\n",
        "        rnd = (1 + self.strength) ** np.random.normal(size=rnd_shape)\n",
        "        rnd = torch.from_numpy(rnd).type(x.data.type()).to(x)\n",
        "        return x * rnd\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, channels_in, spectral, init='xavier_uniform'):\n",
        "        super().__init__()\n",
        "        d_key = max(channels_in // 8, 2)\n",
        "        conv_conf = dict(kernel_size=1, equalized=False, spectral=spectral,\n",
        "                         init=init, bias=False, act_alpha=-1)\n",
        "        self.gamma = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
        "        self.pooling = nn.MaxPool1d(4)\n",
        "        self.key_conv = GeneralConv(channels_in, d_key, **conv_conf)\n",
        "        self.query_conv = GeneralConv(channels_in, d_key, **conv_conf)\n",
        "        self.value_conv = GeneralConv(channels_in, channels_in // 2, **conv_conf)\n",
        "        self.final_conv = GeneralConv(channels_in // 2, channels_in, **conv_conf)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.scale = 1.0\n",
        "\n",
        "    def forward(self, x):  # BCT\n",
        "        query = self.query_conv(x)  # BC/8T\n",
        "        key = self.pooling(self.key_conv(x))  # BC/8T[/4]\n",
        "        value = self.pooling(self.value_conv(x))  # BC/2T[/4]\n",
        "        out = F.softmax(torch.bmm(key.permute(0, 2, 1), query) / self.scale, dim=1)  # Bnormed(T[/4])T\n",
        "        attention_map = out\n",
        "        out = torch.bmm(value, out)  # BC/2T\n",
        "        out = self.final_conv(out)  # BCT\n",
        "        return self.gamma * out + x, attention_map\n",
        "\n",
        "\n",
        "class MinibatchStddev(nn.Module):\n",
        "    def __init__(self, group_size=4, temporal_groups_per_window=1, kernel_size=32):\n",
        "        super().__init__()\n",
        "        self.group_size = group_size if group_size != 0 else 1e6\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride_size = self.kernel_size // temporal_groups_per_window\n",
        "\n",
        "    def forward(self, x):  # B, C, T\n",
        "        s = x.size()\n",
        "        group_size = min(s[0], self.group_size)\n",
        "        all_y = []\n",
        "        for i in range(s[2] // self.stride_size):\n",
        "            y = x[..., i * self.stride_size:(i + 1) * self.stride_size]\n",
        "            y = y.view(group_size, -1, s[1], self.stride_size)  # G,B//G,C,T\n",
        "            y = y - y.mean(dim=0, keepdim=True)  # G,B//G,C,T\n",
        "            y = torch.sqrt((y ** 2).mean(dim=0))  # B//G,C,T\n",
        "            y = y.mean(dim=1, keepdim=True).mean(dim=2, keepdim=True)  # B//G,1,1\n",
        "            y = y.repeat((group_size, 1, self.stride_size))  # B,1,T\n",
        "            all_y.append(y)\n",
        "        return torch.cat([x, torch.cat(all_y, dim=2)], dim=1)\n",
        "\n",
        "\n",
        "class ConditionalBatchNorm(nn.Module):\n",
        "    def __init__(self, num_features, num_classes, latent_size, spectral):\n",
        "        super().__init__()\n",
        "        no_cond = latent_size == 0 and num_classes == 0\n",
        "        self.num_features = num_features\n",
        "        self.normalizer = nn.BatchNorm1d(num_features, affine=no_cond)\n",
        "        if no_cond:\n",
        "            self.mode = 'BN'  # batch norm\n",
        "        elif latent_size == 0:\n",
        "            self.embed = GeneralConv(num_classes, num_features * 2, kernel_size=1, equalized=False,\n",
        "                                     act_alpha=-1, spectral=spectral, bias=False)\n",
        "            self.mode = 'CBN'  # conditional batch norm\n",
        "        else:  # both 'SM'(self modulation) and 'CSM'(conditional self modulation)\n",
        "            # NOTE maybe reduce it to a single layer linear network?\n",
        "            self.embed = nn.Sequential(\n",
        "                GeneralConv(latent_size + num_classes, num_features * 2, kernel_size=1,\n",
        "                            equalized=False, act_alpha=0.0, spectral=spectral, bias=True),\n",
        "                GeneralConv(num_features * 2, num_features * 2, kernel_size=1,\n",
        "                            equalized=False, act_alpha=-1, spectral=spectral, bias=False))\n",
        "            if num_classes == 0:\n",
        "                self.mode = 'SM'  # self modulation\n",
        "            else:\n",
        "                self.mode = 'CSM'  # conditional self modulation(biggan)\n",
        "\n",
        "    def forward(self, x, y, z):  # y = B*num_classes*Ty ; x = B*num_features*Tx ; z = B*latent_size\n",
        "        out = self.normalizer(x)\n",
        "        if self.mode == 'BN':\n",
        "            return out\n",
        "        if y is not None and y.ndimension() == 2:\n",
        "            y = y.unsqueeze(2)\n",
        "        if self.mode == 'CBN':\n",
        "            cond = y\n",
        "        else:\n",
        "            if self.mode == 'CSM':\n",
        "                z = expand3d(z)\n",
        "                cond = torch.cat([resample_signal(z, z.size(2), y.size(2), pytorch=True), y], dim=1)\n",
        "            else:\n",
        "                cond = expand3d(z)\n",
        "        embed = self.embed(cond)  # B, num_features*2, Ty\n",
        "        embed = resample_signal(embed, embed.shape[2], out.shape[2], pytorch=True)\n",
        "        gamma, beta = embed.chunk(2, dim=1)\n",
        "        return out + gamma * out + beta  # trick to make sure gamma is 1.0 at the beginning of the training\n",
        "\n",
        "\n",
        "class EqualizedSeparableConv1d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size,\n",
        "                 padding, spectral, equalized, init, act_alpha, bias, groups, stride):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            EqualizedConv1d(in_channels, in_channels, kernel_size, padding,\n",
        "                            spectral, equalized, init, -1, True, groups=in_channels, stride=stride),\n",
        "            EqualizedConv1d(in_channels, out_channels, 1, 0, spectral, equalized, init, act_alpha, bias, groups, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class EqualizedConv1d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, padding,\n",
        "                 spectral, equalized, init, act_alpha, bias, groups, stride):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv1d(in_channels=in_channels, out_channels=out_channels, stride=stride,\n",
        "                              kernel_size=kernel_size, padding=padding, bias=bias, groups=groups)\n",
        "        if bias:\n",
        "            self.conv.bias.data.zero_()\n",
        "        act_alpha = act_alpha if act_alpha > 0 else 1\n",
        "        if init == 'kaiming_normal':\n",
        "            torch.nn.init.kaiming_normal_(self.conv.weight, a=act_alpha)\n",
        "        elif init == 'xavier_uniform':\n",
        "            torch.nn.init.xavier_uniform_(self.conv.weight, gain=calculate_gain('leaky_relu', param=act_alpha))\n",
        "        elif init == 'orthogonal':\n",
        "            torch.nn.init.orthogonal_(self.conv.weight, gain=calculate_gain('leaky_relu', param=act_alpha))\n",
        "        if not equalized:\n",
        "            self.scale = 1.0\n",
        "        else:\n",
        "            self.scale = ((torch.mean(self.conv.weight.data ** 2)) ** 0.5).item()\n",
        "        self.conv.weight.data.copy_(self.conv.weight.data / self.scale)\n",
        "        if spectral:\n",
        "            self.conv = spectral_norm(self.conv)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x * self.scale)\n",
        "\n",
        "\n",
        "class GeneralConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, z_to_bn_size=0, kernel_size=3, equalized=True,\n",
        "                 pad=None, act_alpha=0.2, do=0, num_classes=0, act_norm=None, spectral=False,\n",
        "                 init='kaiming_normal', bias=True, separable=False, stride=1):\n",
        "        super().__init__()\n",
        "        pad = (kernel_size - 1) // 2 if pad is None else pad\n",
        "        if separable:\n",
        "            conv_class = EqualizedSeparableConv1d\n",
        "        else:\n",
        "            conv_class = EqualizedConv1d\n",
        "        conv = conv_class(in_channels, out_channels, kernel_size, padding=pad, spectral=spectral,\n",
        "                          equalized=equalized, init=init, act_alpha=act_alpha,\n",
        "                          bias=bias if act_norm != 'batch' else False, groups=1, stride=stride)\n",
        "        norm = None\n",
        "        if act_norm == 'batch':\n",
        "            norm = ConditionalBatchNorm(out_channels, num_classes, z_to_bn_size, spectral)\n",
        "        self.conv = conv\n",
        "        self.norm = norm\n",
        "        self.net = []\n",
        "        if act_alpha >= 0:\n",
        "            if act_alpha == 0:\n",
        "                self.net.append(nn.ReLU())  # DO NOT use inplace, gradient penalty will break\n",
        "            else:\n",
        "                self.net.append(nn.LeakyReLU(act_alpha))  # DO NOT use inplace, gradient penalty will break\n",
        "        if act_norm == 'pixel':\n",
        "            self.net.append(PixelNorm())\n",
        "        if do != 0:\n",
        "            self.net.append(GDropLayer(strength=do))\n",
        "        self.net = nn.Sequential(*self.net)\n",
        "\n",
        "    def forward(self, x, y=None, z=None, conv_noise=None):\n",
        "        c = self.conv(x)\n",
        "        if conv_noise is not None:\n",
        "            c = c * conv_noise\n",
        "        if self.norm:\n",
        "            c = self.norm(c, y, z)\n",
        "        return self.net(c)\n",
        "\n",
        "\n",
        "class PassChannelResidual(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        if x.size(1) >= y.size(1):\n",
        "            x[:, :y.size(1)] = x[:, :y.size(1)] + y\n",
        "            return x\n",
        "        return y[:, :x.size(1)] + x\n",
        "\n",
        "\n",
        "class ConcatResidual(nn.Module):\n",
        "    def __init__(self, ch_in, ch_out, equalized, spectral, init):\n",
        "        super().__init__()\n",
        "        assert ch_out >= ch_in\n",
        "        if ch_out > ch_in:\n",
        "            self.net = GeneralConv(ch_in, ch_out - ch_in, kernel_size=1, equalized=equalized, act_alpha=-1,\n",
        "                                   spectral=spectral, init=init)\n",
        "        else:\n",
        "            self.net = None\n",
        "\n",
        "    def forward(self, h, x):\n",
        "        if self.net:\n",
        "            return h + torch.cat([x, self.net(x)], dim=1)\n",
        "        return h + x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YgbPgo6jgV2U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#losses.py\n",
        "import torch\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable, grad\n",
        "\n",
        "one = None\n",
        "zero = None\n",
        "mixing_factors = None\n",
        "\n",
        "\n",
        "def get_mixing_factor(batch_size):\n",
        "    global mixing_factors\n",
        "    if mixing_factors is None or batch_size != mixing_factors.size(0):\n",
        "        mixing_factors = cudize(torch.FloatTensor(batch_size, 1, 1))\n",
        "    mixing_factors.uniform_()\n",
        "    return mixing_factors\n",
        "\n",
        "\n",
        "def get_one(batch_size):\n",
        "    global one\n",
        "    if one is None or batch_size != one.size(0):\n",
        "        one = cudize(torch.ones(batch_size))\n",
        "    return one\n",
        "\n",
        "\n",
        "def get_zero(batch_size):\n",
        "    global zero\n",
        "    if zero is None or batch_size != zero.size(0):\n",
        "        zero = cudize(torch.zeros(batch_size))\n",
        "    return zero\n",
        "\n",
        "\n",
        "def calc_grad(x_hat, pred_hat):\n",
        "    return grad(outputs=pred_hat, inputs=x_hat, grad_outputs=get_one(pred_hat.size(0)),\n",
        "                create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
        "\n",
        "\n",
        "def generator_loss(dis: torch.nn.Module, gen: torch.nn.Module, real, z, loss_type: str,\n",
        "                   random_multiply: bool, feature_matching_lambda: float = 0.0):\n",
        "    gen.zero_grad()\n",
        "    g_, _ = gen(z)\n",
        "    d_fake, fake_features, _ = dis(g_)\n",
        "    real_features = None\n",
        "    scale = random.random() if random_multiply else 1.0\n",
        "    if loss_type == 'hinge' or loss_type.startswith('wgan'):\n",
        "        g_loss = -d_fake.mean()\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            d_real, real_features, _ = dis(real)\n",
        "        if loss_type == 'rsgan':\n",
        "            g_loss = F.binary_cross_entropy_with_logits(d_fake - d_real, get_one(d_fake.size(0)))\n",
        "        elif loss_type == 'rasgan':\n",
        "            batch_size = d_fake.size(0)\n",
        "            g_loss = (F.binary_cross_entropy_with_logits(d_fake - d_real.mean(),\n",
        "                                                         get_one(batch_size)) + F.binary_cross_entropy_with_logits(\n",
        "                d_real - d_fake.mean(), get_zero(batch_size))) / 2.0\n",
        "        elif loss_type == 'rahinge':\n",
        "            g_loss = (torch.mean(F.relu(1.0 + (d_real - torch.mean(d_fake)))) + torch.mean(\n",
        "                F.relu(1.0 - (d_fake - torch.mean(d_real))))) / 2\n",
        "        else:\n",
        "            raise ValueError('Invalid loss type')\n",
        "    if feature_matching_lambda != 0.0:\n",
        "        if real_features is None:\n",
        "            with torch.no_grad():\n",
        "                _, real_features, _ = dis(real)\n",
        "        diff = real_features.mean(dim=0) - fake_features.mean(dim=0)\n",
        "        g_loss = g_loss + (diff * diff).mean()\n",
        "    return g_loss * scale\n",
        "\n",
        "\n",
        "def discriminator_loss(dis: torch.nn.Module, gen: torch.nn.Module, real, z, loss_type: str,\n",
        "                       iwass_drift_epsilon: float, grad_lambda: float, iwass_target: float):\n",
        "    dis.zero_grad()\n",
        "    d_real, _, _ = dis(real)\n",
        "    with torch.no_grad():\n",
        "        g_, _ = gen(z)\n",
        "    d_fake, _, _ = dis(g_)\n",
        "    batch_size = d_real.size(0)\n",
        "    gp_gain = 1.0 if grad_lambda != 0 else 0\n",
        "    if loss_type == 'hinge':\n",
        "        d_loss = F.relu(1.0 - d_real).mean() + F.relu(1.0 + d_fake).mean()\n",
        "    elif loss_type == 'rsgan':\n",
        "        d_loss = F.binary_cross_entropy_with_logits(d_real - d_fake, get_one(batch_size))\n",
        "    elif loss_type == 'rasgan':\n",
        "        d_loss = (F.binary_cross_entropy_with_logits(d_real - d_fake.mean(),\n",
        "                                                     get_one(batch_size)) + F.binary_cross_entropy_with_logits(\n",
        "            d_fake - d_real.mean(), get_zero(batch_size))) / 2.0\n",
        "    elif loss_type == 'rahinge':\n",
        "        d_loss = (torch.mean(F.relu(1.0 - (d_real - d_fake.mean()))) + torch.mean(\n",
        "            F.relu(1.0 + (d_fake - d_real.mean())))) / 2\n",
        "    elif loss_type.startswith('wgan'):  # wgan and wgan_theirs\n",
        "        d_fake_mean = d_fake.mean()\n",
        "        d_real_mean = d_real.mean()\n",
        "        if loss_type == 'wgan_theirs':\n",
        "            d_loss = d_fake_mean - d_real_mean + (d_fake_mean + d_real_mean) ** 2 * iwass_drift_epsilon\n",
        "            gp_gain = F.relu(d_real_mean - d_fake_mean)\n",
        "        elif loss_type == 'wgan_gp':\n",
        "            d_loss = d_fake_mean - d_real_mean + (d_real ** 2).mean() * iwass_drift_epsilon\n",
        "            gp_gain = 1\n",
        "        else:\n",
        "            raise ValueError('Invalid loss type')\n",
        "    else:\n",
        "        raise ValueError('Invalid loss type')\n",
        "    if gp_gain != 0 and grad_lambda != 0:\n",
        "        alpha = get_mixing_factor(real['x'].size(0))\n",
        "        print(type(alpha), type(real), )\n",
        "        x_hat = {'x': Variable(alpha * real['x'].data + (1.0 - alpha) * g_['x'].data, requires_grad=True)}\n",
        "        beta = alpha.squeeze(dim=2)\n",
        "        for k in real.keys():\n",
        "            if k.startswith('global_') or k.startswith('temporal_'):\n",
        "                x_hat[k] = Variable(beta * real[k].data + (1.0 - beta) * g_[k].data, requires_grad=True)\n",
        "        pred_hat, _, _ = dis(x_hat)\n",
        "        g = calc_grad(x_hat['x'], pred_hat).view(batch_size, -1)\n",
        "        gp = g.norm(p=2, dim=1) - iwass_target\n",
        "        if loss_type == 'wgan_theirs':\n",
        "            gp = F.relu(gp)\n",
        "        gp_loss = gp_gain * (gp ** 2).mean() * grad_lambda / (iwass_target ** 2)\n",
        "        d_loss = d_loss + gp_loss\n",
        "    return d_loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nlWJZ3Eygnyy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#network.py\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.utils import spectral_norm\n",
        "\n",
        "\n",
        "class GBlock(nn.Module):\n",
        "    def __init__(self, ch_in, ch_out, ch_rgb, k_size=3, initial_kernel_size=None, is_residual=False,\n",
        "                 no_tanh=False, deep=False, per_channel_noise=False, to_rgb_mode='pggan', **layer_settings):\n",
        "        super().__init__()\n",
        "        is_first = initial_kernel_size is not None\n",
        "        first_k_size = initial_kernel_size if is_first else k_size\n",
        "        hidden_size = (ch_in // 4) if deep else ch_out\n",
        "        self.c1 = GeneralConv(ch_in, hidden_size, kernel_size=first_k_size,\n",
        "                              pad=initial_kernel_size - 1 if is_first else None, **layer_settings)\n",
        "        self.c2 = GeneralConv(hidden_size, hidden_size, kernel_size=k_size, **layer_settings)\n",
        "        if per_channel_noise:\n",
        "            self.c1_noise_weight = nn.Parameter(torch.zeros(1, hidden_size, 1))\n",
        "            self.c2_noise_weight = nn.Parameter(torch.zeros(1, hidden_size, 1))\n",
        "        else:\n",
        "            self.c1_noise_weight, self.c2_noise_weight = None, None\n",
        "        if deep:\n",
        "            self.c3 = GeneralConv(hidden_size, hidden_size, kernel_size=k_size, **layer_settings)\n",
        "            self.c4 = GeneralConv(hidden_size, ch_out, kernel_size=k_size, **layer_settings)\n",
        "            if per_channel_noise:\n",
        "                self.c3_noise_weight = nn.Parameter(torch.zeros(1, hidden_size, 1))\n",
        "                self.c4_noise_weight = nn.Parameter(torch.zeros(1, ch_out, 1))\n",
        "            else:\n",
        "                self.c3_noise_weight, self.c4_noise_weight = None, None\n",
        "        reduced_layer_settings = dict(equalized=layer_settings['equalized'], spectral=layer_settings['equalized'],\n",
        "                                      init=layer_settings['equalized'])\n",
        "        if to_rgb_mode == 'pggan':\n",
        "            to_rgb = GeneralConv(ch_out, ch_rgb, kernel_size=1, act_alpha=-1, **reduced_layer_settings)\n",
        "        elif to_rgb_mode in {'sngan', 'sagan'}:\n",
        "            to_rgb = GeneralConv(ch_out, ch_rgb if to_rgb_mode == 'sngan' else ch_out,\n",
        "                                 kernel_size=3, act_alpha=0.2, **reduced_layer_settings)\n",
        "            if to_rgb_mode == 'sagan':\n",
        "                to_rgb = nn.Sequential(\n",
        "                    GeneralConv(ch_out, ch_rgb, kernel_size=1, act_alpha=-1, **reduced_layer_settings), to_rgb)\n",
        "        elif to_rgb_mode == 'biggan':\n",
        "            to_rgb = nn.Sequential(nn.BatchNorm1d(ch_out), nn.ReLU(),\n",
        "                                   GeneralConv(ch_out, ch_rgb, kernel_size=3, act_alpha=-1, **reduced_layer_settings))\n",
        "        else:\n",
        "            raise ValueError()\n",
        "        if no_tanh:\n",
        "            self.toRGB = to_rgb\n",
        "        else:\n",
        "            self.toRGB = nn.Sequential(to_rgb, ScaledTanh())\n",
        "        if deep:\n",
        "            self.residual = PassChannelResidual()\n",
        "        else:\n",
        "            if not is_first and is_residual:\n",
        "                self.residual = nn.Sequential() if ch_in == ch_out else \\\n",
        "                    GeneralConv(ch_in, ch_out, 1, act_alpha=-1, **reduced_layer_settings)\n",
        "            else:\n",
        "                self.residual = None\n",
        "        self.deep = deep\n",
        "\n",
        "    @staticmethod\n",
        "    def get_per_channel_noise(noise_weight):\n",
        "        return None if noise_weight is None else torch.randn(*noise_weight.size()) * noise_weight\n",
        "\n",
        "    def forward(self, x, y=None, z=None, last=False):\n",
        "        h = self.c1(x, y=y, z=z, conv_noise=self.get_per_channel_noise(self.c1_noise_weight))\n",
        "        h = self.c2(h, y=y, z=z, conv_noise=self.get_per_channel_noise(self.c2_noise_weight))\n",
        "        if self.deep:\n",
        "            h = self.c3(h, y=y, z=z, conv_noise=self.get_per_channel_noise(self.c3_noise_weight))\n",
        "            h = self.c4(h, y=y, z=z, conv_noise=self.get_per_channel_noise(self.c4_noise_weight))\n",
        "            h = self.residual(h, x)\n",
        "        elif self.residual is not None:\n",
        "            h = h + self.residual(x)\n",
        "        if last:\n",
        "            return self.toRGB(h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, initial_kernel_size, num_rgb_channels, fmap_base, fmap_max, fmap_min, kernel_size,\n",
        "                 self_attention_layers, progression_scale_up, progression_scale_down, residual, separable,\n",
        "                 equalized, init, act_alpha, num_classes, deep, z_distribution, spectral=False,\n",
        "                 latent_size=256, no_tanh=False, per_channel_noise=False, to_rgb_mode='pggan', z_to_bn=False,\n",
        "                 split_z=False, dropout=0.2, act_norm='pixel', conv_only=False, shared_embedding_size=32,\n",
        "                 normalize_latents=True, rgb_generation_mode='pggan'):\n",
        "        \"\"\"\n",
        "        :param initial_kernel_size: int, this should be always correct regardless of conv_only\n",
        "        :param num_rgb_channels: int\n",
        "        :param fmap_base: int\n",
        "        :param fmap_max: int\n",
        "        :param fmap_min: int\n",
        "        :param kernel_size: int\n",
        "        :param self_attention_layers: list[int]\n",
        "        :param progression_scale_up: list[int]\n",
        "        :param progression_scale_down: list[int]\n",
        "        :param residual: bool\n",
        "        :param separable: bool\n",
        "        :param equalized: bool\n",
        "        :param spectral: bool\n",
        "        :param init: 'kaiming_normal' or 'xavier_uniform' or 'orthogonal'\n",
        "        :param act_alpha: float, 0 is relu, -1 is linear and 0.2 is recommended\n",
        "        :param z_distribution: 'normal' or 'bernoulli' or 'censored'\n",
        "        :param latent_size: int\n",
        "        :param no_tanh: bool\n",
        "        :param deep: bool, in case it's true it will turn off split_z\n",
        "        :param per_channel_noise: bool\n",
        "        :param to_rgb_mode: 'pggan' or 'sagan' or 'sngan' or 'biggan'\n",
        "        :param z_to_bn: bool, whether to concatenate z with y(if available) to feed to cbn or not\n",
        "        :param split_z: bool\n",
        "        :param dropout: float\n",
        "        :param num_classes: int, input y.shape == (batch_size, num_classes, T_y)\n",
        "        :param act_norm: 'batch' or 'pixel' or None\n",
        "        :param conv_only: bool\n",
        "        :param shared_embedding_size: int, in case it's none zero, y will be transformed to (batch_size, shared_embedding_size, T_y)\n",
        "        :param normalize_latents: bool\n",
        "        :param rgb_generation_mode: 'residual'sum([rgbs]) or 'mean'mean([rgbs]) or 'pggan'(last_rgb)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        R = len(progression_scale_up)\n",
        "        assert len(progression_scale_up) == len(progression_scale_down)\n",
        "        self.progression_scale_up = progression_scale_up\n",
        "        self.progression_scale_down = progression_scale_down\n",
        "        self.depth = 0\n",
        "        self.alpha = 1.0\n",
        "        if deep:\n",
        "            split_z = False\n",
        "        self.split_z = split_z\n",
        "        self.z_distribution = z_distribution\n",
        "        self.conv_only = conv_only\n",
        "        self.initial_kernel_size = initial_kernel_size\n",
        "        self.normalize_latents = normalize_latents\n",
        "        self.z_to_bn = z_to_bn\n",
        "\n",
        "        def nf(stage):\n",
        "            return min(max(int(fmap_base / (2.0 ** stage)), fmap_min), fmap_max)\n",
        "\n",
        "        if latent_size is None:\n",
        "            latent_size = nf(0)\n",
        "        self.input_latent_size = latent_size\n",
        "        if num_classes != 0:\n",
        "            if shared_embedding_size > 0:\n",
        "                self.y_encoder = GeneralConv(num_classes, shared_embedding_size, kernel_size=1,\n",
        "                                             equalized=False, act_alpha=act_alpha, spectral=False, bias=False)\n",
        "                num_classes = shared_embedding_size\n",
        "            else:\n",
        "                self.y_encoder = nn.Sequential()\n",
        "        else:\n",
        "            self.y_encoder = None\n",
        "        if split_z:\n",
        "            latent_size //= R + 2  # we also give part of the z to the first layer\n",
        "        self.latent_size = latent_size\n",
        "        block_settings = dict(ch_rgb=num_rgb_channels, k_size=kernel_size, is_residual=residual, deep=deep,\n",
        "                              no_tanh=no_tanh, per_channel_noise=per_channel_noise, to_rgb_mode=to_rgb_mode)\n",
        "        layer_settings = dict(z_to_bn_size=latent_size if z_to_bn else 0, equalized=equalized, spectral=spectral,\n",
        "                              init=init, act_alpha=act_alpha, do=dropout, num_classes=num_classes, act_norm=act_norm,\n",
        "                              bias=True, separable=separable)\n",
        "        self.block0 = GBlock(latent_size, nf(1), **block_settings, **layer_settings,\n",
        "                             initial_kernel_size=None if conv_only else initial_kernel_size)\n",
        "        dummy = []  # to make SA layers registered\n",
        "        self.self_attention = dict()\n",
        "        for layer in self_attention_layers:\n",
        "            dummy.append(SelfAttention(nf(layer + 1), spectral, init))\n",
        "            self.self_attention[layer] = dummy[-1]\n",
        "        if len(dummy) != 0:\n",
        "            self.dummy = nn.ModuleList(dummy)\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [GBlock(nf(i + 1), nf(i + 2), **block_settings, **layer_settings) for i in range(R)])\n",
        "        self.max_depth = len(self.blocks)\n",
        "        self.deep = deep\n",
        "        self.rgb_generation_mode = rgb_generation_mode\n",
        "\n",
        "    def _split_z(self, l, z):\n",
        "        if not self.z_to_bn:\n",
        "            return None\n",
        "        if self.split_z:\n",
        "            return z[:, (2 + l) * self.latent_size:(3 + l) * self.latent_size]\n",
        "        return z\n",
        "\n",
        "    def _do_layer(self, l, h, y, z):\n",
        "        if l in self.self_attention:\n",
        "            h, attention_map = self.self_attention[l](h)\n",
        "        else:\n",
        "            attention_map = None\n",
        "        h = resample_signal(h, self.progression_scale_down[l], self.progression_scale_up[l], True)\n",
        "        return self.blocks[l](h, y, self._split_z(l, z), last=False), attention_map\n",
        "\n",
        "    def _combine_rgbs(self, last_rgb, saved_rgbs):\n",
        "        if self.rgb_generation_mode == 'residual':\n",
        "            return_value = saved_rgbs[0]\n",
        "            for rgb in saved_rgbs[1:]:\n",
        "                return_value = resample_signal(return_value, return_value.size(2), rgb.size(2)) + rgb\n",
        "            if self.alpha == 1.0:\n",
        "                return return_value\n",
        "            return return_value - (1.0 - self.alpha) * saved_rgbs[-1]\n",
        "        elif self.rgb_generation_mode == 'mean':\n",
        "            return_value = saved_rgbs[0]\n",
        "            for rgb in saved_rgbs[1:]:\n",
        "                return_value = resample_signal(return_value, return_value.size(2), rgb.size(2)) + rgb\n",
        "            return_value = return_value / len(saved_rgbs)\n",
        "            if self.alpha == 1.0:\n",
        "                return return_value\n",
        "            return (return_value * len(saved_rgbs) - saved_rgbs[-1]) / (len(saved_rgbs) - 1) * (\n",
        "                    1.0 - self.alpha) + return_value * self.alpha\n",
        "        return last_rgb\n",
        "\n",
        "    def _wrap_output(self, last_rgb, all_rgbs, y):\n",
        "        return {'x': self._combine_rgbs(last_rgb, all_rgbs), 'y': y}\n",
        "\n",
        "    def forward(self, z):\n",
        "        if isinstance(z, dict):\n",
        "            z, y = z['z'], z.get('y', None)\n",
        "        elif isinstance(z, tuple):\n",
        "            z, y = z\n",
        "        else:\n",
        "            y = None\n",
        "        if y is not None:\n",
        "            if y.ndimension() == 2:\n",
        "                y = y.unsqueeze(2)\n",
        "            if self.y_encoder is not None:\n",
        "                y = self.y_encoder(y)\n",
        "            else:\n",
        "                y = None\n",
        "        if self.normalize_latents:\n",
        "            z = pixel_norm(z)\n",
        "        if z.ndimension() == 2:\n",
        "            z = z.unsqueeze(2)\n",
        "        if self.split_z and not self.deep:\n",
        "            h = z[:, :self.latent_size, :]\n",
        "        else:\n",
        "            h = z\n",
        "        save_rgb = self.rgb_generation_mode != 'pggan'\n",
        "        saved_rgbs = []\n",
        "        if self.depth == 0:\n",
        "            h = self.block0(h, y, self._split_z(-1, z), last=True)\n",
        "            if save_rgb:\n",
        "                saved_rgbs.append(h)\n",
        "            return self._wrap_output(h, saved_rgbs, y), {}\n",
        "        h = self.block0(h, y, self._split_z(-1, z))\n",
        "        if save_rgb:\n",
        "            saved_rgbs.append(self.block0.toRGB(h))\n",
        "        all_attention_maps = {}\n",
        "        for i in range(self.depth - 1):\n",
        "            h, attention_map = self._do_layer(i, h, y, z)\n",
        "            if save_rgb:\n",
        "                saved_rgbs.append(self.blocks[i].toRGB(h))\n",
        "            if attention_map is not None:\n",
        "                all_attention_maps[i] = attention_map\n",
        "        h = resample_signal(h, self.progression_scale_down[self.depth - 1], self.progression_scale_up[self.depth - 1],\n",
        "                            True)\n",
        "        ult = self.blocks[self.depth - 1](h, y, self._split_z(self.depth - 1, z), True)\n",
        "        if save_rgb:\n",
        "            saved_rgbs.append(ult)\n",
        "        if self.alpha == 1.0:\n",
        "            return self._wrap_output(ult, saved_rgbs, y), all_attention_maps\n",
        "        preult_rgb = self.blocks[self.depth - 2].toRGB(h) if self.depth > 1 else self.block0.toRGB(h)\n",
        "        return self._wrap_output(preult_rgb * (1.0 - self.alpha) + ult * self.alpha, saved_rgbs, y), all_attention_maps\n",
        "\n",
        "\n",
        "class DBlock(nn.Module):\n",
        "    def __init__(self, ch_in, ch_out, ch_rgb, k_size=3, initial_kernel_size=None, is_residual=False,\n",
        "                 deep=False, group_size=4, temporal_groups_per_window=1, conv_disc=False, **layer_settings):\n",
        "        super().__init__()\n",
        "        is_last = initial_kernel_size is not None\n",
        "        self.net = []\n",
        "        if is_last:\n",
        "            self.net.append(MinibatchStddev(group_size, temporal_groups_per_window, initial_kernel_size))\n",
        "        hidden_size = (ch_out // 4) if deep else ch_in\n",
        "        self.net.append(\n",
        "            GeneralConv(ch_in + (1 if is_last else 0), hidden_size, kernel_size=k_size, **layer_settings))\n",
        "        if deep:\n",
        "            self.net.append(\n",
        "                GeneralConv(hidden_size, hidden_size, kernel_size=k_size, **layer_settings))\n",
        "            self.net.append(\n",
        "                GeneralConv(hidden_size, hidden_size, kernel_size=k_size, **layer_settings))\n",
        "        is_linear_last = is_last and not conv_disc\n",
        "        self.net.append(GeneralConv(hidden_size, ch_out, kernel_size=initial_kernel_size if is_linear_last else k_size,\n",
        "                                    pad=0 if is_linear_last else None, **layer_settings))\n",
        "        self.net = nn.Sequential(*self.net)\n",
        "        reduced_layer_settings = dict(equalized=layer_settings['equalized'], spectral=layer_settings['equalized'],\n",
        "                                      init=layer_settings['equalized'])\n",
        "        self.fromRGB = GeneralConv(ch_rgb, ch_in, kernel_size=1, act_alpha=layer_settings['act_alpha'],\n",
        "                                   **reduced_layer_settings)\n",
        "        if deep:\n",
        "            self.residual = ConcatResidual(ch_in, ch_out, **reduced_layer_settings)\n",
        "        else:\n",
        "            if is_residual and (not is_last or conv_disc):\n",
        "                self.residual = nn.Sequential() if ch_in == ch_out else GeneralConv(ch_in, ch_out, kernel_size=1,\n",
        "                                                                                    act_alpha=-1,\n",
        "                                                                                    **reduced_layer_settings)\n",
        "            else:\n",
        "                self.residual = None\n",
        "        self.deep = deep\n",
        "\n",
        "    def forward(self, x, first=False):\n",
        "        if first:\n",
        "            x = self.fromRGB(x)\n",
        "        h = self.net(x)\n",
        "        if self.deep:\n",
        "            return self.residual(h, x)\n",
        "        if self.residual:\n",
        "            h = h + self.residual(x)\n",
        "        return h\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, initial_kernel_size, num_rgb_channels, fmap_base, fmap_max, fmap_min, kernel_size,\n",
        "                 self_attention_layers, progression_scale_up, progression_scale_down, residual, separable,\n",
        "                 equalized, init, act_alpha, num_classes, deep, spectral=False, dropout=0.2, act_norm=None,\n",
        "                 group_size=4, temporal_groups_per_window=1, conv_only=False, input_to_all_layers=False):\n",
        "        \"\"\"\n",
        "        NOTE we only support global conidtioning(not temporal) for now\n",
        "        :param initial_kernel_size:\n",
        "        :param num_rgb_channels:\n",
        "        :param fmap_base:\n",
        "        :param fmap_max:\n",
        "        :param fmap_min:\n",
        "        :param kernel_size:\n",
        "        :param self_attention_layers:\n",
        "        :param progression_scale_up:\n",
        "        :param progression_scale_down:\n",
        "        :param residual:\n",
        "        :param separable:\n",
        "        :param equalized:\n",
        "        :param spectral:\n",
        "        :param init:\n",
        "        :param act_alpha:\n",
        "        :param num_classes:\n",
        "        :param deep:\n",
        "        :param dropout:\n",
        "        :param act_norm:\n",
        "        :param group_size:\n",
        "        :param temporal_groups_per_window:\n",
        "        :param conv_only:\n",
        "        :param input_to_all_layers:\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        R = len(progression_scale_up)\n",
        "        assert len(progression_scale_up) == len(progression_scale_down)\n",
        "        self.progression_scale_up = progression_scale_up\n",
        "        self.progression_scale_down = progression_scale_down\n",
        "        self.depth = 0\n",
        "        self.alpha = 1.0\n",
        "        self.input_to_all_layers = input_to_all_layers\n",
        "\n",
        "        def nf(stage):\n",
        "            return min(max(int(fmap_base / (2.0 ** stage)), fmap_min), fmap_max)\n",
        "\n",
        "        layer_settings = dict(equalized=equalized, spectral=spectral, init=init, act_alpha=act_alpha,\n",
        "                              do=dropout, num_classes=0, act_norm=act_norm, bias=True, separable=separable)\n",
        "        block_settings = dict(ch_rgb=num_rgb_channels, k_size=kernel_size, is_residual=residual, conv_disc=conv_only,\n",
        "                              group_size=group_size, temporal_groups_per_window=temporal_groups_per_window, deep=deep)\n",
        "\n",
        "        last_block = DBlock(nf(1), nf(0), initial_kernel_size=initial_kernel_size, **block_settings, **layer_settings)\n",
        "        dummy = []  # to make SA layers registered\n",
        "        self.self_attention = dict()\n",
        "        for layer in self_attention_layers:\n",
        "            dummy.append(SelfAttention(nf(layer + 1), spectral, init))\n",
        "            self.self_attention[layer] = dummy[-1]\n",
        "        if len(dummy):\n",
        "            self.dummy = nn.ModuleList(dummy)\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [DBlock(nf(i + 2), nf(i + 1), **block_settings, **layer_settings) for i in range(R - 1, -1, -1)] + [\n",
        "                last_block])\n",
        "\n",
        "        if num_classes != 0:\n",
        "            self.class_emb = nn.Linear(num_classes, nf(0), False)\n",
        "            if spectral:\n",
        "                self.class_emb = spectral_norm(self.class_emb)\n",
        "        else:\n",
        "            self.class_emb = None\n",
        "        self.linear = GeneralConv(nf(0), 1, kernel_size=1, equalized=equalized, act_alpha=-1,\n",
        "                                  spectral=spectral, init=init)\n",
        "        self.max_depth = len(self.blocks) - 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        if isinstance(x, dict):\n",
        "            x, y = x['x'], x.get('y', None)\n",
        "        elif isinstance(x, tuple):\n",
        "            x, y = x\n",
        "        else:\n",
        "            y = None\n",
        "        h = self.blocks[-(self.depth + 1)](x, True)\n",
        "        if self.depth > 0:\n",
        "            h = resample_signal(h, self.progression_scale_up[self.depth - 1],\n",
        "                                self.progression_scale_down[self.depth - 1], True)\n",
        "            if self.alpha < 1.0 or self.input_to_all_layers:\n",
        "                x_lowres = resample_signal(x, self.progression_scale_up[self.depth - 1],\n",
        "                                           self.progression_scale_down[self.depth - 1], True)\n",
        "                preult_rgb = self.blocks[-self.depth].fromRGB(x_lowres)\n",
        "                if self.input_to_all_layers:\n",
        "                    h = (h * self.alpha + preult_rgb) / (1.0 + self.alpha)\n",
        "                else:\n",
        "                    h = h * self.alpha + (1.0 - self.alpha) * preult_rgb\n",
        "        all_attention_maps = {}\n",
        "        for i in range(self.depth, 0, -1):\n",
        "            h = self.blocks[-i](h)\n",
        "            if i > 1:\n",
        "                h = resample_signal(h, self.progression_scale_up[i - 2], self.progression_scale_down[i - 2], True)\n",
        "                if self.input_to_all_layers:\n",
        "                    x_lowres = resample_signal(x_lowres, self.progression_scale_up[i - 2],\n",
        "                                               self.progression_scale_down[i - 2], True)\n",
        "                    h = (h + self.blocks[-i + 1].fromRGB(x_lowres)) / 2.0\n",
        "            if (i - 2) in self.self_attention:\n",
        "                h, attention_map = self.self_attention[i - 2](h)\n",
        "                if attention_map is not None:\n",
        "                    all_attention_maps[i] = attention_map\n",
        "        o = self.linear(h).mean(dim=2).squeeze()\n",
        "        if y is not None:\n",
        "            emb = self.class_emb(y)\n",
        "            cond_loss = (emb * h.squeeze()).sum(dim=1)\n",
        "        else:\n",
        "            cond_loss = 0.0\n",
        "        return o + cond_loss, h, all_attention_maps\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DMfsNdeVg-DF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#torch_utils.py\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "class Plugin(object):\n",
        "\n",
        "    def __init__(self, interval=None):\n",
        "        if interval is None:\n",
        "            interval = []\n",
        "        self.trigger_interval = interval\n",
        "\n",
        "    def register(self, trainer):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class Monitor(Plugin):\n",
        "\n",
        "    def __init__(self, running_average=True, epoch_average=True, smoothing=0.7,\n",
        "                 precision=None, number_format=None, unit=''):\n",
        "        if precision is None:\n",
        "            precision = 4\n",
        "        if number_format is None:\n",
        "            number_format = '.{}f'.format(precision)\n",
        "        number_format = ':' + number_format\n",
        "        super(Monitor, self).__init__([(1, 'iteration'), (1, 'epoch')])\n",
        "\n",
        "        self.smoothing = smoothing\n",
        "        self.with_running_average = running_average\n",
        "        self.with_epoch_average = epoch_average\n",
        "\n",
        "        self.log_format = number_format\n",
        "        self.log_unit = unit\n",
        "        self.log_epoch_fields = None\n",
        "        self.log_iter_fields = ['{last' + number_format + '}' + unit]\n",
        "        if self.with_running_average:\n",
        "            self.log_iter_fields += [' ({running_avg' + number_format + '}' + unit + ')']\n",
        "        if self.with_epoch_average:\n",
        "            self.log_epoch_fields = ['{epoch_mean' + number_format + '}' + unit]\n",
        "\n",
        "    def register(self, trainer):\n",
        "        self.trainer = trainer\n",
        "        stats = self.trainer.stats.setdefault(self.stat_name, {})\n",
        "        stats['log_format'] = self.log_format\n",
        "        stats['log_unit'] = self.log_unit\n",
        "        stats['log_iter_fields'] = self.log_iter_fields\n",
        "        if self.with_epoch_average:\n",
        "            stats['log_epoch_fields'] = self.log_epoch_fields\n",
        "        if self.with_epoch_average:\n",
        "            stats['epoch_stats'] = (0, 0)\n",
        "\n",
        "    def iteration(self, *args):\n",
        "        stats = self.trainer.stats.setdefault(self.stat_name, {})\n",
        "        stats['last'] = self._get_value(*args)\n",
        "\n",
        "        if self.with_epoch_average:\n",
        "            stats['epoch_stats'] = tuple(sum(t) for t in\n",
        "                                         zip(stats['epoch_stats'], (stats['last'], 1)))\n",
        "\n",
        "        if self.with_running_average:\n",
        "            previous_avg = stats.get('running_avg', 0)\n",
        "            stats['running_avg'] = previous_avg * self.smoothing + \\\n",
        "                                   stats['last'] * (1 - self.smoothing)\n",
        "\n",
        "    def epoch(self, idx):\n",
        "        stats = self.trainer.stats.setdefault(self.stat_name, {})\n",
        "        if self.with_epoch_average:\n",
        "            epoch_stats = stats['epoch_stats']\n",
        "            stats['epoch_mean'] = epoch_stats[0] / epoch_stats[1]\n",
        "            stats['epoch_stats'] = (0, 0)\n",
        "\n",
        "\n",
        "class LossMonitor(Monitor):\n",
        "    stat_name = 'loss'\n",
        "\n",
        "    def _get_value(self, iteration, input, target, output, loss):\n",
        "        return loss.item()\n",
        "\n",
        "\n",
        "class Logger(Plugin):\n",
        "    alignment = 4\n",
        "    separator = '#' * 80\n",
        "\n",
        "    def __init__(self, fields, interval=None):\n",
        "        if interval is None:\n",
        "            interval = [(1, 'iteration'), (1, 'epoch')]\n",
        "        super(Logger, self).__init__(interval)\n",
        "        self.field_widths = defaultdict(lambda: defaultdict(int))\n",
        "        self.fields = list(map(lambda f: f.split('.'), fields))\n",
        "\n",
        "    def _join_results(self, results):\n",
        "        joined_out = map(lambda i: (i[0], ' '.join(i[1])), results)\n",
        "        joined_fields = map(lambda i: '{}: {}'.format(i[0], i[1]), joined_out)\n",
        "        return '\\t'.join(joined_fields)\n",
        "\n",
        "    def log(self, msg):\n",
        "        print(msg)\n",
        "\n",
        "    def register(self, trainer):\n",
        "        self.trainer = trainer\n",
        "\n",
        "    def gather_stats(self):\n",
        "        result = {}\n",
        "        return result\n",
        "\n",
        "    def _align_output(self, field_idx, output):\n",
        "        for output_idx, o in enumerate(output):\n",
        "            if len(o) < self.field_widths[field_idx][output_idx]:\n",
        "                num_spaces = self.field_widths[field_idx][output_idx] - len(o)\n",
        "                output[output_idx] += ' ' * num_spaces\n",
        "            else:\n",
        "                self.field_widths[field_idx][output_idx] = len(o)\n",
        "\n",
        "    def _gather_outputs(self, field, log_fields, stat_parent, stat, require_dict=False):\n",
        "        output = []\n",
        "        name = ''\n",
        "        if isinstance(stat, dict):\n",
        "            log_fields = stat.get(log_fields, [])\n",
        "            name = stat.get('log_name', '.'.join(field))\n",
        "            for f in log_fields:\n",
        "                output.append(f.format(**stat))\n",
        "        elif not require_dict:\n",
        "            name = '.'.join(field)\n",
        "            number_format = stat_parent.get('log_format', '')\n",
        "            unit = stat_parent.get('log_unit', '')\n",
        "            fmt = '{' + number_format + '}' + unit\n",
        "            output.append(fmt.format(stat))\n",
        "        return name, output\n",
        "\n",
        "    def _log_all(self, log_fields, prefix=None, suffix=None, require_dict=False):\n",
        "        results = []\n",
        "        for field_idx, field in enumerate(self.fields):\n",
        "            parent, stat = None, self.trainer.stats\n",
        "            for f in field:\n",
        "                parent, stat = stat, stat[f]\n",
        "            name, output = self._gather_outputs(field, log_fields,\n",
        "                                                parent, stat, require_dict)\n",
        "            if not output:\n",
        "                continue\n",
        "            self._align_output(field_idx, output)\n",
        "            results.append((name, output))\n",
        "        if not results:\n",
        "            return\n",
        "        output = self._join_results(results)\n",
        "        if prefix is not None:\n",
        "            self.log(prefix)\n",
        "        self.log(output)\n",
        "        if suffix is not None:\n",
        "            self.log(suffix)\n",
        "\n",
        "    def iteration(self, *args):\n",
        "        self._log_all('log_iter_fields')\n",
        "\n",
        "    def epoch(self, epoch_idx):\n",
        "        self._log_all('log_epoch_fields',\n",
        "                      prefix=self.separator + '\\nEpoch summary:',\n",
        "                      suffix=self.separator,\n",
        "                      require_dict=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c1WEwYCohH7d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#trainer.py\n",
        "import heapq\n",
        "\n",
        "\n",
        "class Trainer(object):\n",
        "    def __init__(self, discriminator: Discriminator, generator: Generator, d_loss, g_loss, dataset,\n",
        "                 random_latents_generator, resume_nimg, optimizer_g, optimizer_d, d_training_repeats: int = 5,\n",
        "                 tick_kimg_default: float = 5.0):\n",
        "        assert d_training_repeats >= 1\n",
        "        self.d_training_repeats = d_training_repeats\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.d_loss = d_loss\n",
        "        self.g_loss = g_loss\n",
        "        self.dataset = dataset\n",
        "        self.cur_nimg = resume_nimg\n",
        "        self.random_latents_generator = random_latents_generator\n",
        "        self.tick_start_nimg = self.cur_nimg\n",
        "        self.tick_duration_nimg = int(tick_kimg_default * 1000)\n",
        "        self.iterations = 0\n",
        "        self.cur_tick = 0\n",
        "        self.time = 0\n",
        "        self.lr_scheduler_g = None\n",
        "        self.lr_scheduler_d = None\n",
        "        self.optimizer_g = optimizer_g\n",
        "        self.optimizer_d = optimizer_d\n",
        "        self.stats = {\n",
        "            'kimg_stat': {'val': self.cur_nimg / 1000., 'log_epoch_fields': ['{val:8.3f}'], 'log_name': 'kimg'},\n",
        "            'tick_stat': {'val': self.cur_tick, 'log_epoch_fields': ['{val:5}'], 'log_name': 'tick'}\n",
        "        }\n",
        "        self.plugin_queues = {\n",
        "            'iteration': [],\n",
        "            'epoch': [],  # this is tick\n",
        "            'end': []\n",
        "        }\n",
        "\n",
        "    def register_plugin(self, plugin):\n",
        "        plugin.register(self)\n",
        "        intervals = plugin.trigger_interval\n",
        "        if not isinstance(intervals, list):\n",
        "            intervals = [intervals]\n",
        "        for (duration, unit) in intervals:\n",
        "            queue = self.plugin_queues[unit]\n",
        "            queue.append((duration, len(queue), plugin))\n",
        "\n",
        "    def call_plugins(self, queue_name, time, *args):\n",
        "        args = (time,) + args\n",
        "        queue = self.plugin_queues[queue_name]\n",
        "        if len(queue) == 0:\n",
        "            return\n",
        "        while queue[0][0] <= time:\n",
        "            plugin = queue[0][2]\n",
        "            getattr(plugin, queue_name)(*args)\n",
        "            for trigger in plugin.trigger_interval:\n",
        "                if trigger[1] == queue_name:\n",
        "                    interval = trigger[0]\n",
        "            new_item = (time + interval, queue[0][1], plugin)\n",
        "            heapq.heappushpop(queue, new_item)\n",
        "\n",
        "    def run(self, total_kimg=1):\n",
        "        for q in self.plugin_queues.values():\n",
        "            heapq.heapify(q)\n",
        "        total_nimg = int(total_kimg * 1000)\n",
        "        try:\n",
        "            while self.cur_nimg < total_nimg:\n",
        "                self.train()\n",
        "                if self.cur_nimg >= self.tick_start_nimg + self.tick_duration_nimg or self.cur_nimg >= total_nimg:\n",
        "                    self.cur_tick += 1\n",
        "                    self.tick_start_nimg = self.cur_nimg\n",
        "                    self.stats['kimg_stat']['val'] = self.cur_nimg / 1000.\n",
        "                    self.stats['tick_stat']['val'] = self.cur_tick\n",
        "                    self.call_plugins('epoch', self.cur_tick)\n",
        "        except KeyboardInterrupt:\n",
        "            return\n",
        "        self.call_plugins('end', 1)\n",
        "\n",
        "    def train(self):\n",
        "        if self.lr_scheduler_g is not None:\n",
        "            self.lr_scheduler_g.step(self.cur_nimg / self.d_training_repeats)\n",
        "        fake_latents_in = cudize(next(self.random_latents_generator))\n",
        "        for i in range(self.d_training_repeats):\n",
        "            if self.lr_scheduler_d is not None:\n",
        "                self.lr_scheduler_d.step(self.cur_nimg)\n",
        "            real_images_expr = cudize(next(self.dataiter))\n",
        "            self.cur_nimg += real_images_expr['x'].size(0)\n",
        "            d_loss = self.d_loss(self.discriminator, self.generator, real_images_expr, fake_latents_in)\n",
        "            d_loss.backward()\n",
        "            self.optimizer_d.step()\n",
        "            fake_latents_in = cudize(next(self.random_latents_generator))\n",
        "        g_loss = self.g_loss(self.discriminator, self.generator, real_images_expr, fake_latents_in)\n",
        "        g_loss.backward()\n",
        "        self.optimizer_g.step()\n",
        "        self.iterations += 1\n",
        "        self.call_plugins('iteration', self.iterations, *(g_loss, d_loss))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hbFwwtr8g3Ij",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import os\n",
        "import time\n",
        "from copy import deepcopy\n",
        "from datetime import timedelta\n",
        "from glob import glob\n",
        "\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from scipy import misc\n",
        "from sklearn.utils.extmath import randomized_svd\n",
        "\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class DepthManager(Plugin):\n",
        "    minibatch_override = {0: 256, 1: 256, 2: 128, 3: 128, 4: 48, 5: 32,\n",
        "                          6: 32, 7: 32, 8: 16, 9: 16, 10: 8, 11: 8}\n",
        "\n",
        "    tick_kimg_override = {4: 4, 5: 4, 6: 4, 7: 3, 8: 3, 9: 2, 10: 2, 11: 1}\n",
        "    training_kimg_override = {1: 200, 2: 200, 3: 200, 4: 200}\n",
        "    transition_kimg_override = {1: 200, 2: 200, 3: 200, 4: 200}\n",
        "\n",
        "    def __init__(self,  # everything starts from 0 or 1\n",
        "                 create_dataloader_fun, create_rlg, max_depth,\n",
        "                 tick_kimg_default, get_optimizer, default_lr,\n",
        "                 reset_optimizer: bool = True, disable_progression=False,\n",
        "                 minibatch_default=256, depth_offset=0,  # starts form 0\n",
        "                 lod_training_kimg=400, lod_transition_kimg=400):\n",
        "        super().__init__([(1, 'iteration')])\n",
        "        self.reset_optimizer = reset_optimizer\n",
        "        self.minibatch_default = minibatch_default\n",
        "        self.tick_kimg_default = tick_kimg_default\n",
        "        self.create_dataloader_fun = create_dataloader_fun\n",
        "        self.create_rlg = create_rlg\n",
        "        self.trainer = None\n",
        "        self.depth = -1\n",
        "        self.alpha = -1\n",
        "        self.get_optimizer = get_optimizer\n",
        "        self.disable_progression = disable_progression\n",
        "        self.depth_offset = depth_offset\n",
        "        self.max_depth = max_depth\n",
        "        self.default_lr = default_lr\n",
        "        self.alpha_map = self.pre_compute_alpha_map(self.depth_offset, max_depth, lod_training_kimg,\n",
        "                                                    self.training_kimg_override, lod_transition_kimg,\n",
        "                                                    self.transition_kimg_override)\n",
        "\n",
        "    def register(self, trainer):\n",
        "        self.trainer = trainer\n",
        "        self.trainer.stats['minibatch_size'] = self.minibatch_default\n",
        "        self.trainer.stats['alpha'] = {'log_name': 'alpha', 'log_epoch_fields': ['{val:.2f}'], 'val': self.alpha}\n",
        "        self.iteration(is_resuming=self.trainer.optimizer_d is not None)\n",
        "\n",
        "    @staticmethod\n",
        "    def pre_compute_alpha_map(start_depth, max_depth, lod_training_kimg, lod_training_kimg_overrides,\n",
        "                              lod_transition_kimg, lod_transition_kimg_overrides):\n",
        "        points = []\n",
        "        pointer = 0\n",
        "        for i in range(start_depth, max_depth):\n",
        "            pointer += int(lod_training_kimg_overrides.get(i + 1, lod_training_kimg) * 1000)\n",
        "            points.append(pointer)\n",
        "            pointer += int(lod_transition_kimg_overrides.get(i + 1, lod_transition_kimg) * 1000)\n",
        "            points.append(pointer)\n",
        "        return points\n",
        "\n",
        "    def calc_progress(self, cur_nimg=None):\n",
        "        if cur_nimg is None:\n",
        "            cur_nimg = self.trainer.cur_nimg\n",
        "        depth = self.depth_offset\n",
        "        alpha = 1.0\n",
        "        for i, point in enumerate(self.alpha_map):\n",
        "            if cur_nimg == point:\n",
        "                break\n",
        "            if cur_nimg > point and i % 2 == 0:\n",
        "                depth += 1\n",
        "            if cur_nimg < point and i % 2 == 1:\n",
        "                alpha = (cur_nimg - self.alpha_map[i - 1]) / (point - self.alpha_map[i - 1])\n",
        "                break\n",
        "            if cur_nimg < point:\n",
        "                break\n",
        "        depth = min(self.max_depth, depth)\n",
        "        if self.disable_progression:\n",
        "            depth = self.max_depth\n",
        "            alpha = 1.0\n",
        "        return depth, alpha\n",
        "\n",
        "    def iteration(self, is_resuming=False, *args):\n",
        "        depth, alpha = self.calc_progress()\n",
        "        dataset = self.trainer.dataset\n",
        "        if depth != self.depth:\n",
        "            self.trainer.discriminator.depth = self.trainer.generator.depth = dataset.model_depth = depth\n",
        "            self.depth = depth\n",
        "            minibatch_size = self.minibatch_override.get(depth - self.depth_offset, self.minibatch_default)\n",
        "            if self.reset_optimizer and not is_resuming:\n",
        "                self.trainer.optimizer_g, self.trainer.optimizer_d, self.trainer.lr_scheduler_g, self.trainer.lr_scheduler_d = self.get_optimizer(\n",
        "                    self.minibatch_default * self.default_lr / minibatch_size)\n",
        "            self.data_loader = self.create_dataloader_fun(minibatch_size)\n",
        "            self.trainer.dataiter = iter(self.data_loader)\n",
        "            self.trainer.random_latents_generator = self.create_rlg(minibatch_size)\n",
        "            tick_duration_kimg = self.tick_kimg_override.get(depth - self.depth_offset, self.tick_kimg_default)\n",
        "            self.trainer.tick_duration_nimg = int(tick_duration_kimg * 1000)\n",
        "            self.trainer.stats['minibatch_size'] = minibatch_size\n",
        "        if alpha != self.alpha:\n",
        "            self.trainer.discriminator.alpha = self.trainer.generator.alpha = dataset.alpha = alpha\n",
        "            self.alpha = alpha\n",
        "        self.trainer.stats['depth'] = depth\n",
        "        self.trainer.stats['alpha']['val'] = alpha\n",
        "\n",
        "\n",
        "class EfficientLossMonitor(LossMonitor):\n",
        "    def __init__(self, loss_no, stat_name, monitor_threshold: float = 10.0,\n",
        "                 monitor_warmup: int = 50, monitor_patience: int = 5):\n",
        "        super().__init__()\n",
        "        self.loss_no = loss_no\n",
        "        self.stat_name = stat_name\n",
        "        self.threshold = monitor_threshold\n",
        "        self.warmup = monitor_warmup\n",
        "        self.patience = monitor_patience\n",
        "        self.counter = 0\n",
        "\n",
        "    def _get_value(self, iteration, *args):\n",
        "        val = args[self.loss_no].item()\n",
        "        if val != val:\n",
        "            raise ValueError('loss value is NaN :((')\n",
        "        return val\n",
        "\n",
        "    def epoch(self, idx):\n",
        "        super().epoch(idx)\n",
        "        if idx > self.warmup:\n",
        "            loss_value = self.trainer.stats[self.stat_name]['epoch_mean']\n",
        "            if abs(loss_value) > self.threshold:\n",
        "                self.counter += 1\n",
        "                if self.counter > self.patience:\n",
        "                    raise ValueError('loss value exceeded the threshold')\n",
        "            else:\n",
        "                self.counter = 0\n",
        "\n",
        "\n",
        "class AbsoluteTimeMonitor(Plugin):\n",
        "    def __init__(self):\n",
        "        super().__init__([(1, 'epoch')])\n",
        "        self.start_time = time.time()\n",
        "        self.epoch_start = self.start_time\n",
        "        self.start_nimg = None\n",
        "        self.epoch_time = 0\n",
        "\n",
        "    def register(self, trainer):\n",
        "        self.trainer = trainer\n",
        "        self.start_nimg = trainer.cur_nimg\n",
        "        self.trainer.stats['sec'] = {'log_format': ':.1f'}\n",
        "\n",
        "    def epoch(self, epoch_index):\n",
        "        cur_time = time.time()\n",
        "        tick_time = cur_time - self.epoch_start\n",
        "        self.epoch_start = cur_time\n",
        "        kimg_time = tick_time / (self.trainer.cur_nimg - self.start_nimg) * 1000\n",
        "        self.start_nimg = self.trainer.cur_nimg\n",
        "        self.trainer.stats['time'] = timedelta(seconds=time.time() - self.start_time)\n",
        "        self.trainer.stats['sec']['tick'] = tick_time\n",
        "        self.trainer.stats['sec']['kimg'] = kimg_time\n",
        "\n",
        "\n",
        "class SaverPlugin(Plugin):\n",
        "    last_pattern = 'network-snapshot-{}-{}.dat'\n",
        "\n",
        "    def __init__(self, checkpoints_path, keep_old_checkpoints: bool = True, network_snapshot_ticks: int = 50):\n",
        "        super().__init__([(network_snapshot_ticks, 'epoch')])\n",
        "        self.checkpoints_path = checkpoints_path\n",
        "        self.keep_old_checkpoints = keep_old_checkpoints\n",
        "\n",
        "    def register(self, trainer: Trainer):\n",
        "        self.trainer = trainer\n",
        "\n",
        "    def epoch(self, epoch_index):\n",
        "        if not self.keep_old_checkpoints:\n",
        "            self._clear(self.last_pattern.format('*', '*'))\n",
        "        dest = os.path.join(self.checkpoints_path,\n",
        "                            self.last_pattern.format('{}', '{:06}'.format(self.trainer.cur_nimg // 1000)))\n",
        "        for model, optimizer, name in [(self.trainer.generator, self.trainer.optimizer_g, 'generator'),\n",
        "                                       (self.trainer.discriminator, self.trainer.optimizer_d, 'discriminator')]:\n",
        "            torch.save({'cur_nimg': self.trainer.cur_nimg, 'model': model.state_dict(),\n",
        "                        'optimizer': optimizer.state_dict()}, dest.format(name))\n",
        "\n",
        "    def end(self, *args):\n",
        "        self.epoch(*args)\n",
        "\n",
        "    def _clear(self, pattern):\n",
        "        pattern = os.path.join(self.checkpoints_path, pattern)\n",
        "        for file_name in glob(pattern):\n",
        "            os.remove(file_name)\n",
        "\n",
        "\n",
        "class EvalDiscriminator(Plugin):\n",
        "    def __init__(self, create_dataloader_fun, output_snapshot_ticks):\n",
        "        super().__init__([(1, 'epoch')])\n",
        "        self.create_dataloader_fun = create_dataloader_fun\n",
        "        self.output_snapshot_ticks = output_snapshot_ticks\n",
        "\n",
        "    def register(self, trainer):\n",
        "        self.trainer = trainer\n",
        "        self.trainer.stats['memorization'] = {\n",
        "            'log_name': 'memorization',\n",
        "            'log_epoch_fields': ['{val:.2f}', '{epoch:.2f}'],\n",
        "            'val': float('nan'), 'epoch': 0,\n",
        "        }\n",
        "\n",
        "    def epoch(self, epoch_index):\n",
        "        if epoch_index % self.output_snapshot_ticks != 0:\n",
        "            return\n",
        "        values = []\n",
        "        with torch.no_grad():\n",
        "            i = 0\n",
        "            for data in self.create_dataloader_fun(min(self.trainer.stats['minibatch_size'], 1024), False,\n",
        "                                                   self.trainer.dataset.model_depth, self.trainer.dataset.alpha):\n",
        "                d_real, _, _ = self.trainer.discriminator(cudize(data))\n",
        "                values.append(d_real.mean().item())\n",
        "                i += 1\n",
        "        values = np.array(values).mean()\n",
        "        self.trainer.stats['memorization']['val'] = values\n",
        "        self.trainer.stats['memorization']['epoch'] = epoch_index\n",
        "\n",
        "\n",
        "# TODO\n",
        "class OutputGenerator(Plugin):\n",
        "\n",
        "    def __init__(self, sample_fn, checkpoints_dir: str, seq_len: int, max_freq: float,\n",
        "                 samples_count: int = 8, output_snapshot_ticks: int = 25, old_weight: float = 0.59):\n",
        "        super().__init__([(1, 'epoch')])\n",
        "        self.old_weight = old_weight\n",
        "        self.sample_fn = sample_fn\n",
        "        self.samples_count = samples_count\n",
        "        self.checkpoints_dir = checkpoints_dir\n",
        "        self.seq_len = seq_len\n",
        "        self.max_freq = max_freq\n",
        "        self.my_g_clone = None\n",
        "        self.output_snapshot_ticks = output_snapshot_ticks\n",
        "\n",
        "    @staticmethod\n",
        "    def flatten_params(model):\n",
        "        return deepcopy(list(p.data for p in model.parameters()))\n",
        "\n",
        "    @staticmethod\n",
        "    def load_params(flattened, model):\n",
        "        for p, avg_p in zip(model.parameters(), flattened):\n",
        "            p.data.copy_(avg_p)\n",
        "\n",
        "    def register(self, trainer):\n",
        "        self.trainer = trainer\n",
        "        self.my_g_clone = self.flatten_params(self.trainer.generator)\n",
        "\n",
        "    @staticmethod\n",
        "    def running_mean(x, n=8):\n",
        "        return pd.Series(x).rolling(window=n).mean().values\n",
        "\n",
        "    @staticmethod\n",
        "    def get_images(frequency, epoch, generated, my_range=range):\n",
        "        num_channels = generated.shape[1]\n",
        "        seq_len = generated.shape[2]\n",
        "        t = np.linspace(0, seq_len / frequency, seq_len)\n",
        "        f = np.fft.rfftfreq(seq_len, d=1. / frequency)\n",
        "        images = []\n",
        "        for index in my_range(len(generated)):\n",
        "            fig, (axs) = plt.subplots(num_channels, 4)\n",
        "            if num_channels == 1:\n",
        "                axs = axs.reshape(1, -1)\n",
        "            fig.set_figheight(40)\n",
        "            fig.set_figwidth(40)\n",
        "            for ch in range(num_channels):\n",
        "                data = generated[index, ch, :]\n",
        "                axs[ch][0].plot(t, data, color=(0.8, 0, 0, 0.5), label='time domain')\n",
        "                axs[ch][1].plot(f, np.abs(np.fft.rfft(data)), color=(0.8, 0, 0, 0.5), label='freq domain')\n",
        "                axs[ch][2].plot(f, OutputGenerator.running_mean(np.abs(np.fft.rfft(data))),\n",
        "                                color=(0.8, 0, 0, 0.5), label='freq domain(smooth)')\n",
        "                axs[ch][3].semilogy(f, np.abs(np.fft.rfft(data)), color=(0.8, 0, 0, 0.5), label='freq domain(log)')\n",
        "                axs[ch][0].set_ylim([-1.1, 1.1])\n",
        "                axs[ch][0].legend()\n",
        "                axs[ch][1].legend()\n",
        "                axs[ch][2].legend()\n",
        "                axs[ch][3].legend()\n",
        "            fig.suptitle('epoch: {}, sample: {}'.format(epoch, index))\n",
        "            fig.canvas.draw()\n",
        "            image = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n",
        "            image = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "            images.append(image)\n",
        "            plt.close(fig)\n",
        "        return images\n",
        "\n",
        "    def epoch(self, epoch_index):\n",
        "        for p, avg_p in zip(self.trainer.generator.parameters(), self.my_g_clone):\n",
        "            avg_p.mul_(self.old_weight).add_((1.0 - self.old_weight) * p.data)\n",
        "        if epoch_index % self.output_snapshot_ticks == 0:\n",
        "            z = next(self.sample_fn(self.samples_count))\n",
        "            gen_input = cudize(z)\n",
        "            original_param = self.flatten_params(self.trainer.generator)\n",
        "            self.load_params(self.my_g_clone, self.trainer.generator)\n",
        "            dest = os.path.join(self.checkpoints_dir, SaverPlugin.last_pattern.format('smooth_generator',\n",
        "                                                                                      '{:06}'.format(\n",
        "                                                                                          self.trainer.cur_nimg // 1000)))\n",
        "            torch.save({'cur_nimg': self.trainer.cur_nimg, 'model': self.trainer.generator.state_dict()}, dest)\n",
        "            out = generate_samples(self.trainer.generator, gen_input)\n",
        "            self.load_params(original_param, self.trainer.generator)\n",
        "            frequency = self.max_freq * out.shape[2] / self.seq_len\n",
        "            images = self.get_images(frequency, epoch_index, out)\n",
        "            for i, image in enumerate(images):\n",
        "                misc.imsave(os.path.join(self.checkpoints_dir, '{}_{}.png'.format(epoch_index, i)), image)\n",
        "\n",
        "\n",
        "class TeeLogger(Logger):\n",
        "\n",
        "    def __init__(self, log_file, exp_name, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.log_file = open(log_file, 'a', 1)\n",
        "        self.exp_name = exp_name\n",
        "\n",
        "    def log(self, msg):\n",
        "        print(self.exp_name, msg, flush=True)\n",
        "        self.log_file.write(msg + '\\n')\n",
        "\n",
        "    def epoch(self, epoch_idx):\n",
        "        self._log_all('log_epoch_fields')\n",
        "\n",
        "\n",
        "class WatchSingularValues(Plugin):\n",
        "    def __init__(self, network, one_divided_two: float = 10.0, output_snapshot_ticks: int = 20):\n",
        "        super().__init__([(1, 'epoch')])\n",
        "        self.network = network\n",
        "        self.one_divided_two = one_divided_two\n",
        "        self.output_snapshot_ticks = output_snapshot_ticks\n",
        "\n",
        "    def register(self, trainer):\n",
        "        self.trainer = trainer\n",
        "\n",
        "    def epoch(self, epoch_index):\n",
        "        if epoch_index % self.output_snapshot_ticks != 0:\n",
        "            return\n",
        "        for module in self.network.modules:\n",
        "            if isinstance(module, torch.nn.Conv1d):\n",
        "                weight = module.weight.data.cpu().numpy()\n",
        "                _, s, _ = randomized_svd(weight.reshape(weight.shape[0], -1), n_components=2)\n",
        "                if abs(s[0] / s[1]) > self.one_divided_two:\n",
        "                    raise ValueError(module)\n",
        "\n",
        "\n",
        "class SlicedWDistance(Plugin):\n",
        "    def __init__(self, progression_scale: int, output_snapshot_ticks: int, patches_per_item: int = 16,\n",
        "                 patch_size: int = 49, max_items: int = 1024, number_of_projections: int = 512,\n",
        "                 dir_repeats: int = 4, dirs_per_repeat: int = 128):\n",
        "        super().__init__([(1, 'epoch')])\n",
        "        self.output_snapshot_ticks = output_snapshot_ticks\n",
        "        self.progression_scale = progression_scale\n",
        "        self.patches_per_item = patches_per_item\n",
        "        self.patch_size = patch_size\n",
        "        self.max_items = max_items\n",
        "        self.number_of_projections = number_of_projections\n",
        "        self.dir_repeats = dir_repeats\n",
        "        self.dirs_per_repeat = dirs_per_repeat\n",
        "\n",
        "    def register(self, trainer):\n",
        "        self.trainer = trainer\n",
        "        self.trainer.stats['swd'] = {\n",
        "            'log_name': 'swd',\n",
        "            'log_epoch_fields': ['{val:.2f}', '{epoch:.2f}'],\n",
        "            'val': float('nan'), 'epoch': 0,\n",
        "        }\n",
        "\n",
        "    def sliced_wasserstein(self, A, B):\n",
        "        results = []\n",
        "        for repeat in range(self.dir_repeats):\n",
        "            dirs = torch.randn(A.shape[1], self.dirs_per_repeat)  # (descriptor_component, direction)\n",
        "            dirs /= torch.sqrt(\n",
        "                (dirs * dirs).sum(dim=0, keepdim=True) + EPSILON)  # normalize descriptor components for each direction\n",
        "            projA = torch.matmul(A, dirs)  # (neighborhood, direction)\n",
        "            projB = torch.matmul(B, dirs)\n",
        "            projA = torch.sort(projA, dim=0)[0]  # sort neighborhood projections for each direction\n",
        "            projB = torch.sort(projB, dim=0)[0]\n",
        "            dists = (projA - projB).abs()  # pointwise wasserstein distances\n",
        "            results.append(dists.mean())  # average over neighborhoods and directions\n",
        "        return torch.mean(torch.stack(results)).item()  # average over repeats\n",
        "\n",
        "    def epoch(self, epoch_index):\n",
        "        if epoch_index % self.output_snapshot_ticks != 0:\n",
        "            return\n",
        "        gc.collect()\n",
        "        all_fakes = []\n",
        "        all_reals = []\n",
        "        with torch.no_grad():\n",
        "            remaining_items = self.max_items\n",
        "            while remaining_items > 0:\n",
        "                z = next(self.trainer.random_latents_generator)\n",
        "                fake_latents_in = cudize(z)\n",
        "                all_fakes.append(self.trainer.generator(fake_latents_in)[0]['x'].data.cpu())\n",
        "                if all_fakes[-1].size(2) < self.patch_size:\n",
        "                    break\n",
        "                remaining_items -= all_fakes[-1].size(0)\n",
        "            all_fakes = torch.cat(all_fakes, dim=0)\n",
        "            remaining_items = self.max_items\n",
        "            while remaining_items > 0:\n",
        "                all_reals.append(next(self.trainer.dataiter)['x'])\n",
        "                if all_reals[-1].size(2) < self.patch_size:\n",
        "                    break\n",
        "                remaining_items -= all_reals[-1].size(0)\n",
        "            all_reals = torch.cat(all_reals, dim=0)\n",
        "        swd = self.get_descriptors(all_fakes, all_reals)\n",
        "        if len(swd) > 0:\n",
        "            swd.append(np.array(swd).mean())\n",
        "        self.trainer.stats['swd']['val'] = swd\n",
        "        self.trainer.stats['swd']['epoch'] = epoch_index\n",
        "\n",
        "    def get_descriptors(self, batch1, batch2):\n",
        "        b, c, t_max = batch1.shape\n",
        "        t = t_max\n",
        "        num_levels = 0\n",
        "        while t >= self.patch_size:\n",
        "            num_levels += 1\n",
        "            t //= self.progression_scale\n",
        "        swd = []\n",
        "        for level in range(num_levels):\n",
        "            both_descriptors = [None, None]\n",
        "            batchs = [batch1, batch2]\n",
        "            for i in range(2):\n",
        "                descriptors = []\n",
        "                max_index = batchs[i].shape[2] - self.patch_size\n",
        "                for j in range(b):\n",
        "                    for k in range(self.patches_per_item):\n",
        "                        rand_index = np.random.randint(0, max_index)\n",
        "                        descriptors.append(batchs[i][j, :, rand_index:rand_index + self.patch_size])\n",
        "                descriptors = torch.stack(descriptors, dim=0)  # N, c, patch_size\n",
        "                descriptors = descriptors.reshape((-1, c))\n",
        "                descriptors -= torch.mean(descriptors, dim=0, keepdim=True)\n",
        "                descriptors /= torch.std(descriptors, dim=0, keepdim=True) + EPSILON\n",
        "                both_descriptors[i] = descriptors\n",
        "                batchs[i] = batchs[i][:, :, ::self.progression_scale]\n",
        "            swd.append(self.sliced_wasserstein(both_descriptors[0], both_descriptors[1]))\n",
        "        return swd\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a9Czff_yhYz8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "outputId": "dbfd9104-15f7-49ac-efaf-d245d0b25ece"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import signal\n",
        "import subprocess\n",
        "import time\n",
        "from functools import partial\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import yaml\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "default_params = dict(\n",
        "    result_dir='results',\n",
        "    exp_name='',\n",
        "    lr=0.001,  # generator's learning rate\n",
        "    total_kimg=6000,\n",
        "    resume_network='',  # 001-test/network-snapshot-{}-000025.dat\n",
        "    num_data_workers=0,\n",
        "    random_seed=1373,\n",
        "    grad_lambda=10.0,  # must set it to zero to disable gp loss (even for non wgan based losses)\n",
        "    iwass_drift_epsilon=0.001,\n",
        "    iwass_target=1.0,\n",
        "    feature_matching_lambda=0.0,\n",
        "    loss_type='wgan_gp',  # wgan_gp, hinge, wgan_theirs, rsgan, rasgan, rahinge\n",
        "    cuda_device=0,\n",
        "    ttur=False,\n",
        "    config_file=None,\n",
        "    fmap_base=1024,\n",
        "    fmap_max=256,\n",
        "    fmap_min=64,\n",
        "    equalized=True,\n",
        "    kernel_size=3,\n",
        "    self_attention_layers=[],  # starts from 0 or null (for G it means putting it after ith layer)\n",
        "    random_multiply=False,\n",
        "    lr_rampup_kimg=0.0,  # set to 0 to disable (used to be 40)\n",
        "    z_distribution='normal',  # or 'bernoulli' or 'censored'\n",
        "    init='kaiming_normal',  # or xavier_uniform or orthogonal\n",
        "    act_alpha=0.2,\n",
        "    residual=False,\n",
        "    calc_swd=False,\n",
        "    separable=False,\n",
        "    num_classes=0,\n",
        "    deep=False\n",
        ")\n",
        "\n",
        "\n",
        "class InfiniteRandomSampler(SubsetRandomSampler):\n",
        "    def __iter__(self):\n",
        "        while True:\n",
        "            it = super().__iter__()\n",
        "            for x in it:\n",
        "                yield x\n",
        "\n",
        "\n",
        "def load_models(resume_network, result_dir, logger):\n",
        "    logger.log('Resuming {}'.format(resume_network))\n",
        "    dest = os.path.join(result_dir, resume_network)\n",
        "    generator, g_optimizer, g_cur_img = load_model(dest.format('generator'), True)\n",
        "    discriminator, d_optimizer, d_cur_img = load_model(dest.format('discriminator'), True)\n",
        "    assert g_cur_img == d_cur_img\n",
        "    return generator, g_optimizer, discriminator, d_optimizer, g_cur_img\n",
        "\n",
        "\n",
        "def thread_exit(_signal, frame):\n",
        "    exit(0)\n",
        "\n",
        "\n",
        "def worker_init(x):\n",
        "    signal.signal(signal.SIGINT, thread_exit)\n",
        "\n",
        "\n",
        "def main(params):\n",
        "    dataset_params = params['EEGDataset']\n",
        "    dataset, val_dataset = EEGDataset.from_config(**dataset_params)\n",
        "    if params['config_file'] and params['exp_name'] == '':\n",
        "        params['exp_name'] = params['config_file'].split('/')[-1].split('.')[0]\n",
        "    result_dir = create_result_subdir(params['result_dir'], params['exp_name'])\n",
        "\n",
        "    losses = ['G_loss', 'D_loss']\n",
        "    stats_to_log = ['tick_stat', 'kimg_stat']\n",
        "    stats_to_log.extend(['depth', 'alpha', 'minibatch_size'])\n",
        "    stats_to_log.extend(['time', 'sec.tick', 'sec.kimg'] + losses)\n",
        "    if dataset_params['validation_ratio'] > 0:\n",
        "        stats_to_log.extend(['memorization.val', 'memorization.epoch'])\n",
        "    if params['calc_swd']:\n",
        "        stats_to_log.extend(['swd.val', 'swd.epoch'])\n",
        "\n",
        "    logger = TeeLogger(os.path.join(result_dir, 'log.txt'), params['exp_name'], stats_to_log, [(1, 'epoch')])\n",
        "    shared_model_params = dict(initial_kernel_size=dataset.initial_kernel_size, num_rgb_channels=dataset.num_channels,\n",
        "                               fmap_base=params['fmap_base'], fmap_max=params['fmap_max'], fmap_min=params['fmap_min'],\n",
        "                               kernel_size=params['kernel_size'], self_attention_layers=params['self_attention_layers'],\n",
        "                               progression_scale_up=dataset.progression_scale_up,\n",
        "                               progression_scale_down=dataset.progression_scale_down, residual=params['residual'],\n",
        "                               separable=params['separable'], equalized=params['equalized'], init=params['init'],\n",
        "                               act_alpha=params['act_alpha'], num_classes=params['num_classes'], deep=params['deep'])\n",
        "    for n in ('Generator', 'Discriminator'):\n",
        "        p = params[n]\n",
        "        if p['spectral']:\n",
        "            if p['act_norm'] == 'pixel':\n",
        "                logger.log('Warning, setting pixel normalization with spectral norm in {} is not a good idea'.format(n))\n",
        "            if params['equalized']:\n",
        "                logger.log('Warning, setting equalized weights with spectral norm in {} is not a good idea'.format(n))\n",
        "    if params['DepthManager']['disable_progression'] and not params['residual']:\n",
        "        logger.log('Warning, you have set the residual to false and disabled the progression')\n",
        "    if params['Discriminator']['act_norm'] is not None:\n",
        "        logger.log('Warning, you are using an activation normalization in discriminator')\n",
        "    generator = Generator(**shared_model_params, z_distribution=params['z_distribution'], **params['Generator'])\n",
        "    discriminator = Discriminator(**shared_model_params, **params['Discriminator'])\n",
        "\n",
        "    def rampup(cur_nimg):\n",
        "        if cur_nimg < params['lr_rampup_kimg'] * 1000:\n",
        "            p = max(0.0, 1 - cur_nimg / (params['lr_rampup_kimg'] * 1000))\n",
        "            return np.exp(-p * p * 5.0)\n",
        "        else:\n",
        "            return 1.0\n",
        "\n",
        "    if params['ttur']:\n",
        "        params['Adam']['betas'] = (0, 0.9)\n",
        "\n",
        "    def get_optimizers(g_lr):\n",
        "        d_lr = g_lr\n",
        "        if params['ttur']:\n",
        "            d_lr *= 4.0\n",
        "        opt_g = Adam(trainable_params(generator), g_lr, **params['Adam'])\n",
        "        opt_d = Adam(trainable_params(discriminator), d_lr, **params['Adam'])\n",
        "        if params['lr_rampup_kimg'] > 0:\n",
        "            lr_scheduler_g = LambdaLR(opt_g, rampup, -1)\n",
        "            lr_scheduler_d = LambdaLR(opt_d, rampup, -1)\n",
        "            return opt_g, opt_d, lr_scheduler_g, lr_scheduler_d\n",
        "        return opt_g, opt_d, None, None\n",
        "\n",
        "    if params['resume_network'] != '':\n",
        "        logger.log('resuming networks')\n",
        "        generator_state, opt_g_state, discriminator_state, opt_d_state, train_cur_img = load_models(\n",
        "            params['resume_network'], params['result_dir'], logger)\n",
        "        generator.load_state_dict(generator_state)\n",
        "        discriminator.load_state_dict(discriminator_state)\n",
        "        opt_g, opt_d, _, _ = get_optimizers(params['lr'])\n",
        "        opt_g.load_state_dict(opt_g_state)\n",
        "        opt_d.load_state_dict(opt_d_state)\n",
        "    else:\n",
        "        opt_g = None\n",
        "        opt_d = None\n",
        "        train_cur_img = 0\n",
        "    latent_size = generator.input_latent_size\n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "    generator = cudize(generator)\n",
        "    discriminator = cudize(discriminator)\n",
        "    if opt_g is not None:\n",
        "        for opt in [opt_g, opt_d]:\n",
        "            for state in opt.state.values():\n",
        "                for k, v in state.items():\n",
        "                    if torch.is_tensor(v):\n",
        "                        state[k] = cudize(v)\n",
        "    d_loss_fun = partial(discriminator_loss, loss_type=params['loss_type'], iwass_target=params['iwass_target'],\n",
        "                         iwass_drift_epsilon=params['iwass_drift_epsilon'], grad_lambda=params['grad_lambda'])\n",
        "    g_loss_fun = partial(generator_loss, random_multiply=params['random_multiply'], loss_type=params['loss_type'],\n",
        "                         feature_matching_lambda=params['feature_matching_lambda'])\n",
        "    max_depth = generator.max_depth\n",
        "\n",
        "    logger.log('exp name: {}'.format(params['exp_name']))\n",
        "    try:\n",
        "        logger.log('commit hash: {}'.format(subprocess.check_output(['git', 'describe', '--always']).strip()))\n",
        "    except:\n",
        "        logger.log('current time: {}'.format(time.time()))\n",
        "    logger.log('training dataset shape: {}'.format(dataset.shape))\n",
        "    if dataset_params['validation_ratio'] > 0:\n",
        "        logger.log('val dataset shape: {}'.format(val_dataset.shape))\n",
        "    logger.log('Total number of parameters in Generator: {}'.format(num_params(generator)))\n",
        "    logger.log('Total number of parameters in Discriminator: {}'.format(num_params(discriminator)))\n",
        "\n",
        "    mb_def = params['DepthManager']['minibatch_default']\n",
        "\n",
        "    collate_real = get_collate_real(dataset.end_sampling_freq, dataset.seq_len)\n",
        "    collate_fake = get_collate_fake(latent_size, params['z_distribution'], collate_real)\n",
        "\n",
        "    def get_dataloader(minibatch_size, is_training=True, depth=0, alpha=1, is_real=True):\n",
        "        ds = dataset if is_training else val_dataset\n",
        "        shared_dataloader_params = {'dataset': ds, 'batch_size': minibatch_size, 'drop_last': True,\n",
        "                                    'worker_init_fn': worker_init, 'num_workers': params['num_data_workers'],\n",
        "                                    'collate_fn': collate_real if is_real else collate_fake}\n",
        "        if not is_training:\n",
        "            ds.model_depth = depth\n",
        "            ds.alpha = alpha\n",
        "            # NOTE you must drop last in order to be compatible with D.stats layer\n",
        "            return DataLoader(**shared_dataloader_params, shuffle=True)\n",
        "        return DataLoader(**shared_dataloader_params, sampler=InfiniteRandomSampler(list(range(len(ds)))))\n",
        "\n",
        "    # NOTE you can not put the if inside your function (a function should either return or yield)\n",
        "    def get_random_latents(minibatch_size, is_training=True, depth=0, alpha=1):\n",
        "        while True:\n",
        "            yield {'z': cudize(random_latents(minibatch_size, latent_size, params['z_distribution']))}\n",
        "\n",
        "    trainer = Trainer(discriminator, generator, d_loss_fun, g_loss_fun, dataset, get_random_latents(mb_def),\n",
        "                      train_cur_img, opt_g, opt_d, **params['Trainer'])\n",
        "    dm = DepthManager(get_dataloader, get_random_latents, max_depth, params['Trainer']['tick_kimg_default'],\n",
        "                      get_optimizers, params['lr'], **params['DepthManager'])\n",
        "    trainer.register_plugin(dm)\n",
        "    for i, loss_name in enumerate(losses):\n",
        "        trainer.register_plugin(EfficientLossMonitor(i, loss_name, **params['EfficientLossMonitor']))\n",
        "    trainer.register_plugin(SaverPlugin(result_dir, **params['SaverPlugin']))\n",
        "    trainer.register_plugin(\n",
        "        OutputGenerator(lambda x: get_random_latents(x), result_dir, dataset.seq_len,\n",
        "                        dataset.end_sampling_freq, **params['OutputGenerator']))\n",
        "    if dataset_params['validation_ratio'] > 0:\n",
        "        trainer.register_plugin(EvalDiscriminator(get_dataloader, params['SaverPlugin']['network_snapshot_ticks']))\n",
        "    if params['calc_swd']:\n",
        "        trainer.register_plugin(\n",
        "            SlicedWDistance(dataset.progression_scale, params['SaverPlugin']['network_snapshot_ticks'],\n",
        "                            **params['SlicedWDistance']))\n",
        "    trainer.register_plugin(AbsoluteTimeMonitor())\n",
        "    if params['Generator']['spectral']:\n",
        "        trainer.register_plugin(WatchSingularValues(generator, **params['WatchSingularValues']))\n",
        "    if params['Discriminator']['spectral']:\n",
        "        trainer.register_plugin(WatchSingularValues(discriminator, **params['WatchSingularValues']))\n",
        "    trainer.register_plugin(logger)\n",
        "    params['EEGDataset']['progression_scale_up'] = dataset.progression_scale_up\n",
        "    params['EEGDataset']['progression_scale_down'] = dataset.progression_scale_down\n",
        "    params['EEGDataset']['picked_channels'] = dataset.picked_channels\n",
        "    params['DepthManager']['minibatch_override'] = dm.minibatch_override\n",
        "    params['DepthManager']['tick_kimg_override'] = dm.tick_kimg_override\n",
        "    params['DepthManager']['training_kimg_override'] = dm.training_kimg_override\n",
        "    params['DepthManager']['transition_kimg_override'] = dm.transition_kimg_override\n",
        "    yaml.dump(params, open(os.path.join(result_dir, 'conf.yml'), 'w'))\n",
        "    trainer.run(params['total_kimg'])\n",
        "    del trainer\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    need_arg_classes = [Trainer, Generator, Discriminator, Adam, OutputGenerator, DepthManager, SaverPlugin,\n",
        "                        SlicedWDistance, EfficientLossMonitor, EvalDiscriminator, EEGDataset, WatchSingularValues]\n",
        "    main(parse_config(default_params, need_arg_classes, True, False))\n",
        "    print('training finished!')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading train dataset from file\n",
            "loading val dataset from file\n",
            " exp name: \n",
            " current time: 1554705972.6557548\n",
            " training dataset shape: (9501, 5, 1920)\n",
            " val dataset shape: (1155, 5, 1920)\n",
            " Total number of parameters in Generator: 2976360\n",
            " Total number of parameters in Discriminator: 2978497\n",
            " tick:     1\tkimg:    5.120\tdepth: 0\talpha: 1.00\tminibatch_size: 256\ttime: 0:00:12.304734\tsec.tick: 12.3\tsec.kimg: 2.4\tG_loss: 0.0427\tD_loss: 8.7106\tmemorization.val: nan\tmemorization.epoch: 0\n",
            " tick:     2\tkimg:   10.240\tdepth: 0\talpha: 1.00\tminibatch_size: 256\ttime: 0:00:18.866216\tsec.tick: 6.6 \tsec.kimg: 1.3\tG_loss: 0.1024\tD_loss: 6.8374\tmemorization.val: nan\tmemorization.epoch: 0\n",
            " tick:     3\tkimg:   15.360\tdepth: 0\talpha: 1.00\tminibatch_size: 256\ttime: 0:00:25.456018\tsec.tick: 6.6 \tsec.kimg: 1.3\tG_loss: 0.1803\tD_loss: 4.3948\tmemorization.val: nan\tmemorization.epoch: 0\n",
            " tick:     4\tkimg:   20.480\tdepth: 0\talpha: 1.00\tminibatch_size: 256\ttime: 0:00:32.020246\tsec.tick: 6.6 \tsec.kimg: 1.3\tG_loss: 0.3283\tD_loss: 2.3556\tmemorization.val: nan\tmemorization.epoch: 0\n",
            " tick:     5\tkimg:   25.600\tdepth: 0\talpha: 1.00\tminibatch_size: 256\ttime: 0:00:38.607832\tsec.tick: 6.6 \tsec.kimg: 1.3\tG_loss: 0.5276\tD_loss: 1.9203\tmemorization.val: nan\tmemorization.epoch: 0\n",
            " tick:     6\tkimg:   30.720\tdepth: 0\talpha: 1.00\tminibatch_size: 256\ttime: 0:00:45.175789\tsec.tick: 6.6 \tsec.kimg: 1.3\tG_loss: 0.6037\tD_loss: 1.2213\tmemorization.val: nan\tmemorization.epoch: 0\n",
            " tick:     7\tkimg:   35.840\tdepth: 0\talpha: 1.00\tminibatch_size: 256\ttime: 0:00:51.745298\tsec.tick: 6.6 \tsec.kimg: 1.3\tG_loss: 0.5661\tD_loss: 0.8560\tmemorization.val: nan\tmemorization.epoch: 0\n",
            " tick:     8\tkimg:   40.960\tdepth: 0\talpha: 1.00\tminibatch_size: 256\ttime: 0:00:58.305777\tsec.tick: 6.6 \tsec.kimg: 1.3\tG_loss: 1.1040\tD_loss: 0.9078\tmemorization.val: nan\tmemorization.epoch: 0\n",
            " tick:     9\tkimg:   46.080\tdepth: 0\talpha: 1.00\tminibatch_size: 256\ttime: 0:01:04.853075\tsec.tick: 6.5 \tsec.kimg: 1.3\tG_loss: 1.1420\tD_loss: 0.9825\tmemorization.val: nan\tmemorization.epoch: 0\n",
            " tick:    10\tkimg:   51.200\tdepth: 0\talpha: 1.00\tminibatch_size: 256\ttime: 0:01:11.427674\tsec.tick: 6.6 \tsec.kimg: 1.3\tG_loss: 1.2231\tD_loss: 0.8367\tmemorization.val: nan\tmemorization.epoch: 0\n",
            " tick:    11\tkimg:   56.320\tdepth: 0\talpha: 1.00\tminibatch_size: 256\ttime: 0:01:18.018552\tsec.tick: 6.6 \tsec.kimg: 1.3\tG_loss: 1.2851\tD_loss: 0.7418\tmemorization.val: nan\tmemorization.epoch: 0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}