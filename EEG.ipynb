{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EEG.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "HwbqZXezfbQU",
        "colab_type": "code",
        "colab": {},
        "pycharm": {}
      },
      "cell_type": "code",
      "source": [
        "#utils.py\n",
        "import os\n",
        "import yaml\n",
        "import torch\n",
        "import random\n",
        "import inspect\n",
        "import numpy as np\n",
        "from typing import Dict\n",
        "from pickle import load, dump\n",
        "from functools import partial\n",
        "from fractions import Fraction\n",
        "import torch.nn.functional as F\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "EPSILON \u003d 1e-8\n",
        "half_tensor \u003d None\n",
        "\n",
        "\n",
        "def generate_samples(generator, gen_input):\n",
        "    return generator(gen_input)[0][\u0027x\u0027].data.cpu().numpy()\n",
        "\n",
        "\n",
        "def save_pkl(file_name, obj):\n",
        "    with open(file_name, \u0027wb\u0027) as f:\n",
        "        dump(obj, f, protocol\u003d4)\n",
        "\n",
        "\n",
        "def load_pkl(file_name):\n",
        "    if not os.path.exists(file_name):\n",
        "        return None\n",
        "    with open(file_name, \u0027rb\u0027) as f:\n",
        "        return load(f)\n",
        "\n",
        "\n",
        "def get_half(num_latents, latent_size):\n",
        "    global half_tensor\n",
        "    if half_tensor is None or half_tensor.size() !\u003d (num_latents, latent_size):\n",
        "        half_tensor \u003d torch.ones(num_latents, latent_size) * 0.5\n",
        "    return half_tensor\n",
        "\n",
        "\n",
        "def random_latents(num_latents, latent_size, z_distribution\u003d\u0027normal\u0027):\n",
        "    if z_distribution \u003d\u003d \u0027normal\u0027:\n",
        "        return torch.randn(num_latents, latent_size)\n",
        "    elif z_distribution \u003d\u003d \u0027censored\u0027:\n",
        "        return F.relu(torch.randn(num_latents, latent_size))\n",
        "    elif z_distribution \u003d\u003d \u0027bernoulli\u0027:\n",
        "        return torch.bernoulli(get_half(num_latents, latent_size))\n",
        "    else:\n",
        "        raise ValueError()\n",
        "\n",
        "\n",
        "def mkdir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "\n",
        "def create_result_subdir(results_dir, experiment_name, dir_pattern\u003d\u0027{new_num:03}-{exp_name}\u0027):\n",
        "    if not os.path.exists(results_dir):\n",
        "        os.makedirs(results_dir)\n",
        "    fnames \u003d os.listdir(results_dir)\n",
        "    max_num \u003d max(map(int, filter(lambda x: all(y.isdigit() for y in x), (x.split(\u0027-\u0027)[0] for x in fnames))), default\u003d0)\n",
        "    path \u003d os.path.join(results_dir, dir_pattern.format(new_num\u003dmax_num + 1, exp_name\u003dexperiment_name))\n",
        "    os.makedirs(path, exist_ok\u003dFalse)\n",
        "    return path\n",
        "\n",
        "\n",
        "def num_params(net):\n",
        "    model_parameters \u003d trainable_params(net)\n",
        "    return sum([np.prod(p.size()) for p in model_parameters])\n",
        "\n",
        "\n",
        "def generic_arg_parse(x, hinttype\u003dNone):\n",
        "    if hinttype is int or hinttype is float or hinttype is str:\n",
        "        return hinttype(x)\n",
        "    try:\n",
        "        for _ in range(2):\n",
        "            x \u003d x.strip(\u0027\\\u0027\u0027).strip(\"\\\"\")\n",
        "        __special_tmp \u003d eval(x, {}, {})\n",
        "    except:  # the string contained some name - probably path, treat as string\n",
        "        __special_tmp \u003d x  # treat as string\n",
        "    return __special_tmp\n",
        "\n",
        "\n",
        "def create_params(classes, excludes\u003dNone, overrides\u003dNone):\n",
        "    params \u003d {}\n",
        "    if not excludes:\n",
        "        excludes \u003d {}\n",
        "    if not overrides:\n",
        "        overrides \u003d {}\n",
        "    for cls in classes:\n",
        "        nm \u003d cls.__name__\n",
        "        params[nm] \u003d {\n",
        "            k: (v.default if nm not in overrides or k not in overrides[nm] else overrides[nm][k])\n",
        "            for k, v in dict(inspect.signature(cls.__init__).parameters).items()\n",
        "            if v.default !\u003d inspect._empty and (nm not in excludes or k not in excludes[nm])\n",
        "        }\n",
        "    return params\n",
        "\n",
        "\n",
        "def get_structured_params(params):\n",
        "    new_params \u003d {}\n",
        "    for p in params:\n",
        "        if \u0027.\u0027 in p:\n",
        "            [cls, attr] \u003d p.split(\u0027.\u0027, 1)\n",
        "            if cls not in new_params:\n",
        "                new_params[cls] \u003d {}\n",
        "            new_params[cls][attr] \u003d params[p]\n",
        "        else:\n",
        "            new_params[p] \u003d params[p]\n",
        "    return new_params\n",
        "\n",
        "\n",
        "def cudize(thing):\n",
        "    if thing is None:\n",
        "        return None\n",
        "    has_cuda \u003d torch.cuda.is_available()\n",
        "    if not has_cuda:\n",
        "        return thing\n",
        "    if isinstance(thing, (list, tuple)):\n",
        "        return [item.cuda() for item in thing]\n",
        "    if isinstance(thing, dict):\n",
        "        return {k: v.cuda() for k, v in thing.items()}\n",
        "    return thing.cuda()\n",
        "\n",
        "\n",
        "def trainable_params(model):\n",
        "    return filter(lambda p: p.requires_grad, model.parameters())\n",
        "\n",
        "\n",
        "def pixel_norm(h):\n",
        "    mean \u003d torch.mean(h * h, dim\u003d1, keepdim\u003dTrue)\n",
        "    dom \u003d torch.rsqrt(mean + EPSILON)\n",
        "    return h * dom\n",
        "\n",
        "\n",
        "def simple_argparser(default_params):\n",
        "    parser \u003d ArgumentParser()\n",
        "    for k in default_params:\n",
        "        parser.add_argument(\u0027--{}\u0027.format(k), type\u003dpartial(generic_arg_parse, hinttype\u003dtype(default_params[k])))\n",
        "    parser.set_defaults(**default_params)\n",
        "    return get_structured_params(vars(parser.parse_args()))\n",
        "\n",
        "\n",
        "def enable_benchmark():\n",
        "    torch.backends.cudnn.benchmark \u003d True  # for fast training(if network input size is almost constant)\n",
        "\n",
        "\n",
        "def load_model(model_path, return_all\u003dFalse):\n",
        "    state \u003d torch.load(model_path, map_location\u003d\u0027cpu\u0027)\n",
        "    if not return_all:\n",
        "        return state[\u0027model\u0027]\n",
        "    return state[\u0027model\u0027], state[\u0027optimizer\u0027], state[\u0027cur_nimg\u0027]\n",
        "\n",
        "\n",
        "def parse_config(default_params, need_arg_classes, exclude_adam\u003dTrue, read_cli\u003dTrue):\n",
        "    parser \u003d ArgumentParser()\n",
        "    if exclude_adam:\n",
        "        excludes \u003d {\u0027Adam\u0027: {\u0027lr\u0027, \u0027amsgrad\u0027}}\n",
        "        default_overrides \u003d {\u0027Adam\u0027: {\u0027betas\u0027: (0.0, 0.99)}}\n",
        "        auto_args \u003d create_params(need_arg_classes, excludes, default_overrides)\n",
        "    else:\n",
        "        auto_args \u003d create_params(need_arg_classes)\n",
        "    for k in default_params:\n",
        "        parser.add_argument(\u0027--{}\u0027.format(k), type\u003dpartial(generic_arg_parse, hinttype\u003dtype(default_params[k])))\n",
        "    for cls in auto_args:\n",
        "        group \u003d parser.add_argument_group(cls, \u0027Arguments for initialization of class {}\u0027.format(cls))\n",
        "        for k in auto_args[cls]:\n",
        "            name \u003d \u0027{}.{}\u0027.format(cls, k)\n",
        "            group.add_argument(\u0027--{}\u0027.format(name), type\u003dgeneric_arg_parse)\n",
        "            default_params[name] \u003d auto_args[cls][k]\n",
        "    parser.set_defaults(**default_params)\n",
        "    if read_cli:\n",
        "        params \u003d vars(parser.parse_args())\n",
        "    else:\n",
        "        params \u003d default_params\n",
        "    if params[\u0027config_file\u0027]:\n",
        "        print(\u0027loading config_file\u0027)\n",
        "        with open(params[\u0027config_file\u0027]) as f:\n",
        "            params \u003d _update_params(params, yaml.load(f))\n",
        "    params \u003d get_structured_params(params)\n",
        "    random.seed(params[\u0027random_seed\u0027])\n",
        "    np.random.seed(params[\u0027random_seed\u0027])\n",
        "    torch.manual_seed(params[\u0027random_seed\u0027])\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.set_device(params[\u0027cuda_device\u0027])\n",
        "        torch.cuda.manual_seed_all(params[\u0027random_seed\u0027])\n",
        "        enable_benchmark()\n",
        "    return params\n",
        "\n",
        "\n",
        "def _update_params(params: Dict, given_conf: Dict):\n",
        "    for k, v in given_conf.items():\n",
        "        if isinstance(v, dict):\n",
        "            for kk, vv in v.items():\n",
        "                params[\u0027{}.{}\u0027.format(k, kk)] \u003d vv\n",
        "        else:\n",
        "            params[k] \u003d v\n",
        "    return params\n",
        "\n",
        "\n",
        "def upsample_signal(signal, upsample_factor):\n",
        "    return F.interpolate(signal, scale_factor\u003dupsample_factor, mode\u003d\u0027linear\u0027, align_corners\u003dFalse)\n",
        "\n",
        "\n",
        "def downsample_signal(signal, downsample_factor):\n",
        "    return F.avg_pool1d(signal, downsample_factor, downsample_factor, 0, False, True)\n",
        "\n",
        "\n",
        "def expand3d(signal):\n",
        "    orig_dim \u003d signal.dim()\n",
        "    if orig_dim \u003d\u003d 2:\n",
        "        return signal[None]\n",
        "    if orig_dim \u003d\u003d 1:\n",
        "        return signal[None, None]\n",
        "    return signal\n",
        "\n",
        "\n",
        "def resample_signal(signal, signal_freq, desired_freq, pytorch\u003dFalse):\n",
        "    if isinstance(signal, np.ndarray):\n",
        "        new_signal \u003d torch.from_numpy(signal)\n",
        "    else:\n",
        "        new_signal \u003d signal\n",
        "    orig_dim \u003d new_signal.dim()\n",
        "    if orig_dim \u003d\u003d 2:\n",
        "        new_signal \u003d new_signal[None]\n",
        "    if orig_dim \u003d\u003d 1:\n",
        "        new_signal \u003d new_signal[None, None]\n",
        "    if pytorch and (orig_dim \u003c 3 or signal.shape[2] \u003d\u003d 1):\n",
        "        ratio \u003d desired_freq / signal_freq\n",
        "        assert ratio \u003d\u003d int(ratio)\n",
        "        return new_signal.repeat(1, 1, int(ratio))\n",
        "    if isinstance(desired_freq, float):\n",
        "        if desired_freq \u003d\u003d int(desired_freq) and signal_freq \u003d\u003d int(signal_freq):\n",
        "            desired_freq \u003d int(desired_freq)\n",
        "            signal_freq \u003d int(signal_freq)\n",
        "        else:\n",
        "            desired_freq \u003d desired_freq / signal_freq\n",
        "            signal_freq \u003d None\n",
        "    ratio \u003d Fraction(desired_freq, signal_freq)\n",
        "    if ratio.numerator !\u003d 1:\n",
        "        new_signal \u003d upsample_signal(new_signal, ratio.numerator)\n",
        "    if ratio.denominator !\u003d 1:\n",
        "        new_signal \u003d downsample_signal(new_signal, ratio.denominator)\n",
        "    if not pytorch:\n",
        "        if orig_dim \u003d\u003d 2:\n",
        "            return new_signal[0].numpy()\n",
        "        return new_signal[0, 0].numpy()\n",
        "    return new_signal\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uIZvF_9-fwUL",
        "colab_type": "code",
        "colab": {},
        "pycharm": {}
      },
      "cell_type": "code",
      "source": "#dataset.py\nimport os\nimport glob\nfrom random import shuffle\n\nimport torch\nimport numpy as np\nfrom scipy.io import loadmat\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset\nfrom torch.utils.data.dataloader import default_collate\n\nDATASET_VERSION \u003d 6\n\n\u0027\u0027\u0027\n!pip install imageio\n!pip install gdown\n!gdown --id 1rVey-8ZN1mAYcCISie5kGuZ_2WNWZ8Jb\n!gdown --id 1QMKt1c6v-Pa6sGXDia4x_OwNkSNSocCW\n!gdown --id 1vMVSmtdUzIt7MfDnNzB5CO8u8Xl19k5s\n!gdown --id 1GpBX7pkmUHu6JP63WH9FcvHp9Wxz7ZUi\n\u0027\u0027\u0027\n\n\n# bio_sampling_freq: 1 -\u003e 4 -\u003e 8 -\u003e 16 -\u003e 24 -\u003e 32 -\u003e 40 -\u003e 60 -\u003e 100\nclass EEGDataset(Dataset):\n    # for 60(sampling), starting from 1 hz(sampling) [32 samples at the beginning]\n    progression_scale_up \u003d [4, 2, 2, 3, 4, 5, 3]\n    progression_scale_down \u003d [1, 1, 1, 2, 3, 4, 2]\n\n    # for 60(sampling), starting from 0.25 hz(sampling) [8 samples at the beginning]\n    # progression_scale_up   \u003d [2, 2] + progression_scale_up\n    # progression_scale_down \u003d [1, 1] + progression_scale_down\n\n    # for 100(sampling), starting from 1 hz(sampling) [32 samples at the beginning]\n    # progression_scale_up   \u003d progression_scale_up + [5]\n    # progression_scale_down \u003d progression_scale_down + [3]\n\n    # for 100(sampling), starting from 0.25 hz(sampling) [8 samples at the beginning]\n    # progression_scale_up   \u003d [2, 2] + progression_scale_up + [5]\n    # progression_scale_down \u003d [1, 1] + progression_scale_down + [3]\n\n    picked_channels \u003d None\n\n    # picked_channels \u003d [3, 5, 9, 15, 16]\n    # picked_channels \u003d [3, 5, 9, 12, 13, 14, 15, 16]\n    # picked_channels \u003d [3, 5, 8, 9, 10, 13, 15, 16]\n\n    def __init__(self, train_files, norms, given_data, validation_ratio: float \u003d 0.1, dir_path: str \u003d \u0027./\u0027,\n                 data_sampling_freq: float \u003d 80, start_sampling_freq: float \u003d 1, end_sampling_freq: float \u003d 60,\n                 start_seq_len: int \u003d 32, stride: float \u003d 0.5, num_channels: int \u003d 5, number_of_files: int \u003d 100000,\n                 per_user_normalization: bool \u003d True, per_channel_normalization: bool \u003d False):\n        super().__init__()\n        self.model_depth \u003d 0\n        self.alpha \u003d 1.0\n        self.dir_path \u003d dir_path\n        self.end_sampling_freq \u003d end_sampling_freq\n        seq_len \u003d start_seq_len * end_sampling_freq / start_sampling_freq\n        assert seq_len \u003d\u003d int(seq_len), \u0027seq_len must be an int\u0027\n        seq_len \u003d int(seq_len)\n        self.seq_len \u003d seq_len\n        self.initial_kernel_size \u003d start_seq_len\n        self.stride \u003d int(seq_len * stride)\n        self.per_user_normalization \u003d per_user_normalization\n        self.per_channel_normalization \u003d per_channel_normalization\n        self.max_dataset_depth \u003d len(self.progression_scale_up)\n        self.norms \u003d norms\n        self.num_channels \u003d num_channels if self.picked_channels is None else len(self.picked_channels)\n        if given_data is not None:\n            self.sizes \u003d given_data[0][\u0027sizes\u0027]\n            self.files \u003d given_data[0][\u0027files\u0027]\n            self.norms \u003d given_data[0][\u0027norms\u0027]\n            self.data_pointers \u003d given_data[0][\u0027pointers\u0027]\n            self.datas \u003d [given_data[1][\u0027arr_{}\u0027.format(i)] for i in range(len(given_data[1].keys()))]\n            return\n        all_files \u003d glob.glob(os.path.join(dir_path, \u0027*_1.txt\u0027))[:number_of_files]\n        is_matlab \u003d len(all_files) \u003d\u003d 0\n        if is_matlab:\n            all_files \u003d glob.glob(os.path.join(dir_path, \u0027*.mat\u0027))[:number_of_files]\n        files \u003d len(all_files)\n        files \u003d [i for i in range(files)]\n        if train_files is None:\n            shuffle(files)\n            files \u003d files[:int(len(all_files) * (1.0 - validation_ratio))]\n        else:\n            files \u003d list(set(files) - set(train_files))\n        self.files \u003d files\n        sizes \u003d []\n        num_points \u003d []\n        self.datas \u003d []\n        for i in tqdm(files):\n            is_ok \u003d True\n            if is_matlab:\n                try:\n                    tmp \u003d loadmat(all_files[i])[\u0027eeg_signal\u0027]\n                    tmp \u003d resample_signal(tmp, data_sampling_freq, end_sampling_freq)\n                    size \u003d int(np.ceil((tmp.shape[1] - seq_len + 1) / self.stride))\n                except:\n                    size \u003d 0\n                if size \u003c\u003d 0:\n                    is_ok \u003d False\n                else:\n                    sizes.append(size)\n                    num_points.append((sizes[-1] - 1) * self.stride + seq_len)\n                    if self.picked_channels is None:\n                        self.datas.append(tmp[:num_channels, :num_points[-1]])\n                    else:\n                        self.datas.append(tmp[self.picked_channels, :num_points[-1]])\n            else:\n                for_range \u003d range(num_channels) if self.picked_channels is None else self.picked_channels\n                for kk, j in enumerate(for_range):\n                    with open(\u0027{}_{}.txt\u0027.format(all_files[i][:-6], j + 1)) as f:\n                        tmp \u003d list(map(float, f.read().split()))\n                        tmp \u003d np.array(tmp, dtype\u003dnp.float32)\n                        tmp \u003d resample_signal(tmp, data_sampling_freq, end_sampling_freq)\n                        if kk \u003d\u003d 0:\n                            size \u003d int(np.ceil((len(tmp) - seq_len + 1) / self.stride))\n                            if size \u003c\u003d 0:\n                                is_ok \u003d False\n                                break\n                            sizes.append(size)\n                            num_points.append((sizes[-1] - 1) * self.stride + seq_len)\n                            self.datas.append(np.zeros((num_channels, num_points[-1]), dtype\u003dnp.float32))\n                        tmp \u003d tmp[:num_points[-1]]\n                        self.datas[-1][j, :] \u003d tmp\n            if is_ok and per_user_normalization:\n                self.datas[-1], is_ok \u003d self.normalize(self.datas[-1], self.per_channel_normalization)\n                if not is_ok:\n                    del sizes[-1]\n                    del num_points[-1]\n                    del self.datas[-1]\n        self.sizes \u003d sizes\n        self.data_pointers \u003d [(i, j) for i, s in enumerate(self.sizes) for j in range(s)]\n        if not per_user_normalization:\n            self.normalize_all()\n\n    @classmethod\n    def from_config(cls, validation_ratio: float, dir_path: str, number_of_files: int,\n                    data_sampling_freq: float, start_sampling_freq: float, end_sampling_freq: float,\n                    start_seq_len: int, stride: float, num_channels: int,\n                    per_user_normalization: bool, per_channel_normalization: bool):\n        assert end_sampling_freq \u003c\u003d data_sampling_freq\n        mode \u003d per_user_normalization * 2 + per_channel_normalization * 1\n        train_files \u003d None\n        train_norms \u003d None\n        datasets \u003d [None, None]\n        for index, split in enumerate((\u0027train\u0027, \u0027val\u0027)):\n            target_location \u003d os.path.join(dir_path, \u0027{}%_{}c_{}m_{}s_{}v_{}ss_{}es_{}l_{}n_{}.npz\u0027\n                                           .format(validation_ratio, num_channels, mode, stride,\n                                                   DATASET_VERSION, start_sampling_freq, end_sampling_freq,\n                                                   start_seq_len, number_of_files, split))\n            given_data \u003d None\n            if os.path.exists(target_location):\n                print(\u0027loading {} dataset from file\u0027.format(split))\n                if split \u003d\u003d \u0027val\u0027 and validation_ratio \u003d\u003d 0.0:\n                    print(\u0027creating {} dataset from scratch\u0027.format(split))\n                else:\n                    given_data \u003d (load_pkl(target_location + \u0027.pkl\u0027), np.load(target_location))\n            else:\n                print(\u0027creating {} dataset from scratch\u0027.format(split))\n            dataset \u003d cls(train_files, train_norms, given_data, validation_ratio, dir_path, data_sampling_freq,\n                          start_sampling_freq, end_sampling_freq, start_seq_len, stride, num_channels,\n                          number_of_files, per_user_normalization, per_channel_normalization)\n            if train_files is None:\n                train_files \u003d dataset.files\n                train_norms \u003d dataset.norms\n            if given_data is None:\n                np.savez_compressed(target_location, *dataset.datas)\n                save_pkl(target_location + \u0027.pkl\u0027, {\u0027sizes\u0027: dataset.sizes, \u0027pointers\u0027: dataset.data_pointers,\n                                                    \u0027norms\u0027: dataset.norms, \u0027files\u0027: dataset.files})\n            datasets[index] \u003d dataset\n        return datasets[0], datasets[1]\n\n    def normalize_all(self):\n        num_files \u003d len(self.datas)\n        if self.norms is None:\n            all_max \u003d np.max(\n                np.array([data.max(axis\u003d1 if self.per_channel_normalization else None) for data in self.datas]), axis\u003d0)\n            all_min \u003d np.min(\n                np.array([data.min(axis\u003d1 if self.per_channel_normalization else None) for data in self.datas]), axis\u003d0)\n            self.norms \u003d (all_max, all_min)\n        else:\n            all_max, all_min \u003d self.norms\n        is_ok \u003d True\n        for i in range(num_files):\n            self.datas[i], is_ok \u003d self.normalize(self.datas[i], self.per_channel_normalization, all_max, all_min)\n        if not is_ok:\n            raise ValueError(\u0027data is constant!\u0027)\n\n    @staticmethod\n    def normalize(arr, per_channel, arr_max\u003dNone, arr_min\u003dNone):\n        if arr_max is None:\n            arr_max \u003d arr.max(axis\u003d1 if per_channel else None)\n        if arr_min is None:\n            arr_min \u003d arr.min(axis\u003d1 if per_channel else None)\n        is_ok \u003d arr_max !\u003d arr_min\n        if per_channel:\n            is_ok \u003d is_ok.all()\n        return ((arr - arr_min) / ((arr_max - arr_min) if is_ok else 1.0)) * 2.0 - 1.0, is_ok\n\n    @property\n    def shape(self):\n        return len(self), self.num_channels, self.seq_len\n\n    def __len__(self):\n        return len(self.data_pointers)\n\n    def load_file(self, item):\n        i, k \u003d self.data_pointers[item]\n        res \u003d self.datas[i][:, k * self.stride:k * self.stride + self.seq_len]\n        return res\n\n    def resample_data(self, data, index, forward\u003dTrue, alpha_fade\u003dFalse):\n        up_scale \u003d self.progression_scale_up[index - (1 if alpha_fade else 0)]\n        down_scale \u003d self.progression_scale_down[index - (1 if alpha_fade else 0)]\n        if forward:\n            return resample_signal(data, down_scale, up_scale, True)\n        return resample_signal(data, up_scale, down_scale, True)\n\n    def __getitem__(self, item):\n        with torch.no_grad():\n            datapoint \u003d torch.from_numpy(self.load_file(item).astype(np.float32)).unsqueeze(0)\n            target_depth \u003d self.model_depth\n            if self.max_dataset_depth !\u003d target_depth:\n                datapoint \u003d self.create_datapoint_from_depth(datapoint, target_depth)\n        return {\u0027x\u0027: self.alpha_fade(datapoint).squeeze(0)}\n\n    def create_datapoint_from_depth(self, datapoint, target_depth):\n        depth_diff \u003d (self.max_dataset_depth - target_depth)\n        for index in reversed(list(range(len(self.progression_scale_up)))[-depth_diff:]):\n            datapoint \u003d self.resample_data(datapoint, index, False)\n        return datapoint\n\n    def alpha_fade(self, datapoint):\n        if self.alpha \u003d\u003d 1:\n            return datapoint\n        t \u003d self.resample_data(datapoint, self.model_depth, False, alpha_fade\u003dTrue)\n        t \u003d self.resample_data(t, self.model_depth, True, alpha_fade\u003dTrue)\n        return datapoint + (t - datapoint) * (1 - self.alpha)\n\n\ndef get_collate_real(max_sampling_freq, max_len):\n    def collate_real(batch):\n        return cudize(default_collate(batch))\n\n    return collate_real\n\n\ndef get_collate_fake(latent_size, z_distribution, collate_real):\n    def collate_fake(batch):\n        batch \u003d collate_real(batch)  # extract condition(features)\n        batch[\u0027z\u0027] \u003d random_latents(batch[\u0027x\u0027].size(0), latent_size, z_distribution)\n        del batch[\u0027x\u0027]\n        return batch\n\n    return collate_fake\n",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yV4pcrAAf_aE",
        "colab_type": "code",
        "colab": {},
        "pycharm": {}
      },
      "cell_type": "code",
      "source": [
        "#layers.py\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.nn.init import calculate_gain\n",
        "from torch.nn.utils import spectral_norm\n",
        "\n",
        "\n",
        "class PixelNorm(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return pixel_norm(x)\n",
        "\n",
        "\n",
        "class ScaledTanh(nn.Tanh):\n",
        "    def __init__(self, scale\u003d0.5):\n",
        "        super().__init__()\n",
        "        self.scale \u003d scale\n",
        "\n",
        "    def forward(self, x):\n",
        "        return super().forward(x * self.scale)\n",
        "\n",
        "\n",
        "class GDropLayer(nn.Module):\n",
        "    def __init__(self, strength\u003d0.2, axes\u003d(0, 1)):\n",
        "        super().__init__()\n",
        "        self.strength \u003d strength\n",
        "        self.axes \u003d [axes] if isinstance(axes, int) else list(axes)\n",
        "\n",
        "    def forward(self, x, deterministic\u003dFalse):\n",
        "        if deterministic or not self.strength:\n",
        "            return x\n",
        "        rnd_shape \u003d [s if axis in self.axes else 1 for axis, s in enumerate(x.size())]\n",
        "        rnd \u003d (1 + self.strength) ** np.random.normal(size\u003drnd_shape)\n",
        "        rnd \u003d torch.from_numpy(rnd).type(x.data.type()).to(x)\n",
        "        return x * rnd\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, channels_in, spectral, init\u003d\u0027xavier_uniform\u0027):\n",
        "        super().__init__()\n",
        "        d_key \u003d max(channels_in // 8, 2)\n",
        "        conv_conf \u003d dict(kernel_size\u003d1, equalized\u003dFalse, spectral\u003dspectral,\n",
        "                         init\u003dinit, bias\u003dFalse, act_alpha\u003d-1)\n",
        "        self.gamma \u003d nn.Parameter(torch.tensor(0.), requires_grad\u003dTrue)\n",
        "        self.pooling \u003d nn.MaxPool1d(4)\n",
        "        self.key_conv \u003d GeneralConv(channels_in, d_key, **conv_conf)\n",
        "        self.query_conv \u003d GeneralConv(channels_in, d_key, **conv_conf)\n",
        "        self.value_conv \u003d GeneralConv(channels_in, channels_in // 2, **conv_conf)\n",
        "        self.final_conv \u003d GeneralConv(channels_in // 2, channels_in, **conv_conf)\n",
        "        self.softmax \u003d nn.Softmax(dim\u003d-1)\n",
        "        self.scale \u003d 1.0\n",
        "\n",
        "    def forward(self, x):  # BCT\n",
        "        query \u003d self.query_conv(x)  # BC/8T\n",
        "        key \u003d self.pooling(self.key_conv(x))  # BC/8T[/4]\n",
        "        value \u003d self.pooling(self.value_conv(x))  # BC/2T[/4]\n",
        "        out \u003d F.softmax(torch.bmm(key.permute(0, 2, 1), query) / self.scale, dim\u003d1)  # Bnormed(T[/4])T\n",
        "        attention_map \u003d out\n",
        "        out \u003d torch.bmm(value, out)  # BC/2T\n",
        "        out \u003d self.final_conv(out)  # BCT\n",
        "        return self.gamma * out + x, attention_map\n",
        "\n",
        "\n",
        "class MinibatchStddev(nn.Module):\n",
        "    def __init__(self, group_size\u003d4, temporal_groups_per_window\u003d1, kernel_size\u003d32):\n",
        "        super().__init__()\n",
        "        self.group_size \u003d group_size if group_size !\u003d 0 else 1e6\n",
        "        self.kernel_size \u003d kernel_size\n",
        "        self.stride_size \u003d self.kernel_size // temporal_groups_per_window\n",
        "\n",
        "    def forward(self, x):  # B, C, T\n",
        "        s \u003d x.size()\n",
        "        group_size \u003d min(s[0], self.group_size)\n",
        "        all_y \u003d []\n",
        "        for i in range(s[2] // self.stride_size):\n",
        "            y \u003d x[..., i * self.stride_size:(i + 1) * self.stride_size]\n",
        "            y \u003d y.view(group_size, -1, s[1], self.stride_size)  # G,B//G,C,T\n",
        "            y \u003d y - y.mean(dim\u003d0, keepdim\u003dTrue)  # G,B//G,C,T\n",
        "            y \u003d torch.sqrt((y ** 2).mean(dim\u003d0))  # B//G,C,T\n",
        "            y \u003d y.mean(dim\u003d1, keepdim\u003dTrue).mean(dim\u003d2, keepdim\u003dTrue)  # B//G,1,1\n",
        "            y \u003d y.repeat((group_size, 1, self.stride_size))  # B,1,T\n",
        "            all_y.append(y)\n",
        "        return torch.cat([x, torch.cat(all_y, dim\u003d2)], dim\u003d1)\n",
        "\n",
        "\n",
        "class ConditionalBatchNorm(nn.Module):\n",
        "    def __init__(self, num_features, num_classes, latent_size, spectral):\n",
        "        super().__init__()\n",
        "        no_cond \u003d latent_size \u003d\u003d 0 and num_classes \u003d\u003d 0\n",
        "        self.num_features \u003d num_features\n",
        "        self.normalizer \u003d nn.BatchNorm1d(num_features, affine\u003dno_cond)\n",
        "        if no_cond:\n",
        "            self.mode \u003d \u0027BN\u0027  # batch norm\n",
        "        elif latent_size \u003d\u003d 0:\n",
        "            self.embed \u003d GeneralConv(num_classes, num_features * 2, kernel_size\u003d1, equalized\u003dFalse,\n",
        "                                     act_alpha\u003d-1, spectral\u003dspectral, bias\u003dFalse)\n",
        "            self.mode \u003d \u0027CBN\u0027  # conditional batch norm\n",
        "        else:  # both \u0027SM\u0027(self modulation) and \u0027CSM\u0027(conditional self modulation)\n",
        "            # NOTE maybe reduce it to a single layer linear network?\n",
        "            self.embed \u003d nn.Sequential(\n",
        "                GeneralConv(latent_size + num_classes, num_features * 2, kernel_size\u003d1,\n",
        "                            equalized\u003dFalse, act_alpha\u003d0.0, spectral\u003dspectral, bias\u003dTrue),\n",
        "                GeneralConv(num_features * 2, num_features * 2, kernel_size\u003d1,\n",
        "                            equalized\u003dFalse, act_alpha\u003d-1, spectral\u003dspectral, bias\u003dFalse))\n",
        "            if num_classes \u003d\u003d 0:\n",
        "                self.mode \u003d \u0027SM\u0027  # self modulation\n",
        "            else:\n",
        "                self.mode \u003d \u0027CSM\u0027  # conditional self modulation(biggan)\n",
        "\n",
        "    def forward(self, x, y, z):  # y \u003d B*num_classes*Ty ; x \u003d B*num_features*Tx ; z \u003d B*latent_size\n",
        "        out \u003d self.normalizer(x)\n",
        "        if self.mode \u003d\u003d \u0027BN\u0027:\n",
        "            return out\n",
        "        if y is not None and y.ndimension() \u003d\u003d 2:\n",
        "            y \u003d y.unsqueeze(2)\n",
        "        if self.mode \u003d\u003d \u0027CBN\u0027:\n",
        "            cond \u003d y\n",
        "        else:\n",
        "            if self.mode \u003d\u003d \u0027CSM\u0027:\n",
        "                z \u003d expand3d(z)\n",
        "                cond \u003d torch.cat([resample_signal(z, z.size(2), y.size(2), pytorch\u003dTrue), y], dim\u003d1)\n",
        "            else:\n",
        "                cond \u003d expand3d(z)\n",
        "        embed \u003d self.embed(cond)  # B, num_features*2, Ty\n",
        "        embed \u003d resample_signal(embed, embed.shape[2], out.shape[2], pytorch\u003dTrue)\n",
        "        gamma, beta \u003d embed.chunk(2, dim\u003d1)\n",
        "        return out + gamma * out + beta  # trick to make sure gamma is 1.0 at the beginning of the training\n",
        "\n",
        "\n",
        "class EqualizedSeparableConv1d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size,\n",
        "                 padding, spectral, equalized, init, act_alpha, bias, groups, stride):\n",
        "        super().__init__()\n",
        "        self.net \u003d nn.Sequential(\n",
        "            EqualizedConv1d(in_channels, in_channels, kernel_size, padding,\n",
        "                            spectral, equalized, init, -1, True, groups\u003din_channels, stride\u003dstride),\n",
        "            EqualizedConv1d(in_channels, out_channels, 1, 0, spectral, equalized, init, act_alpha, bias, groups, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class EqualizedConv1d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, padding,\n",
        "                 spectral, equalized, init, act_alpha, bias, groups, stride):\n",
        "        super().__init__()\n",
        "        self.conv \u003d nn.Conv1d(in_channels\u003din_channels, out_channels\u003dout_channels, stride\u003dstride,\n",
        "                              kernel_size\u003dkernel_size, padding\u003dpadding, bias\u003dbias, groups\u003dgroups)\n",
        "        if bias:\n",
        "            self.conv.bias.data.zero_()\n",
        "        act_alpha \u003d act_alpha if act_alpha \u003e 0 else 1\n",
        "        if init \u003d\u003d \u0027kaiming_normal\u0027:\n",
        "            torch.nn.init.kaiming_normal_(self.conv.weight, a\u003dact_alpha)\n",
        "        elif init \u003d\u003d \u0027xavier_uniform\u0027:\n",
        "            torch.nn.init.xavier_uniform_(self.conv.weight, gain\u003dcalculate_gain(\u0027leaky_relu\u0027, param\u003dact_alpha))\n",
        "        elif init \u003d\u003d \u0027orthogonal\u0027:\n",
        "            torch.nn.init.orthogonal_(self.conv.weight, gain\u003dcalculate_gain(\u0027leaky_relu\u0027, param\u003dact_alpha))\n",
        "        if not equalized:\n",
        "            self.scale \u003d 1.0\n",
        "        else:\n",
        "            self.scale \u003d ((torch.mean(self.conv.weight.data ** 2)) ** 0.5).item()\n",
        "        self.conv.weight.data.copy_(self.conv.weight.data / self.scale)\n",
        "        if spectral:\n",
        "            self.conv \u003d spectral_norm(self.conv)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x * self.scale)\n",
        "\n",
        "\n",
        "class GeneralConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, z_to_bn_size\u003d0, kernel_size\u003d3, equalized\u003dTrue,\n",
        "                 pad\u003dNone, act_alpha\u003d0.2, do\u003d0, num_classes\u003d0, act_norm\u003dNone, spectral\u003dFalse,\n",
        "                 init\u003d\u0027kaiming_normal\u0027, bias\u003dTrue, separable\u003dFalse, stride\u003d1):\n",
        "        super().__init__()\n",
        "        pad \u003d (kernel_size - 1) // 2 if pad is None else pad\n",
        "        if separable:\n",
        "            conv_class \u003d EqualizedSeparableConv1d\n",
        "        else:\n",
        "            conv_class \u003d EqualizedConv1d\n",
        "        conv \u003d conv_class(in_channels, out_channels, kernel_size, padding\u003dpad, spectral\u003dspectral,\n",
        "                          equalized\u003dequalized, init\u003dinit, act_alpha\u003dact_alpha,\n",
        "                          bias\u003dbias if act_norm !\u003d \u0027batch\u0027 else False, groups\u003d1, stride\u003dstride)\n",
        "        norm \u003d None\n",
        "        if act_norm \u003d\u003d \u0027batch\u0027:\n",
        "            norm \u003d ConditionalBatchNorm(out_channels, num_classes, z_to_bn_size, spectral)\n",
        "        self.conv \u003d conv\n",
        "        self.norm \u003d norm\n",
        "        self.net \u003d []\n",
        "        if act_alpha \u003e\u003d 0:\n",
        "            if act_alpha \u003d\u003d 0:\n",
        "                self.net.append(nn.ReLU())  # DO NOT use inplace, gradient penalty will break\n",
        "            else:\n",
        "                self.net.append(nn.LeakyReLU(act_alpha))  # DO NOT use inplace, gradient penalty will break\n",
        "        if act_norm \u003d\u003d \u0027pixel\u0027:\n",
        "            self.net.append(PixelNorm())\n",
        "        if do !\u003d 0:\n",
        "            self.net.append(GDropLayer(strength\u003ddo))\n",
        "        self.net \u003d nn.Sequential(*self.net)\n",
        "\n",
        "    def forward(self, x, y\u003dNone, z\u003dNone, conv_noise\u003dNone):\n",
        "        c \u003d self.conv(x)\n",
        "        if conv_noise is not None:\n",
        "            c \u003d c * conv_noise\n",
        "        if self.norm:\n",
        "            c \u003d self.norm(c, y, z)\n",
        "        return self.net(c)\n",
        "\n",
        "\n",
        "class PassChannelResidual(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        if x.size(1) \u003e\u003d y.size(1):\n",
        "            x[:, :y.size(1)] \u003d x[:, :y.size(1)] + y\n",
        "            return x\n",
        "        return y[:, :x.size(1)] + x\n",
        "\n",
        "\n",
        "class ConcatResidual(nn.Module):\n",
        "    def __init__(self, ch_in, ch_out, equalized, spectral, init):\n",
        "        super().__init__()\n",
        "        assert ch_out \u003e\u003d ch_in\n",
        "        if ch_out \u003e ch_in:\n",
        "            self.net \u003d GeneralConv(ch_in, ch_out - ch_in, kernel_size\u003d1, equalized\u003dequalized, act_alpha\u003d-1,\n",
        "                                   spectral\u003dspectral, init\u003dinit)\n",
        "        else:\n",
        "            self.net \u003d None\n",
        "\n",
        "    def forward(self, h, x):\n",
        "        if self.net:\n",
        "            return h + torch.cat([x, self.net(x)], dim\u003d1)\n",
        "        return h + x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YgbPgo6jgV2U",
        "colab_type": "code",
        "colab": {},
        "pycharm": {}
      },
      "cell_type": "code",
      "source": [
        "#losses.py\n",
        "import torch\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable, grad\n",
        "\n",
        "one \u003d None\n",
        "zero \u003d None\n",
        "mixing_factors \u003d None\n",
        "\n",
        "\n",
        "def get_mixing_factor(batch_size):\n",
        "    global mixing_factors\n",
        "    if mixing_factors is None or batch_size !\u003d mixing_factors.size(0):\n",
        "        mixing_factors \u003d cudize(torch.FloatTensor(batch_size, 1, 1))\n",
        "    mixing_factors.uniform_()\n",
        "    return mixing_factors\n",
        "\n",
        "\n",
        "def get_one(batch_size):\n",
        "    global one\n",
        "    if one is None or batch_size !\u003d one.size(0):\n",
        "        one \u003d cudize(torch.ones(batch_size))\n",
        "    return one\n",
        "\n",
        "\n",
        "def get_zero(batch_size):\n",
        "    global zero\n",
        "    if zero is None or batch_size !\u003d zero.size(0):\n",
        "        zero \u003d cudize(torch.zeros(batch_size))\n",
        "    return zero\n",
        "\n",
        "\n",
        "def calc_grad(x_hat, pred_hat):\n",
        "    return grad(outputs\u003dpred_hat, inputs\u003dx_hat, grad_outputs\u003dget_one(pred_hat.size(0)),\n",
        "                create_graph\u003dTrue, retain_graph\u003dTrue, only_inputs\u003dTrue)[0]\n",
        "\n",
        "\n",
        "def generator_loss(dis: torch.nn.Module, gen: torch.nn.Module, real, z, loss_type: str,\n",
        "                   random_multiply: bool, feature_matching_lambda: float \u003d 0.0):\n",
        "    gen.zero_grad()\n",
        "    g_, _ \u003d gen(z)\n",
        "    d_fake, fake_features, _ \u003d dis(g_)\n",
        "    real_features \u003d None\n",
        "    scale \u003d random.random() if random_multiply else 1.0\n",
        "    if loss_type \u003d\u003d \u0027hinge\u0027 or loss_type.startswith(\u0027wgan\u0027):\n",
        "        g_loss \u003d -d_fake.mean()\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            d_real, real_features, _ \u003d dis(real)\n",
        "        if loss_type \u003d\u003d \u0027rsgan\u0027:\n",
        "            g_loss \u003d F.binary_cross_entropy_with_logits(d_fake - d_real, get_one(d_fake.size(0)))\n",
        "        elif loss_type \u003d\u003d \u0027rasgan\u0027:\n",
        "            batch_size \u003d d_fake.size(0)\n",
        "            g_loss \u003d (F.binary_cross_entropy_with_logits(d_fake - d_real.mean(),\n",
        "                                                         get_one(batch_size)) + F.binary_cross_entropy_with_logits(\n",
        "                d_real - d_fake.mean(), get_zero(batch_size))) / 2.0\n",
        "        elif loss_type \u003d\u003d \u0027rahinge\u0027:\n",
        "            g_loss \u003d (torch.mean(F.relu(1.0 + (d_real - torch.mean(d_fake)))) + torch.mean(\n",
        "                F.relu(1.0 - (d_fake - torch.mean(d_real))))) / 2\n",
        "        else:\n",
        "            raise ValueError(\u0027Invalid loss type\u0027)\n",
        "    if feature_matching_lambda !\u003d 0.0:\n",
        "        if real_features is None:\n",
        "            with torch.no_grad():\n",
        "                _, real_features, _ \u003d dis(real)\n",
        "        diff \u003d real_features.mean(dim\u003d0) - fake_features.mean(dim\u003d0)\n",
        "        g_loss \u003d g_loss + (diff * diff).mean()\n",
        "    return g_loss * scale\n",
        "\n",
        "\n",
        "def discriminator_loss(dis: torch.nn.Module, gen: torch.nn.Module, real, z, loss_type: str,\n",
        "                       iwass_drift_epsilon: float, grad_lambda: float, iwass_target: float):\n",
        "    dis.zero_grad()\n",
        "    d_real, _, _ \u003d dis(real)\n",
        "    with torch.no_grad():\n",
        "        g_, _ \u003d gen(z)\n",
        "    d_fake, _, _ \u003d dis(g_)\n",
        "    batch_size \u003d d_real.size(0)\n",
        "    gp_gain \u003d 1.0 if grad_lambda !\u003d 0 else 0\n",
        "    if loss_type \u003d\u003d \u0027hinge\u0027:\n",
        "        d_loss \u003d F.relu(1.0 - d_real).mean() + F.relu(1.0 + d_fake).mean()\n",
        "    elif loss_type \u003d\u003d \u0027rsgan\u0027:\n",
        "        d_loss \u003d F.binary_cross_entropy_with_logits(d_real - d_fake, get_one(batch_size))\n",
        "    elif loss_type \u003d\u003d \u0027rasgan\u0027:\n",
        "        d_loss \u003d (F.binary_cross_entropy_with_logits(d_real - d_fake.mean(),\n",
        "                                                     get_one(batch_size)) + F.binary_cross_entropy_with_logits(\n",
        "            d_fake - d_real.mean(), get_zero(batch_size))) / 2.0\n",
        "    elif loss_type \u003d\u003d \u0027rahinge\u0027:\n",
        "        d_loss \u003d (torch.mean(F.relu(1.0 - (d_real - d_fake.mean()))) + torch.mean(\n",
        "            F.relu(1.0 + (d_fake - d_real.mean())))) / 2\n",
        "    elif loss_type.startswith(\u0027wgan\u0027):  # wgan and wgan_theirs\n",
        "        d_fake_mean \u003d d_fake.mean()\n",
        "        d_real_mean \u003d d_real.mean()\n",
        "        if loss_type \u003d\u003d \u0027wgan_theirs\u0027:\n",
        "            d_loss \u003d d_fake_mean - d_real_mean + (d_fake_mean + d_real_mean) ** 2 * iwass_drift_epsilon\n",
        "            gp_gain \u003d F.relu(d_real_mean - d_fake_mean)\n",
        "        elif loss_type \u003d\u003d \u0027wgan_gp\u0027:\n",
        "            d_loss \u003d d_fake_mean - d_real_mean + (d_real ** 2).mean() * iwass_drift_epsilon\n",
        "            gp_gain \u003d 1\n",
        "        else:\n",
        "            raise ValueError(\u0027Invalid loss type\u0027)\n",
        "    else:\n",
        "        raise ValueError(\u0027Invalid loss type\u0027)\n",
        "    if gp_gain !\u003d 0 and grad_lambda !\u003d 0:\n",
        "        alpha \u003d get_mixing_factor(real[\u0027x\u0027].size(0))\n",
        "        print(type(alpha), type(real), )\n",
        "        x_hat \u003d {\u0027x\u0027: Variable(alpha * real[\u0027x\u0027].data + (1.0 - alpha) * g_[\u0027x\u0027].data, requires_grad\u003dTrue)}\n",
        "        beta \u003d alpha.squeeze(dim\u003d2)\n",
        "        for k in real.keys():\n",
        "            if k.startswith(\u0027global_\u0027) or k.startswith(\u0027temporal_\u0027):\n",
        "                x_hat[k] \u003d Variable(beta * real[k].data + (1.0 - beta) * g_[k].data, requires_grad\u003dTrue)\n",
        "        pred_hat, _, _ \u003d dis(x_hat)\n",
        "        g \u003d calc_grad(x_hat[\u0027x\u0027], pred_hat).view(batch_size, -1)\n",
        "        gp \u003d g.norm(p\u003d2, dim\u003d1) - iwass_target\n",
        "        if loss_type \u003d\u003d \u0027wgan_theirs\u0027:\n",
        "            gp \u003d F.relu(gp)\n",
        "        gp_loss \u003d gp_gain * (gp ** 2).mean() * grad_lambda / (iwass_target ** 2)\n",
        "        d_loss \u003d d_loss + gp_loss\n",
        "    return d_loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nlWJZ3Eygnyy",
        "colab_type": "code",
        "colab": {},
        "pycharm": {}
      },
      "cell_type": "code",
      "source": [
        "#network.py\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.utils import spectral_norm\n",
        "\n",
        "\n",
        "class GBlock(nn.Module):\n",
        "    def __init__(self, ch_in, ch_out, ch_rgb, k_size\u003d3, initial_kernel_size\u003dNone, is_residual\u003dFalse,\n",
        "                 no_tanh\u003dFalse, deep\u003dFalse, per_channel_noise\u003dFalse, to_rgb_mode\u003d\u0027pggan\u0027, **layer_settings):\n",
        "        super().__init__()\n",
        "        is_first \u003d initial_kernel_size is not None\n",
        "        first_k_size \u003d initial_kernel_size if is_first else k_size\n",
        "        hidden_size \u003d (ch_in // 4) if deep else ch_out\n",
        "        self.c1 \u003d GeneralConv(ch_in, hidden_size, kernel_size\u003dfirst_k_size,\n",
        "                              pad\u003dinitial_kernel_size - 1 if is_first else None, **layer_settings)\n",
        "        self.c2 \u003d GeneralConv(hidden_size, hidden_size, kernel_size\u003dk_size, **layer_settings)\n",
        "        if per_channel_noise:\n",
        "            self.c1_noise_weight \u003d nn.Parameter(torch.zeros(1, hidden_size, 1))\n",
        "            self.c2_noise_weight \u003d nn.Parameter(torch.zeros(1, hidden_size, 1))\n",
        "        else:\n",
        "            self.c1_noise_weight, self.c2_noise_weight \u003d None, None\n",
        "        if deep:\n",
        "            self.c3 \u003d GeneralConv(hidden_size, hidden_size, kernel_size\u003dk_size, **layer_settings)\n",
        "            self.c4 \u003d GeneralConv(hidden_size, ch_out, kernel_size\u003dk_size, **layer_settings)\n",
        "            if per_channel_noise:\n",
        "                self.c3_noise_weight \u003d nn.Parameter(torch.zeros(1, hidden_size, 1))\n",
        "                self.c4_noise_weight \u003d nn.Parameter(torch.zeros(1, ch_out, 1))\n",
        "            else:\n",
        "                self.c3_noise_weight, self.c4_noise_weight \u003d None, None\n",
        "        reduced_layer_settings \u003d dict(equalized\u003dlayer_settings[\u0027equalized\u0027], spectral\u003dlayer_settings[\u0027equalized\u0027],\n",
        "                                      init\u003dlayer_settings[\u0027equalized\u0027])\n",
        "        if to_rgb_mode \u003d\u003d \u0027pggan\u0027:\n",
        "            to_rgb \u003d GeneralConv(ch_out, ch_rgb, kernel_size\u003d1, act_alpha\u003d-1, **reduced_layer_settings)\n",
        "        elif to_rgb_mode in {\u0027sngan\u0027, \u0027sagan\u0027}:\n",
        "            to_rgb \u003d GeneralConv(ch_out, ch_rgb if to_rgb_mode \u003d\u003d \u0027sngan\u0027 else ch_out,\n",
        "                                 kernel_size\u003d3, act_alpha\u003d0.2, **reduced_layer_settings)\n",
        "            if to_rgb_mode \u003d\u003d \u0027sagan\u0027:\n",
        "                to_rgb \u003d nn.Sequential(\n",
        "                    GeneralConv(ch_out, ch_rgb, kernel_size\u003d1, act_alpha\u003d-1, **reduced_layer_settings), to_rgb)\n",
        "        elif to_rgb_mode \u003d\u003d \u0027biggan\u0027:\n",
        "            to_rgb \u003d nn.Sequential(nn.BatchNorm1d(ch_out), nn.ReLU(),\n",
        "                                   GeneralConv(ch_out, ch_rgb, kernel_size\u003d3, act_alpha\u003d-1, **reduced_layer_settings))\n",
        "        else:\n",
        "            raise ValueError()\n",
        "        if no_tanh:\n",
        "            self.toRGB \u003d to_rgb\n",
        "        else:\n",
        "            self.toRGB \u003d nn.Sequential(to_rgb, ScaledTanh())\n",
        "        if deep:\n",
        "            self.residual \u003d PassChannelResidual()\n",
        "        else:\n",
        "            if not is_first and is_residual:\n",
        "                self.residual \u003d nn.Sequential() if ch_in \u003d\u003d ch_out else \\\n",
        "                    GeneralConv(ch_in, ch_out, 1, act_alpha\u003d-1, **reduced_layer_settings)\n",
        "            else:\n",
        "                self.residual \u003d None\n",
        "        self.deep \u003d deep\n",
        "\n",
        "    @staticmethod\n",
        "    def get_per_channel_noise(noise_weight):\n",
        "        return None if noise_weight is None else torch.randn(*noise_weight.size()) * noise_weight\n",
        "\n",
        "    def forward(self, x, y\u003dNone, z\u003dNone, last\u003dFalse):\n",
        "        h \u003d self.c1(x, y\u003dy, z\u003dz, conv_noise\u003dself.get_per_channel_noise(self.c1_noise_weight))\n",
        "        h \u003d self.c2(h, y\u003dy, z\u003dz, conv_noise\u003dself.get_per_channel_noise(self.c2_noise_weight))\n",
        "        if self.deep:\n",
        "            h \u003d self.c3(h, y\u003dy, z\u003dz, conv_noise\u003dself.get_per_channel_noise(self.c3_noise_weight))\n",
        "            h \u003d self.c4(h, y\u003dy, z\u003dz, conv_noise\u003dself.get_per_channel_noise(self.c4_noise_weight))\n",
        "            h \u003d self.residual(h, x)\n",
        "        elif self.residual is not None:\n",
        "            h \u003d h + self.residual(x)\n",
        "        if last:\n",
        "            return self.toRGB(h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, initial_kernel_size, num_rgb_channels, fmap_base, fmap_max, fmap_min, kernel_size,\n",
        "                 self_attention_layers, progression_scale_up, progression_scale_down, residual, separable,\n",
        "                 equalized, init, act_alpha, num_classes, deep, z_distribution, spectral\u003dFalse,\n",
        "                 latent_size\u003d256, no_tanh\u003dFalse, per_channel_noise\u003dFalse, to_rgb_mode\u003d\u0027pggan\u0027, z_to_bn\u003dFalse,\n",
        "                 split_z\u003dFalse, dropout\u003d0.2, act_norm\u003d\u0027pixel\u0027, conv_only\u003dFalse, shared_embedding_size\u003d32,\n",
        "                 normalize_latents\u003dTrue, rgb_generation_mode\u003d\u0027pggan\u0027):\n",
        "        \"\"\"\n",
        "        :param initial_kernel_size: int, this should be always correct regardless of conv_only\n",
        "        :param num_rgb_channels: int\n",
        "        :param fmap_base: int\n",
        "        :param fmap_max: int\n",
        "        :param fmap_min: int\n",
        "        :param kernel_size: int\n",
        "        :param self_attention_layers: list[int]\n",
        "        :param progression_scale_up: list[int]\n",
        "        :param progression_scale_down: list[int]\n",
        "        :param residual: bool\n",
        "        :param separable: bool\n",
        "        :param equalized: bool\n",
        "        :param spectral: bool\n",
        "        :param init: \u0027kaiming_normal\u0027 or \u0027xavier_uniform\u0027 or \u0027orthogonal\u0027\n",
        "        :param act_alpha: float, 0 is relu, -1 is linear and 0.2 is recommended\n",
        "        :param z_distribution: \u0027normal\u0027 or \u0027bernoulli\u0027 or \u0027censored\u0027\n",
        "        :param latent_size: int\n",
        "        :param no_tanh: bool\n",
        "        :param deep: bool, in case it\u0027s true it will turn off split_z\n",
        "        :param per_channel_noise: bool\n",
        "        :param to_rgb_mode: \u0027pggan\u0027 or \u0027sagan\u0027 or \u0027sngan\u0027 or \u0027biggan\u0027\n",
        "        :param z_to_bn: bool, whether to concatenate z with y(if available) to feed to cbn or not\n",
        "        :param split_z: bool\n",
        "        :param dropout: float\n",
        "        :param num_classes: int, input y.shape \u003d\u003d (batch_size, num_classes, T_y)\n",
        "        :param act_norm: \u0027batch\u0027 or \u0027pixel\u0027 or None\n",
        "        :param conv_only: bool\n",
        "        :param shared_embedding_size: int, in case it\u0027s none zero, y will be transformed to (batch_size, shared_embedding_size, T_y)\n",
        "        :param normalize_latents: bool\n",
        "        :param rgb_generation_mode: \u0027residual\u0027sum([rgbs]) or \u0027mean\u0027mean([rgbs]) or \u0027pggan\u0027(last_rgb)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        R \u003d len(progression_scale_up)\n",
        "        assert len(progression_scale_up) \u003d\u003d len(progression_scale_down)\n",
        "        self.progression_scale_up \u003d progression_scale_up\n",
        "        self.progression_scale_down \u003d progression_scale_down\n",
        "        self.depth \u003d 0\n",
        "        self.alpha \u003d 1.0\n",
        "        if deep:\n",
        "            split_z \u003d False\n",
        "        self.split_z \u003d split_z\n",
        "        self.z_distribution \u003d z_distribution\n",
        "        self.conv_only \u003d conv_only\n",
        "        self.initial_kernel_size \u003d initial_kernel_size\n",
        "        self.normalize_latents \u003d normalize_latents\n",
        "        self.z_to_bn \u003d z_to_bn\n",
        "\n",
        "        def nf(stage):\n",
        "            return min(max(int(fmap_base / (2.0 ** stage)), fmap_min), fmap_max)\n",
        "\n",
        "        if latent_size is None:\n",
        "            latent_size \u003d nf(0)\n",
        "        self.input_latent_size \u003d latent_size\n",
        "        if num_classes !\u003d 0:\n",
        "            if shared_embedding_size \u003e 0:\n",
        "                self.y_encoder \u003d GeneralConv(num_classes, shared_embedding_size, kernel_size\u003d1,\n",
        "                                             equalized\u003dFalse, act_alpha\u003dact_alpha, spectral\u003dFalse, bias\u003dFalse)\n",
        "                num_classes \u003d shared_embedding_size\n",
        "            else:\n",
        "                self.y_encoder \u003d nn.Sequential()\n",
        "        else:\n",
        "            self.y_encoder \u003d None\n",
        "        if split_z:\n",
        "            latent_size //\u003d R + 2  # we also give part of the z to the first layer\n",
        "        self.latent_size \u003d latent_size\n",
        "        block_settings \u003d dict(ch_rgb\u003dnum_rgb_channels, k_size\u003dkernel_size, is_residual\u003dresidual, deep\u003ddeep,\n",
        "                              no_tanh\u003dno_tanh, per_channel_noise\u003dper_channel_noise, to_rgb_mode\u003dto_rgb_mode)\n",
        "        layer_settings \u003d dict(z_to_bn_size\u003dlatent_size if z_to_bn else 0, equalized\u003dequalized, spectral\u003dspectral,\n",
        "                              init\u003dinit, act_alpha\u003dact_alpha, do\u003ddropout, num_classes\u003dnum_classes, act_norm\u003dact_norm,\n",
        "                              bias\u003dTrue, separable\u003dseparable)\n",
        "        self.block0 \u003d GBlock(latent_size, nf(1), **block_settings, **layer_settings,\n",
        "                             initial_kernel_size\u003dNone if conv_only else initial_kernel_size)\n",
        "        dummy \u003d []  # to make SA layers registered\n",
        "        self.self_attention \u003d dict()\n",
        "        for layer in self_attention_layers:\n",
        "            dummy.append(SelfAttention(nf(layer + 1), spectral, init))\n",
        "            self.self_attention[layer] \u003d dummy[-1]\n",
        "        if len(dummy) !\u003d 0:\n",
        "            self.dummy \u003d nn.ModuleList(dummy)\n",
        "        self.blocks \u003d nn.ModuleList(\n",
        "            [GBlock(nf(i + 1), nf(i + 2), **block_settings, **layer_settings) for i in range(R)])\n",
        "        self.max_depth \u003d len(self.blocks)\n",
        "        self.deep \u003d deep\n",
        "        self.rgb_generation_mode \u003d rgb_generation_mode\n",
        "\n",
        "    def _split_z(self, l, z):\n",
        "        if not self.z_to_bn:\n",
        "            return None\n",
        "        if self.split_z:\n",
        "            return z[:, (2 + l) * self.latent_size:(3 + l) * self.latent_size]\n",
        "        return z\n",
        "\n",
        "    def _do_layer(self, l, h, y, z):\n",
        "        if l in self.self_attention:\n",
        "            h, attention_map \u003d self.self_attention[l](h)\n",
        "        else:\n",
        "            attention_map \u003d None\n",
        "        h \u003d resample_signal(h, self.progression_scale_down[l], self.progression_scale_up[l], True)\n",
        "        return self.blocks[l](h, y, self._split_z(l, z), last\u003dFalse), attention_map\n",
        "\n",
        "    def _combine_rgbs(self, last_rgb, saved_rgbs):\n",
        "        if self.rgb_generation_mode \u003d\u003d \u0027residual\u0027:\n",
        "            return_value \u003d saved_rgbs[0]\n",
        "            for rgb in saved_rgbs[1:]:\n",
        "                return_value \u003d resample_signal(return_value, return_value.size(2), rgb.size(2)) + rgb\n",
        "            if self.alpha \u003d\u003d 1.0:\n",
        "                return return_value\n",
        "            return return_value - (1.0 - self.alpha) * saved_rgbs[-1]\n",
        "        elif self.rgb_generation_mode \u003d\u003d \u0027mean\u0027:\n",
        "            return_value \u003d saved_rgbs[0]\n",
        "            for rgb in saved_rgbs[1:]:\n",
        "                return_value \u003d resample_signal(return_value, return_value.size(2), rgb.size(2)) + rgb\n",
        "            return_value \u003d return_value / len(saved_rgbs)\n",
        "            if self.alpha \u003d\u003d 1.0:\n",
        "                return return_value\n",
        "            return (return_value * len(saved_rgbs) - saved_rgbs[-1]) / (len(saved_rgbs) - 1) * (\n",
        "                    1.0 - self.alpha) + return_value * self.alpha\n",
        "        return last_rgb\n",
        "\n",
        "    def _wrap_output(self, last_rgb, all_rgbs, y):\n",
        "        return {\u0027x\u0027: self._combine_rgbs(last_rgb, all_rgbs), \u0027y\u0027: y}\n",
        "\n",
        "    def forward(self, z):\n",
        "        if isinstance(z, dict):\n",
        "            z, y \u003d z[\u0027z\u0027], z.get(\u0027y\u0027, None)\n",
        "        elif isinstance(z, tuple):\n",
        "            z, y \u003d z\n",
        "        else:\n",
        "            y \u003d None\n",
        "        if y is not None:\n",
        "            if y.ndimension() \u003d\u003d 2:\n",
        "                y \u003d y.unsqueeze(2)\n",
        "            if self.y_encoder is not None:\n",
        "                y \u003d self.y_encoder(y)\n",
        "            else:\n",
        "                y \u003d None\n",
        "        if self.normalize_latents:\n",
        "            z \u003d pixel_norm(z)\n",
        "        if z.ndimension() \u003d\u003d 2:\n",
        "            z \u003d z.unsqueeze(2)\n",
        "        if self.split_z and not self.deep:\n",
        "            h \u003d z[:, :self.latent_size, :]\n",
        "        else:\n",
        "            h \u003d z\n",
        "        save_rgb \u003d self.rgb_generation_mode !\u003d \u0027pggan\u0027\n",
        "        saved_rgbs \u003d []\n",
        "        if self.depth \u003d\u003d 0:\n",
        "            h \u003d self.block0(h, y, self._split_z(-1, z), last\u003dTrue)\n",
        "            if save_rgb:\n",
        "                saved_rgbs.append(h)\n",
        "            return self._wrap_output(h, saved_rgbs, y), {}\n",
        "        h \u003d self.block0(h, y, self._split_z(-1, z))\n",
        "        if save_rgb:\n",
        "            saved_rgbs.append(self.block0.toRGB(h))\n",
        "        all_attention_maps \u003d {}\n",
        "        for i in range(self.depth - 1):\n",
        "            h, attention_map \u003d self._do_layer(i, h, y, z)\n",
        "            if save_rgb:\n",
        "                saved_rgbs.append(self.blocks[i].toRGB(h))\n",
        "            if attention_map is not None:\n",
        "                all_attention_maps[i] \u003d attention_map\n",
        "        h \u003d resample_signal(h, self.progression_scale_down[self.depth - 1], self.progression_scale_up[self.depth - 1],\n",
        "                            True)\n",
        "        ult \u003d self.blocks[self.depth - 1](h, y, self._split_z(self.depth - 1, z), True)\n",
        "        if save_rgb:\n",
        "            saved_rgbs.append(ult)\n",
        "        if self.alpha \u003d\u003d 1.0:\n",
        "            return self._wrap_output(ult, saved_rgbs, y), all_attention_maps\n",
        "        preult_rgb \u003d self.blocks[self.depth - 2].toRGB(h) if self.depth \u003e 1 else self.block0.toRGB(h)\n",
        "        return self._wrap_output(preult_rgb * (1.0 - self.alpha) + ult * self.alpha, saved_rgbs, y), all_attention_maps\n",
        "\n",
        "\n",
        "class DBlock(nn.Module):\n",
        "    def __init__(self, ch_in, ch_out, ch_rgb, k_size\u003d3, initial_kernel_size\u003dNone, is_residual\u003dFalse,\n",
        "                 deep\u003dFalse, group_size\u003d4, temporal_groups_per_window\u003d1, conv_disc\u003dFalse, **layer_settings):\n",
        "        super().__init__()\n",
        "        is_last \u003d initial_kernel_size is not None\n",
        "        self.net \u003d []\n",
        "        if is_last:\n",
        "            self.net.append(MinibatchStddev(group_size, temporal_groups_per_window, initial_kernel_size))\n",
        "        hidden_size \u003d (ch_out // 4) if deep else ch_in\n",
        "        self.net.append(\n",
        "            GeneralConv(ch_in + (1 if is_last else 0), hidden_size, kernel_size\u003dk_size, **layer_settings))\n",
        "        if deep:\n",
        "            self.net.append(\n",
        "                GeneralConv(hidden_size, hidden_size, kernel_size\u003dk_size, **layer_settings))\n",
        "            self.net.append(\n",
        "                GeneralConv(hidden_size, hidden_size, kernel_size\u003dk_size, **layer_settings))\n",
        "        is_linear_last \u003d is_last and not conv_disc\n",
        "        self.net.append(GeneralConv(hidden_size, ch_out, kernel_size\u003dinitial_kernel_size if is_linear_last else k_size,\n",
        "                                    pad\u003d0 if is_linear_last else None, **layer_settings))\n",
        "        self.net \u003d nn.Sequential(*self.net)\n",
        "        reduced_layer_settings \u003d dict(equalized\u003dlayer_settings[\u0027equalized\u0027], spectral\u003dlayer_settings[\u0027equalized\u0027],\n",
        "                                      init\u003dlayer_settings[\u0027equalized\u0027])\n",
        "        self.fromRGB \u003d GeneralConv(ch_rgb, ch_in, kernel_size\u003d1, act_alpha\u003dlayer_settings[\u0027act_alpha\u0027],\n",
        "                                   **reduced_layer_settings)\n",
        "        if deep:\n",
        "            self.residual \u003d ConcatResidual(ch_in, ch_out, **reduced_layer_settings)\n",
        "        else:\n",
        "            if is_residual and (not is_last or conv_disc):\n",
        "                self.residual \u003d nn.Sequential() if ch_in \u003d\u003d ch_out else GeneralConv(ch_in, ch_out, kernel_size\u003d1,\n",
        "                                                                                    act_alpha\u003d-1,\n",
        "                                                                                    **reduced_layer_settings)\n",
        "            else:\n",
        "                self.residual \u003d None\n",
        "        self.deep \u003d deep\n",
        "\n",
        "    def forward(self, x, first\u003dFalse):\n",
        "        if first:\n",
        "            x \u003d self.fromRGB(x)\n",
        "        h \u003d self.net(x)\n",
        "        if self.deep:\n",
        "            return self.residual(h, x)\n",
        "        if self.residual:\n",
        "            h \u003d h + self.residual(x)\n",
        "        return h\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, initial_kernel_size, num_rgb_channels, fmap_base, fmap_max, fmap_min, kernel_size,\n",
        "                 self_attention_layers, progression_scale_up, progression_scale_down, residual, separable,\n",
        "                 equalized, init, act_alpha, num_classes, deep, spectral\u003dFalse, dropout\u003d0.2, act_norm\u003dNone,\n",
        "                 group_size\u003d4, temporal_groups_per_window\u003d1, conv_only\u003dFalse, input_to_all_layers\u003dFalse):\n",
        "        \"\"\"\n",
        "        NOTE we only support global conidtioning(not temporal) for now\n",
        "        :param initial_kernel_size:\n",
        "        :param num_rgb_channels:\n",
        "        :param fmap_base:\n",
        "        :param fmap_max:\n",
        "        :param fmap_min:\n",
        "        :param kernel_size:\n",
        "        :param self_attention_layers:\n",
        "        :param progression_scale_up:\n",
        "        :param progression_scale_down:\n",
        "        :param residual:\n",
        "        :param separable:\n",
        "        :param equalized:\n",
        "        :param spectral:\n",
        "        :param init:\n",
        "        :param act_alpha:\n",
        "        :param num_classes:\n",
        "        :param deep:\n",
        "        :param dropout:\n",
        "        :param act_norm:\n",
        "        :param group_size:\n",
        "        :param temporal_groups_per_window:\n",
        "        :param conv_only:\n",
        "        :param input_to_all_layers:\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        R \u003d len(progression_scale_up)\n",
        "        assert len(progression_scale_up) \u003d\u003d len(progression_scale_down)\n",
        "        self.progression_scale_up \u003d progression_scale_up\n",
        "        self.progression_scale_down \u003d progression_scale_down\n",
        "        self.depth \u003d 0\n",
        "        self.alpha \u003d 1.0\n",
        "        self.input_to_all_layers \u003d input_to_all_layers\n",
        "\n",
        "        def nf(stage):\n",
        "            return min(max(int(fmap_base / (2.0 ** stage)), fmap_min), fmap_max)\n",
        "\n",
        "        layer_settings \u003d dict(equalized\u003dequalized, spectral\u003dspectral, init\u003dinit, act_alpha\u003dact_alpha,\n",
        "                              do\u003ddropout, num_classes\u003d0, act_norm\u003dact_norm, bias\u003dTrue, separable\u003dseparable)\n",
        "        block_settings \u003d dict(ch_rgb\u003dnum_rgb_channels, k_size\u003dkernel_size, is_residual\u003dresidual, conv_disc\u003dconv_only,\n",
        "                              group_size\u003dgroup_size, temporal_groups_per_window\u003dtemporal_groups_per_window, deep\u003ddeep)\n",
        "\n",
        "        last_block \u003d DBlock(nf(1), nf(0), initial_kernel_size\u003dinitial_kernel_size, **block_settings, **layer_settings)\n",
        "        dummy \u003d []  # to make SA layers registered\n",
        "        self.self_attention \u003d dict()\n",
        "        for layer in self_attention_layers:\n",
        "            dummy.append(SelfAttention(nf(layer + 1), spectral, init))\n",
        "            self.self_attention[layer] \u003d dummy[-1]\n",
        "        if len(dummy):\n",
        "            self.dummy \u003d nn.ModuleList(dummy)\n",
        "        self.blocks \u003d nn.ModuleList(\n",
        "            [DBlock(nf(i + 2), nf(i + 1), **block_settings, **layer_settings) for i in range(R - 1, -1, -1)] + [\n",
        "                last_block])\n",
        "\n",
        "        if num_classes !\u003d 0:\n",
        "            self.class_emb \u003d nn.Linear(num_classes, nf(0), False)\n",
        "            if spectral:\n",
        "                self.class_emb \u003d spectral_norm(self.class_emb)\n",
        "        else:\n",
        "            self.class_emb \u003d None\n",
        "        self.linear \u003d GeneralConv(nf(0), 1, kernel_size\u003d1, equalized\u003dequalized, act_alpha\u003d-1,\n",
        "                                  spectral\u003dspectral, init\u003dinit)\n",
        "        self.max_depth \u003d len(self.blocks) - 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        if isinstance(x, dict):\n",
        "            x, y \u003d x[\u0027x\u0027], x.get(\u0027y\u0027, None)\n",
        "        elif isinstance(x, tuple):\n",
        "            x, y \u003d x\n",
        "        else:\n",
        "            y \u003d None\n",
        "        h \u003d self.blocks[-(self.depth + 1)](x, True)\n",
        "        if self.depth \u003e 0:\n",
        "            h \u003d resample_signal(h, self.progression_scale_up[self.depth - 1],\n",
        "                                self.progression_scale_down[self.depth - 1], True)\n",
        "            if self.alpha \u003c 1.0 or self.input_to_all_layers:\n",
        "                x_lowres \u003d resample_signal(x, self.progression_scale_up[self.depth - 1],\n",
        "                                           self.progression_scale_down[self.depth - 1], True)\n",
        "                preult_rgb \u003d self.blocks[-self.depth].fromRGB(x_lowres)\n",
        "                if self.input_to_all_layers:\n",
        "                    h \u003d (h * self.alpha + preult_rgb) / (1.0 + self.alpha)\n",
        "                else:\n",
        "                    h \u003d h * self.alpha + (1.0 - self.alpha) * preult_rgb\n",
        "        all_attention_maps \u003d {}\n",
        "        for i in range(self.depth, 0, -1):\n",
        "            h \u003d self.blocks[-i](h)\n",
        "            if i \u003e 1:\n",
        "                h \u003d resample_signal(h, self.progression_scale_up[i - 2], self.progression_scale_down[i - 2], True)\n",
        "                if self.input_to_all_layers:\n",
        "                    x_lowres \u003d resample_signal(x_lowres, self.progression_scale_up[i - 2],\n",
        "                                               self.progression_scale_down[i - 2], True)\n",
        "                    h \u003d (h + self.blocks[-i + 1].fromRGB(x_lowres)) / 2.0\n",
        "            if (i - 2) in self.self_attention:\n",
        "                h, attention_map \u003d self.self_attention[i - 2](h)\n",
        "                if attention_map is not None:\n",
        "                    all_attention_maps[i] \u003d attention_map\n",
        "        o \u003d self.linear(h).mean(dim\u003d2).squeeze()\n",
        "        if y is not None:\n",
        "            emb \u003d self.class_emb(y)\n",
        "            cond_loss \u003d (emb * h.squeeze()).sum(dim\u003d1)\n",
        "        else:\n",
        "            cond_loss \u003d 0.0\n",
        "        return o + cond_loss, h, all_attention_maps\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DMfsNdeVg-DF",
        "colab_type": "code",
        "colab": {},
        "pycharm": {}
      },
      "cell_type": "code",
      "source": [
        "#torch_utils.py\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "class Plugin(object):\n",
        "\n",
        "    def __init__(self, interval\u003dNone):\n",
        "        if interval is None:\n",
        "            interval \u003d []\n",
        "        self.trigger_interval \u003d interval\n",
        "\n",
        "    def register(self, trainer):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class Monitor(Plugin):\n",
        "\n",
        "    def __init__(self, running_average\u003dTrue, epoch_average\u003dTrue, smoothing\u003d0.7,\n",
        "                 precision\u003dNone, number_format\u003dNone, unit\u003d\u0027\u0027):\n",
        "        if precision is None:\n",
        "            precision \u003d 4\n",
        "        if number_format is None:\n",
        "            number_format \u003d \u0027.{}f\u0027.format(precision)\n",
        "        number_format \u003d \u0027:\u0027 + number_format\n",
        "        super(Monitor, self).__init__([(1, \u0027iteration\u0027), (1, \u0027epoch\u0027)])\n",
        "\n",
        "        self.smoothing \u003d smoothing\n",
        "        self.with_running_average \u003d running_average\n",
        "        self.with_epoch_average \u003d epoch_average\n",
        "\n",
        "        self.log_format \u003d number_format\n",
        "        self.log_unit \u003d unit\n",
        "        self.log_epoch_fields \u003d None\n",
        "        self.log_iter_fields \u003d [\u0027{last\u0027 + number_format + \u0027}\u0027 + unit]\n",
        "        if self.with_running_average:\n",
        "            self.log_iter_fields +\u003d [\u0027 ({running_avg\u0027 + number_format + \u0027}\u0027 + unit + \u0027)\u0027]\n",
        "        if self.with_epoch_average:\n",
        "            self.log_epoch_fields \u003d [\u0027{epoch_mean\u0027 + number_format + \u0027}\u0027 + unit]\n",
        "\n",
        "    def register(self, trainer):\n",
        "        self.trainer \u003d trainer\n",
        "        stats \u003d self.trainer.stats.setdefault(self.stat_name, {})\n",
        "        stats[\u0027log_format\u0027] \u003d self.log_format\n",
        "        stats[\u0027log_unit\u0027] \u003d self.log_unit\n",
        "        stats[\u0027log_iter_fields\u0027] \u003d self.log_iter_fields\n",
        "        if self.with_epoch_average:\n",
        "            stats[\u0027log_epoch_fields\u0027] \u003d self.log_epoch_fields\n",
        "        if self.with_epoch_average:\n",
        "            stats[\u0027epoch_stats\u0027] \u003d (0, 0)\n",
        "\n",
        "    def iteration(self, *args):\n",
        "        stats \u003d self.trainer.stats.setdefault(self.stat_name, {})\n",
        "        stats[\u0027last\u0027] \u003d self._get_value(*args)\n",
        "\n",
        "        if self.with_epoch_average:\n",
        "            stats[\u0027epoch_stats\u0027] \u003d tuple(sum(t) for t in\n",
        "                                         zip(stats[\u0027epoch_stats\u0027], (stats[\u0027last\u0027], 1)))\n",
        "\n",
        "        if self.with_running_average:\n",
        "            previous_avg \u003d stats.get(\u0027running_avg\u0027, 0)\n",
        "            stats[\u0027running_avg\u0027] \u003d previous_avg * self.smoothing + \\\n",
        "                                   stats[\u0027last\u0027] * (1 - self.smoothing)\n",
        "\n",
        "    def epoch(self, idx):\n",
        "        stats \u003d self.trainer.stats.setdefault(self.stat_name, {})\n",
        "        if self.with_epoch_average:\n",
        "            epoch_stats \u003d stats[\u0027epoch_stats\u0027]\n",
        "            stats[\u0027epoch_mean\u0027] \u003d epoch_stats[0] / epoch_stats[1]\n",
        "            stats[\u0027epoch_stats\u0027] \u003d (0, 0)\n",
        "\n",
        "\n",
        "class LossMonitor(Monitor):\n",
        "    stat_name \u003d \u0027loss\u0027\n",
        "\n",
        "    def _get_value(self, iteration, input, target, output, loss):\n",
        "        return loss.item()\n",
        "\n",
        "\n",
        "class Logger(Plugin):\n",
        "    alignment \u003d 4\n",
        "    separator \u003d \u0027#\u0027 * 80\n",
        "\n",
        "    def __init__(self, fields, interval\u003dNone):\n",
        "        if interval is None:\n",
        "            interval \u003d [(1, \u0027iteration\u0027), (1, \u0027epoch\u0027)]\n",
        "        super(Logger, self).__init__(interval)\n",
        "        self.field_widths \u003d defaultdict(lambda: defaultdict(int))\n",
        "        self.fields \u003d list(map(lambda f: f.split(\u0027.\u0027), fields))\n",
        "\n",
        "    def _join_results(self, results):\n",
        "        joined_out \u003d map(lambda i: (i[0], \u0027 \u0027.join(i[1])), results)\n",
        "        joined_fields \u003d map(lambda i: \u0027{}: {}\u0027.format(i[0], i[1]), joined_out)\n",
        "        return \u0027\\t\u0027.join(joined_fields)\n",
        "\n",
        "    def log(self, msg):\n",
        "        print(msg)\n",
        "\n",
        "    def register(self, trainer):\n",
        "        self.trainer \u003d trainer\n",
        "\n",
        "    def gather_stats(self):\n",
        "        result \u003d {}\n",
        "        return result\n",
        "\n",
        "    def _align_output(self, field_idx, output):\n",
        "        for output_idx, o in enumerate(output):\n",
        "            if len(o) \u003c self.field_widths[field_idx][output_idx]:\n",
        "                num_spaces \u003d self.field_widths[field_idx][output_idx] - len(o)\n",
        "                output[output_idx] +\u003d \u0027 \u0027 * num_spaces\n",
        "            else:\n",
        "                self.field_widths[field_idx][output_idx] \u003d len(o)\n",
        "\n",
        "    def _gather_outputs(self, field, log_fields, stat_parent, stat, require_dict\u003dFalse):\n",
        "        output \u003d []\n",
        "        name \u003d \u0027\u0027\n",
        "        if isinstance(stat, dict):\n",
        "            log_fields \u003d stat.get(log_fields, [])\n",
        "            name \u003d stat.get(\u0027log_name\u0027, \u0027.\u0027.join(field))\n",
        "            for f in log_fields:\n",
        "                output.append(f.format(**stat))\n",
        "        elif not require_dict:\n",
        "            name \u003d \u0027.\u0027.join(field)\n",
        "            number_format \u003d stat_parent.get(\u0027log_format\u0027, \u0027\u0027)\n",
        "            unit \u003d stat_parent.get(\u0027log_unit\u0027, \u0027\u0027)\n",
        "            fmt \u003d \u0027{\u0027 + number_format + \u0027}\u0027 + unit\n",
        "            output.append(fmt.format(stat))\n",
        "        return name, output\n",
        "\n",
        "    def _log_all(self, log_fields, prefix\u003dNone, suffix\u003dNone, require_dict\u003dFalse):\n",
        "        results \u003d []\n",
        "        for field_idx, field in enumerate(self.fields):\n",
        "            parent, stat \u003d None, self.trainer.stats\n",
        "            for f in field:\n",
        "                parent, stat \u003d stat, stat[f]\n",
        "            name, output \u003d self._gather_outputs(field, log_fields,\n",
        "                                                parent, stat, require_dict)\n",
        "            if not output:\n",
        "                continue\n",
        "            self._align_output(field_idx, output)\n",
        "            results.append((name, output))\n",
        "        if not results:\n",
        "            return\n",
        "        output \u003d self._join_results(results)\n",
        "        if prefix is not None:\n",
        "            self.log(prefix)\n",
        "        self.log(output)\n",
        "        if suffix is not None:\n",
        "            self.log(suffix)\n",
        "\n",
        "    def iteration(self, *args):\n",
        "        self._log_all(\u0027log_iter_fields\u0027)\n",
        "\n",
        "    def epoch(self, epoch_idx):\n",
        "        self._log_all(\u0027log_epoch_fields\u0027,\n",
        "                      prefix\u003dself.separator + \u0027\\nEpoch summary:\u0027,\n",
        "                      suffix\u003dself.separator,\n",
        "                      require_dict\u003dTrue)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c1WEwYCohH7d",
        "colab_type": "code",
        "colab": {},
        "pycharm": {}
      },
      "cell_type": "code",
      "source": [
        "#trainer.py\n",
        "import heapq\n",
        "\n",
        "\n",
        "class Trainer(object):\n",
        "    def __init__(self, discriminator: Discriminator, generator: Generator, d_loss, g_loss, dataset,\n",
        "                 random_latents_generator, resume_nimg, optimizer_g, optimizer_d, d_training_repeats: int \u003d 5,\n",
        "                 tick_kimg_default: float \u003d 5.0):\n",
        "        assert d_training_repeats \u003e\u003d 1\n",
        "        self.d_training_repeats \u003d d_training_repeats\n",
        "        self.discriminator \u003d discriminator\n",
        "        self.generator \u003d generator\n",
        "        self.d_loss \u003d d_loss\n",
        "        self.g_loss \u003d g_loss\n",
        "        self.dataset \u003d dataset\n",
        "        self.cur_nimg \u003d resume_nimg\n",
        "        self.random_latents_generator \u003d random_latents_generator\n",
        "        self.tick_start_nimg \u003d self.cur_nimg\n",
        "        self.tick_duration_nimg \u003d int(tick_kimg_default * 1000)\n",
        "        self.iterations \u003d 0\n",
        "        self.cur_tick \u003d 0\n",
        "        self.time \u003d 0\n",
        "        self.lr_scheduler_g \u003d None\n",
        "        self.lr_scheduler_d \u003d None\n",
        "        self.optimizer_g \u003d optimizer_g\n",
        "        self.optimizer_d \u003d optimizer_d\n",
        "        self.stats \u003d {\n",
        "            \u0027kimg_stat\u0027: {\u0027val\u0027: self.cur_nimg / 1000., \u0027log_epoch_fields\u0027: [\u0027{val:8.3f}\u0027], \u0027log_name\u0027: \u0027kimg\u0027},\n",
        "            \u0027tick_stat\u0027: {\u0027val\u0027: self.cur_tick, \u0027log_epoch_fields\u0027: [\u0027{val:5}\u0027], \u0027log_name\u0027: \u0027tick\u0027}\n",
        "        }\n",
        "        self.plugin_queues \u003d {\n",
        "            \u0027iteration\u0027: [],\n",
        "            \u0027epoch\u0027: [],  # this is tick\n",
        "            \u0027end\u0027: []\n",
        "        }\n",
        "\n",
        "    def register_plugin(self, plugin):\n",
        "        plugin.register(self)\n",
        "        intervals \u003d plugin.trigger_interval\n",
        "        if not isinstance(intervals, list):\n",
        "            intervals \u003d [intervals]\n",
        "        for (duration, unit) in intervals:\n",
        "            queue \u003d self.plugin_queues[unit]\n",
        "            queue.append((duration, len(queue), plugin))\n",
        "\n",
        "    def call_plugins(self, queue_name, time, *args):\n",
        "        args \u003d (time,) + args\n",
        "        queue \u003d self.plugin_queues[queue_name]\n",
        "        if len(queue) \u003d\u003d 0:\n",
        "            return\n",
        "        while queue[0][0] \u003c\u003d time:\n",
        "            plugin \u003d queue[0][2]\n",
        "            getattr(plugin, queue_name)(*args)\n",
        "            for trigger in plugin.trigger_interval:\n",
        "                if trigger[1] \u003d\u003d queue_name:\n",
        "                    interval \u003d trigger[0]\n",
        "            new_item \u003d (time + interval, queue[0][1], plugin)\n",
        "            heapq.heappushpop(queue, new_item)\n",
        "\n",
        "    def run(self, total_kimg\u003d1):\n",
        "        for q in self.plugin_queues.values():\n",
        "            heapq.heapify(q)\n",
        "        total_nimg \u003d int(total_kimg * 1000)\n",
        "        try:\n",
        "            while self.cur_nimg \u003c total_nimg:\n",
        "                self.train()\n",
        "                if self.cur_nimg \u003e\u003d self.tick_start_nimg + self.tick_duration_nimg or self.cur_nimg \u003e\u003d total_nimg:\n",
        "                    self.cur_tick +\u003d 1\n",
        "                    self.tick_start_nimg \u003d self.cur_nimg\n",
        "                    self.stats[\u0027kimg_stat\u0027][\u0027val\u0027] \u003d self.cur_nimg / 1000.\n",
        "                    self.stats[\u0027tick_stat\u0027][\u0027val\u0027] \u003d self.cur_tick\n",
        "                    self.call_plugins(\u0027epoch\u0027, self.cur_tick)\n",
        "        except KeyboardInterrupt:\n",
        "            return\n",
        "        self.call_plugins(\u0027end\u0027, 1)\n",
        "\n",
        "    def train(self):\n",
        "        if self.lr_scheduler_g is not None:\n",
        "            self.lr_scheduler_g.step(self.cur_nimg / self.d_training_repeats)\n",
        "        fake_latents_in \u003d cudize(next(self.random_latents_generator))\n",
        "        for i in range(self.d_training_repeats):\n",
        "            if self.lr_scheduler_d is not None:\n",
        "                self.lr_scheduler_d.step(self.cur_nimg)\n",
        "            real_images_expr \u003d cudize(next(self.dataiter))\n",
        "            self.cur_nimg +\u003d real_images_expr[\u0027x\u0027].size(0)\n",
        "            d_loss \u003d self.d_loss(self.discriminator, self.generator, real_images_expr, fake_latents_in)\n",
        "            d_loss.backward()\n",
        "            self.optimizer_d.step()\n",
        "            fake_latents_in \u003d cudize(next(self.random_latents_generator))\n",
        "        g_loss \u003d self.g_loss(self.discriminator, self.generator, real_images_expr, fake_latents_in)\n",
        "        g_loss.backward()\n",
        "        self.optimizer_g.step()\n",
        "        self.iterations +\u003d 1\n",
        "        self.call_plugins(\u0027iteration\u0027, self.iterations, *(g_loss, d_loss))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hbFwwtr8g3Ij",
        "colab_type": "code",
        "colab": {},
        "pycharm": {}
      },
      "cell_type": "code",
      "source": "#plugins.py\nimport gc\nimport os\nimport time\nfrom copy import deepcopy\nfrom datetime import timedelta\nfrom glob import glob\n\nimport matplotlib\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom imageio import imwrite\nfrom sklearn.utils.extmath import randomized_svd\n\nmatplotlib.use(\u0027Agg\u0027)\nimport matplotlib.pyplot as plt\n\n\nclass DepthManager(Plugin):\n    minibatch_override \u003d {0: 256, 1: 256, 2: 128, 3: 128, 4: 48, 5: 32,\n                          6: 32, 7: 32, 8: 16, 9: 16, 10: 8, 11: 8}\n\n    tick_kimg_override \u003d {4: 4, 5: 4, 6: 4, 7: 3, 8: 3, 9: 2, 10: 2, 11: 1}\n    training_kimg_override \u003d {1: 200, 2: 200, 3: 200, 4: 200}\n    transition_kimg_override \u003d {1: 200, 2: 200, 3: 200, 4: 200}\n\n    def __init__(self,  # everything starts from 0 or 1\n                 create_dataloader_fun, create_rlg, max_depth,\n                 tick_kimg_default, get_optimizer, default_lr,\n                 reset_optimizer: bool \u003d True, disable_progression\u003dFalse,\n                 minibatch_default\u003d256, depth_offset\u003d0,  # starts form 0\n                 lod_training_kimg\u003d400, lod_transition_kimg\u003d400):\n        super().__init__([(1, \u0027iteration\u0027)])\n        self.reset_optimizer \u003d reset_optimizer\n        self.minibatch_default \u003d minibatch_default\n        self.tick_kimg_default \u003d tick_kimg_default\n        self.create_dataloader_fun \u003d create_dataloader_fun\n        self.create_rlg \u003d create_rlg\n        self.trainer \u003d None\n        self.depth \u003d -1\n        self.alpha \u003d -1\n        self.get_optimizer \u003d get_optimizer\n        self.disable_progression \u003d disable_progression\n        self.depth_offset \u003d depth_offset\n        self.max_depth \u003d max_depth\n        self.default_lr \u003d default_lr\n        self.alpha_map \u003d self.pre_compute_alpha_map(self.depth_offset, max_depth, lod_training_kimg,\n                                                    self.training_kimg_override, lod_transition_kimg,\n                                                    self.transition_kimg_override)\n\n    def register(self, trainer):\n        self.trainer \u003d trainer\n        self.trainer.stats[\u0027minibatch_size\u0027] \u003d self.minibatch_default\n        self.trainer.stats[\u0027alpha\u0027] \u003d {\u0027log_name\u0027: \u0027alpha\u0027, \u0027log_epoch_fields\u0027: [\u0027{val:.2f}\u0027], \u0027val\u0027: self.alpha}\n        self.iteration(is_resuming\u003dself.trainer.optimizer_d is not None)\n\n    @staticmethod\n    def pre_compute_alpha_map(start_depth, max_depth, lod_training_kimg, lod_training_kimg_overrides,\n                              lod_transition_kimg, lod_transition_kimg_overrides):\n        points \u003d []\n        pointer \u003d 0\n        for i in range(start_depth, max_depth):\n            pointer +\u003d int(lod_training_kimg_overrides.get(i + 1, lod_training_kimg) * 1000)\n            points.append(pointer)\n            pointer +\u003d int(lod_transition_kimg_overrides.get(i + 1, lod_transition_kimg) * 1000)\n            points.append(pointer)\n        return points\n\n    def calc_progress(self, cur_nimg\u003dNone):\n        if cur_nimg is None:\n            cur_nimg \u003d self.trainer.cur_nimg\n        depth \u003d self.depth_offset\n        alpha \u003d 1.0\n        for i, point in enumerate(self.alpha_map):\n            if cur_nimg \u003d\u003d point:\n                break\n            if cur_nimg \u003e point and i % 2 \u003d\u003d 0:\n                depth +\u003d 1\n            if cur_nimg \u003c point and i % 2 \u003d\u003d 1:\n                alpha \u003d (cur_nimg - self.alpha_map[i - 1]) / (point - self.alpha_map[i - 1])\n                break\n            if cur_nimg \u003c point:\n                break\n        depth \u003d min(self.max_depth, depth)\n        if self.disable_progression:\n            depth \u003d self.max_depth\n            alpha \u003d 1.0\n        return depth, alpha\n\n    def iteration(self, is_resuming\u003dFalse, *args):\n        depth, alpha \u003d self.calc_progress()\n        dataset \u003d self.trainer.dataset\n        if depth !\u003d self.depth:\n            self.trainer.discriminator.depth \u003d self.trainer.generator.depth \u003d dataset.model_depth \u003d depth\n            self.depth \u003d depth\n            minibatch_size \u003d self.minibatch_override.get(depth - self.depth_offset, self.minibatch_default)\n            if self.reset_optimizer and not is_resuming:\n                self.trainer.optimizer_g, self.trainer.optimizer_d, self.trainer.lr_scheduler_g, self.trainer.lr_scheduler_d \u003d self.get_optimizer(\n                    self.minibatch_default * self.default_lr / minibatch_size)\n            self.data_loader \u003d self.create_dataloader_fun(minibatch_size)\n            self.trainer.dataiter \u003d iter(self.data_loader)\n            self.trainer.random_latents_generator \u003d self.create_rlg(minibatch_size)\n            tick_duration_kimg \u003d self.tick_kimg_override.get(depth - self.depth_offset, self.tick_kimg_default)\n            self.trainer.tick_duration_nimg \u003d int(tick_duration_kimg * 1000)\n            self.trainer.stats[\u0027minibatch_size\u0027] \u003d minibatch_size\n        if alpha !\u003d self.alpha:\n            self.trainer.discriminator.alpha \u003d self.trainer.generator.alpha \u003d dataset.alpha \u003d alpha\n            self.alpha \u003d alpha\n        self.trainer.stats[\u0027depth\u0027] \u003d depth\n        self.trainer.stats[\u0027alpha\u0027][\u0027val\u0027] \u003d alpha\n\n\nclass EfficientLossMonitor(LossMonitor):\n    def __init__(self, loss_no, stat_name, monitor_threshold: float \u003d 10.0,\n                 monitor_warmup: int \u003d 50, monitor_patience: int \u003d 5):\n        super().__init__()\n        self.loss_no \u003d loss_no\n        self.stat_name \u003d stat_name\n        self.threshold \u003d monitor_threshold\n        self.warmup \u003d monitor_warmup\n        self.patience \u003d monitor_patience\n        self.counter \u003d 0\n\n    def _get_value(self, iteration, *args):\n        val \u003d args[self.loss_no].item()\n        if val !\u003d val:\n            raise ValueError(\u0027loss value is NaN :((\u0027)\n        return val\n\n    def epoch(self, idx):\n        super().epoch(idx)\n        if idx \u003e self.warmup:\n            loss_value \u003d self.trainer.stats[self.stat_name][\u0027epoch_mean\u0027]\n            if abs(loss_value) \u003e self.threshold:\n                self.counter +\u003d 1\n                if self.counter \u003e self.patience:\n                    raise ValueError(\u0027loss value exceeded the threshold\u0027)\n            else:\n                self.counter \u003d 0\n\n\nclass AbsoluteTimeMonitor(Plugin):\n    def __init__(self):\n        super().__init__([(1, \u0027epoch\u0027)])\n        self.start_time \u003d time.time()\n        self.epoch_start \u003d self.start_time\n        self.start_nimg \u003d None\n        self.epoch_time \u003d 0\n\n    def register(self, trainer):\n        self.trainer \u003d trainer\n        self.start_nimg \u003d trainer.cur_nimg\n        self.trainer.stats[\u0027sec\u0027] \u003d {\u0027log_format\u0027: \u0027:.1f\u0027}\n\n    def epoch(self, epoch_index):\n        cur_time \u003d time.time()\n        tick_time \u003d cur_time - self.epoch_start\n        self.epoch_start \u003d cur_time\n        kimg_time \u003d tick_time / (self.trainer.cur_nimg - self.start_nimg) * 1000\n        self.start_nimg \u003d self.trainer.cur_nimg\n        self.trainer.stats[\u0027time\u0027] \u003d timedelta(seconds\u003dtime.time() - self.start_time)\n        self.trainer.stats[\u0027sec\u0027][\u0027tick\u0027] \u003d tick_time\n        self.trainer.stats[\u0027sec\u0027][\u0027kimg\u0027] \u003d kimg_time\n\n\nclass SaverPlugin(Plugin):\n    last_pattern \u003d \u0027network-snapshot-{}-{}.dat\u0027\n\n    def __init__(self, checkpoints_path, keep_old_checkpoints: bool \u003d True, network_snapshot_ticks: int \u003d 50):\n        super().__init__([(network_snapshot_ticks, \u0027epoch\u0027)])\n        self.checkpoints_path \u003d checkpoints_path\n        self.keep_old_checkpoints \u003d keep_old_checkpoints\n\n    def register(self, trainer: Trainer):\n        self.trainer \u003d trainer\n\n    def epoch(self, epoch_index):\n        if not self.keep_old_checkpoints:\n            self._clear(self.last_pattern.format(\u0027*\u0027, \u0027*\u0027))\n        dest \u003d os.path.join(self.checkpoints_path,\n                            self.last_pattern.format(\u0027{}\u0027, \u0027{:06}\u0027.format(self.trainer.cur_nimg // 1000)))\n        for model, optimizer, name in [(self.trainer.generator, self.trainer.optimizer_g, \u0027generator\u0027),\n                                       (self.trainer.discriminator, self.trainer.optimizer_d, \u0027discriminator\u0027)]:\n            torch.save({\u0027cur_nimg\u0027: self.trainer.cur_nimg, \u0027model\u0027: model.state_dict(),\n                        \u0027optimizer\u0027: optimizer.state_dict()}, dest.format(name))\n\n    def end(self, *args):\n        self.epoch(*args)\n\n    def _clear(self, pattern):\n        pattern \u003d os.path.join(self.checkpoints_path, pattern)\n        for file_name in glob(pattern):\n            os.remove(file_name)\n\n\nclass EvalDiscriminator(Plugin):\n    def __init__(self, create_dataloader_fun, output_snapshot_ticks):\n        super().__init__([(1, \u0027epoch\u0027)])\n        self.create_dataloader_fun \u003d create_dataloader_fun\n        self.output_snapshot_ticks \u003d output_snapshot_ticks\n\n    def register(self, trainer):\n        self.trainer \u003d trainer\n        self.trainer.stats[\u0027memorization\u0027] \u003d {\n            \u0027log_name\u0027: \u0027memorization\u0027,\n            \u0027log_epoch_fields\u0027: [\u0027{val:.2f}\u0027, \u0027{epoch:.2f}\u0027],\n            \u0027val\u0027: float(\u0027nan\u0027), \u0027epoch\u0027: 0,\n        }\n\n    def epoch(self, epoch_index):\n        if epoch_index % self.output_snapshot_ticks !\u003d 0:\n            return\n        values \u003d []\n        with torch.no_grad():\n            i \u003d 0\n            for data in self.create_dataloader_fun(min(self.trainer.stats[\u0027minibatch_size\u0027], 1024), False,\n                                                   self.trainer.dataset.model_depth, self.trainer.dataset.alpha):\n                d_real, _, _ \u003d self.trainer.discriminator(cudize(data))\n                values.append(d_real.mean().item())\n                i +\u003d 1\n        values \u003d np.array(values).mean()\n        self.trainer.stats[\u0027memorization\u0027][\u0027val\u0027] \u003d values\n        self.trainer.stats[\u0027memorization\u0027][\u0027epoch\u0027] \u003d epoch_index\n\n\n# TODO\nclass OutputGenerator(Plugin):\n\n    def __init__(self, sample_fn, checkpoints_dir: str, seq_len: int, max_freq: float,\n                 samples_count: int \u003d 8, output_snapshot_ticks: int \u003d 25, old_weight: float \u003d 0.59):\n        super().__init__([(1, \u0027epoch\u0027)])\n        self.old_weight \u003d old_weight\n        self.sample_fn \u003d sample_fn\n        self.samples_count \u003d samples_count\n        self.checkpoints_dir \u003d checkpoints_dir\n        self.seq_len \u003d seq_len\n        self.max_freq \u003d max_freq\n        self.my_g_clone \u003d None\n        self.output_snapshot_ticks \u003d output_snapshot_ticks\n\n    @staticmethod\n    def flatten_params(model):\n        return deepcopy(list(p.data for p in model.parameters()))\n\n    @staticmethod\n    def load_params(flattened, model):\n        for p, avg_p in zip(model.parameters(), flattened):\n            p.data.copy_(avg_p)\n\n    def register(self, trainer):\n        self.trainer \u003d trainer\n        self.my_g_clone \u003d self.flatten_params(self.trainer.generator)\n\n    @staticmethod\n    def running_mean(x, n\u003d8):\n        return pd.Series(x).rolling(window\u003dn).mean().values\n\n    @staticmethod\n    def get_images(frequency, epoch, generated, my_range\u003drange):\n        num_channels \u003d generated.shape[1]\n        seq_len \u003d generated.shape[2]\n        t \u003d np.linspace(0, seq_len / frequency, seq_len)\n        f \u003d np.fft.rfftfreq(seq_len, d\u003d1. / frequency)\n        images \u003d []\n        for index in my_range(len(generated)):\n            fig, (axs) \u003d plt.subplots(num_channels, 4)\n            if num_channels \u003d\u003d 1:\n                axs \u003d axs.reshape(1, -1)\n            fig.set_figheight(40)\n            fig.set_figwidth(40)\n            for ch in range(num_channels):\n                data \u003d generated[index, ch, :]\n                axs[ch][0].plot(t, data, color\u003d(0.8, 0, 0, 0.5), label\u003d\u0027time domain\u0027)\n                axs[ch][1].plot(f, np.abs(np.fft.rfft(data)), color\u003d(0.8, 0, 0, 0.5), label\u003d\u0027freq domain\u0027)\n                axs[ch][2].plot(f, OutputGenerator.running_mean(np.abs(np.fft.rfft(data))),\n                                color\u003d(0.8, 0, 0, 0.5), label\u003d\u0027freq domain(smooth)\u0027)\n                axs[ch][3].semilogy(f, np.abs(np.fft.rfft(data)), color\u003d(0.8, 0, 0, 0.5), label\u003d\u0027freq domain(log)\u0027)\n                axs[ch][0].set_ylim([-1.1, 1.1])\n                axs[ch][0].legend()\n                axs[ch][1].legend()\n                axs[ch][2].legend()\n                axs[ch][3].legend()\n            fig.suptitle(\u0027epoch: {}, sample: {}\u0027.format(epoch, index))\n            fig.canvas.draw()\n            image \u003d np.frombuffer(fig.canvas.tostring_rgb(), dtype\u003dnp.uint8)\n            image \u003d image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n            images.append(image)\n            plt.close(fig)\n        return images\n\n    def epoch(self, epoch_index):\n        for p, avg_p in zip(self.trainer.generator.parameters(), self.my_g_clone):\n            avg_p.mul_(self.old_weight).add_((1.0 - self.old_weight) * p.data)\n        if epoch_index % self.output_snapshot_ticks \u003d\u003d 0:\n            z \u003d next(self.sample_fn(self.samples_count))\n            gen_input \u003d cudize(z)\n            original_param \u003d self.flatten_params(self.trainer.generator)\n            self.load_params(self.my_g_clone, self.trainer.generator)\n            dest \u003d os.path.join(self.checkpoints_dir, SaverPlugin.last_pattern.format(\u0027smooth_generator\u0027,\n                                                                                      \u0027{:06}\u0027.format(\n                                                                                          self.trainer.cur_nimg // 1000)))\n            torch.save({\u0027cur_nimg\u0027: self.trainer.cur_nimg, \u0027model\u0027: self.trainer.generator.state_dict()}, dest)\n            out \u003d generate_samples(self.trainer.generator, gen_input)\n            self.load_params(original_param, self.trainer.generator)\n            frequency \u003d self.max_freq * out.shape[2] / self.seq_len\n            images \u003d self.get_images(frequency, epoch_index, out)\n            for i, image in enumerate(images):\n                imwrite(os.path.join(self.checkpoints_dir, \u0027{}_{}.png\u0027.format(epoch_index, i)), image)\n\n\nclass TeeLogger(Logger):\n\n    def __init__(self, log_file, exp_name, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.log_file \u003d open(log_file, \u0027a\u0027, 1)\n        self.exp_name \u003d exp_name\n\n    def log(self, msg):\n        print(self.exp_name, msg, flush\u003dTrue)\n        self.log_file.write(msg + \u0027\\n\u0027)\n\n    def epoch(self, epoch_idx):\n        self._log_all(\u0027log_epoch_fields\u0027)\n\n\nclass WatchSingularValues(Plugin):\n    def __init__(self, network, one_divided_two: float \u003d 10.0, output_snapshot_ticks: int \u003d 20):\n        super().__init__([(1, \u0027epoch\u0027)])\n        self.network \u003d network\n        self.one_divided_two \u003d one_divided_two\n        self.output_snapshot_ticks \u003d output_snapshot_ticks\n\n    def register(self, trainer):\n        self.trainer \u003d trainer\n\n    def epoch(self, epoch_index):\n        if epoch_index % self.output_snapshot_ticks !\u003d 0:\n            return\n        for module in self.network.modules:\n            if isinstance(module, torch.nn.Conv1d):\n                weight \u003d module.weight.data.cpu().numpy()\n                _, s, _ \u003d randomized_svd(weight.reshape(weight.shape[0], -1), n_components\u003d2)\n                if abs(s[0] / s[1]) \u003e self.one_divided_two:\n                    raise ValueError(module)\n\n\nclass SlicedWDistance(Plugin):\n    def __init__(self, progression_scale: int, output_snapshot_ticks: int, patches_per_item: int \u003d 16,\n                 patch_size: int \u003d 49, max_items: int \u003d 1024, number_of_projections: int \u003d 512,\n                 dir_repeats: int \u003d 4, dirs_per_repeat: int \u003d 128):\n        super().__init__([(1, \u0027epoch\u0027)])\n        self.output_snapshot_ticks \u003d output_snapshot_ticks\n        self.progression_scale \u003d progression_scale\n        self.patches_per_item \u003d patches_per_item\n        self.patch_size \u003d patch_size\n        self.max_items \u003d max_items\n        self.number_of_projections \u003d number_of_projections\n        self.dir_repeats \u003d dir_repeats\n        self.dirs_per_repeat \u003d dirs_per_repeat\n\n    def register(self, trainer):\n        self.trainer \u003d trainer\n        self.trainer.stats[\u0027swd\u0027] \u003d {\n            \u0027log_name\u0027: \u0027swd\u0027,\n            \u0027log_epoch_fields\u0027: [\u0027{val:.2f}\u0027, \u0027{epoch:.2f}\u0027],\n            \u0027val\u0027: float(\u0027nan\u0027), \u0027epoch\u0027: 0,\n        }\n\n    def sliced_wasserstein(self, A, B):\n        results \u003d []\n        for repeat in range(self.dir_repeats):\n            dirs \u003d torch.randn(A.shape[1], self.dirs_per_repeat)  # (descriptor_component, direction)\n            dirs /\u003d torch.sqrt(\n                (dirs * dirs).sum(dim\u003d0, keepdim\u003dTrue) + EPSILON)  # normalize descriptor components for each direction\n            projA \u003d torch.matmul(A, dirs)  # (neighborhood, direction)\n            projB \u003d torch.matmul(B, dirs)\n            projA \u003d torch.sort(projA, dim\u003d0)[0]  # sort neighborhood projections for each direction\n            projB \u003d torch.sort(projB, dim\u003d0)[0]\n            dists \u003d (projA - projB).abs()  # pointwise wasserstein distances\n            results.append(dists.mean())  # average over neighborhoods and directions\n        return torch.mean(torch.stack(results)).item()  # average over repeats\n\n    def epoch(self, epoch_index):\n        if epoch_index % self.output_snapshot_ticks !\u003d 0:\n            return\n        gc.collect()\n        all_fakes \u003d []\n        all_reals \u003d []\n        with torch.no_grad():\n            remaining_items \u003d self.max_items\n            while remaining_items \u003e 0:\n                z \u003d next(self.trainer.random_latents_generator)\n                fake_latents_in \u003d cudize(z)\n                all_fakes.append(self.trainer.generator(fake_latents_in)[0][\u0027x\u0027].data.cpu())\n                if all_fakes[-1].size(2) \u003c self.patch_size:\n                    break\n                remaining_items -\u003d all_fakes[-1].size(0)\n            all_fakes \u003d torch.cat(all_fakes, dim\u003d0)\n            remaining_items \u003d self.max_items\n            while remaining_items \u003e 0:\n                all_reals.append(next(self.trainer.dataiter)[\u0027x\u0027])\n                if all_reals[-1].size(2) \u003c self.patch_size:\n                    break\n                remaining_items -\u003d all_reals[-1].size(0)\n            all_reals \u003d torch.cat(all_reals, dim\u003d0)\n        swd \u003d self.get_descriptors(all_fakes, all_reals)\n        if len(swd) \u003e 0:\n            swd.append(np.array(swd).mean())\n        self.trainer.stats[\u0027swd\u0027][\u0027val\u0027] \u003d swd\n        self.trainer.stats[\u0027swd\u0027][\u0027epoch\u0027] \u003d epoch_index\n\n    def get_descriptors(self, batch1, batch2):\n        b, c, t_max \u003d batch1.shape\n        t \u003d t_max\n        num_levels \u003d 0\n        while t \u003e\u003d self.patch_size:\n            num_levels +\u003d 1\n            t //\u003d self.progression_scale\n        swd \u003d []\n        for level in range(num_levels):\n            both_descriptors \u003d [None, None]\n            batchs \u003d [batch1, batch2]\n            for i in range(2):\n                descriptors \u003d []\n                max_index \u003d batchs[i].shape[2] - self.patch_size\n                for j in range(b):\n                    for k in range(self.patches_per_item):\n                        rand_index \u003d np.random.randint(0, max_index)\n                        descriptors.append(batchs[i][j, :, rand_index:rand_index + self.patch_size])\n                descriptors \u003d torch.stack(descriptors, dim\u003d0)  # N, c, patch_size\n                descriptors \u003d descriptors.reshape((-1, c))\n                descriptors -\u003d torch.mean(descriptors, dim\u003d0, keepdim\u003dTrue)\n                descriptors /\u003d torch.std(descriptors, dim\u003d0, keepdim\u003dTrue) + EPSILON\n                both_descriptors[i] \u003d descriptors\n                batchs[i] \u003d batchs[i][:, :, ::self.progression_scale]\n            swd.append(self.sliced_wasserstein(both_descriptors[0], both_descriptors[1]))\n        return swd\n",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a9Czff_yhYz8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "outputId": "dbfd9104-15f7-49ac-efaf-d245d0b25ece",
        "pycharm": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import signal\n",
        "import subprocess\n",
        "import time\n",
        "from functools import partial\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import yaml\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "default_params \u003d dict(\n",
        "    result_dir\u003d\u0027results\u0027,\n",
        "    exp_name\u003d\u0027\u0027,\n",
        "    lr\u003d0.001,  # generator\u0027s learning rate\n",
        "    total_kimg\u003d6000,\n",
        "    resume_network\u003d\u0027\u0027,  # 001-test/network-snapshot-{}-000025.dat\n",
        "    num_data_workers\u003d0,\n",
        "    random_seed\u003d1373,\n",
        "    grad_lambda\u003d10.0,  # must set it to zero to disable gp loss (even for non wgan based losses)\n",
        "    iwass_drift_epsilon\u003d0.001,\n",
        "    iwass_target\u003d1.0,\n",
        "    feature_matching_lambda\u003d0.0,\n",
        "    loss_type\u003d\u0027wgan_gp\u0027,  # wgan_gp, hinge, wgan_theirs, rsgan, rasgan, rahinge\n",
        "    cuda_device\u003d0,\n",
        "    ttur\u003dFalse,\n",
        "    config_file\u003dNone,\n",
        "    fmap_base\u003d1024,\n",
        "    fmap_max\u003d256,\n",
        "    fmap_min\u003d64,\n",
        "    equalized\u003dTrue,\n",
        "    kernel_size\u003d3,\n",
        "    self_attention_layers\u003d[],  # starts from 0 or null (for G it means putting it after ith layer)\n",
        "    random_multiply\u003dFalse,\n",
        "    lr_rampup_kimg\u003d0.0,  # set to 0 to disable (used to be 40)\n",
        "    z_distribution\u003d\u0027normal\u0027,  # or \u0027bernoulli\u0027 or \u0027censored\u0027\n",
        "    init\u003d\u0027kaiming_normal\u0027,  # or xavier_uniform or orthogonal\n",
        "    act_alpha\u003d0.2,\n",
        "    residual\u003dFalse,\n",
        "    calc_swd\u003dFalse,\n",
        "    separable\u003dFalse,\n",
        "    num_classes\u003d0,\n",
        "    deep\u003dFalse\n",
        ")\n",
        "\n",
        "\n",
        "class InfiniteRandomSampler(SubsetRandomSampler):\n",
        "    def __iter__(self):\n",
        "        while True:\n",
        "            it \u003d super().__iter__()\n",
        "            for x in it:\n",
        "                yield x\n",
        "\n",
        "\n",
        "def load_models(resume_network, result_dir, logger):\n",
        "    logger.log(\u0027Resuming {}\u0027.format(resume_network))\n",
        "    dest \u003d os.path.join(result_dir, resume_network)\n",
        "    generator, g_optimizer, g_cur_img \u003d load_model(dest.format(\u0027generator\u0027), True)\n",
        "    discriminator, d_optimizer, d_cur_img \u003d load_model(dest.format(\u0027discriminator\u0027), True)\n",
        "    assert g_cur_img \u003d\u003d d_cur_img\n",
        "    return generator, g_optimizer, discriminator, d_optimizer, g_cur_img\n",
        "\n",
        "\n",
        "def thread_exit(_signal, frame):\n",
        "    exit(0)\n",
        "\n",
        "\n",
        "def worker_init(x):\n",
        "    signal.signal(signal.SIGINT, thread_exit)\n",
        "\n",
        "\n",
        "def main(params):\n",
        "    dataset_params \u003d params[\u0027EEGDataset\u0027]\n",
        "    dataset, val_dataset \u003d EEGDataset.from_config(**dataset_params)\n",
        "    if params[\u0027config_file\u0027] and params[\u0027exp_name\u0027] \u003d\u003d \u0027\u0027:\n",
        "        params[\u0027exp_name\u0027] \u003d params[\u0027config_file\u0027].split(\u0027/\u0027)[-1].split(\u0027.\u0027)[0]\n",
        "    result_dir \u003d create_result_subdir(params[\u0027result_dir\u0027], params[\u0027exp_name\u0027])\n",
        "\n",
        "    losses \u003d [\u0027G_loss\u0027, \u0027D_loss\u0027]\n",
        "    stats_to_log \u003d [\u0027tick_stat\u0027, \u0027kimg_stat\u0027]\n",
        "    stats_to_log.extend([\u0027depth\u0027, \u0027alpha\u0027, \u0027minibatch_size\u0027])\n",
        "    stats_to_log.extend([\u0027time\u0027, \u0027sec.tick\u0027, \u0027sec.kimg\u0027] + losses)\n",
        "    if dataset_params[\u0027validation_ratio\u0027] \u003e 0:\n",
        "        stats_to_log.extend([\u0027memorization.val\u0027, \u0027memorization.epoch\u0027])\n",
        "    if params[\u0027calc_swd\u0027]:\n",
        "        stats_to_log.extend([\u0027swd.val\u0027, \u0027swd.epoch\u0027])\n",
        "\n",
        "    logger \u003d TeeLogger(os.path.join(result_dir, \u0027log.txt\u0027), params[\u0027exp_name\u0027], stats_to_log, [(1, \u0027epoch\u0027)])\n",
        "    shared_model_params \u003d dict(initial_kernel_size\u003ddataset.initial_kernel_size, num_rgb_channels\u003ddataset.num_channels,\n",
        "                               fmap_base\u003dparams[\u0027fmap_base\u0027], fmap_max\u003dparams[\u0027fmap_max\u0027], fmap_min\u003dparams[\u0027fmap_min\u0027],\n",
        "                               kernel_size\u003dparams[\u0027kernel_size\u0027], self_attention_layers\u003dparams[\u0027self_attention_layers\u0027],\n",
        "                               progression_scale_up\u003ddataset.progression_scale_up,\n",
        "                               progression_scale_down\u003ddataset.progression_scale_down, residual\u003dparams[\u0027residual\u0027],\n",
        "                               separable\u003dparams[\u0027separable\u0027], equalized\u003dparams[\u0027equalized\u0027], init\u003dparams[\u0027init\u0027],\n",
        "                               act_alpha\u003dparams[\u0027act_alpha\u0027], num_classes\u003dparams[\u0027num_classes\u0027], deep\u003dparams[\u0027deep\u0027])\n",
        "    for n in (\u0027Generator\u0027, \u0027Discriminator\u0027):\n",
        "        p \u003d params[n]\n",
        "        if p[\u0027spectral\u0027]:\n",
        "            if p[\u0027act_norm\u0027] \u003d\u003d \u0027pixel\u0027:\n",
        "                logger.log(\u0027Warning, setting pixel normalization with spectral norm in {} is not a good idea\u0027.format(n))\n",
        "            if params[\u0027equalized\u0027]:\n",
        "                logger.log(\u0027Warning, setting equalized weights with spectral norm in {} is not a good idea\u0027.format(n))\n",
        "    if params[\u0027DepthManager\u0027][\u0027disable_progression\u0027] and not params[\u0027residual\u0027]:\n",
        "        logger.log(\u0027Warning, you have set the residual to false and disabled the progression\u0027)\n",
        "    if params[\u0027Discriminator\u0027][\u0027act_norm\u0027] is not None:\n",
        "        logger.log(\u0027Warning, you are using an activation normalization in discriminator\u0027)\n",
        "    generator \u003d Generator(**shared_model_params, z_distribution\u003dparams[\u0027z_distribution\u0027], **params[\u0027Generator\u0027])\n",
        "    discriminator \u003d Discriminator(**shared_model_params, **params[\u0027Discriminator\u0027])\n",
        "\n",
        "    def rampup(cur_nimg):\n",
        "        if cur_nimg \u003c params[\u0027lr_rampup_kimg\u0027] * 1000:\n",
        "            p \u003d max(0.0, 1 - cur_nimg / (params[\u0027lr_rampup_kimg\u0027] * 1000))\n",
        "            return np.exp(-p * p * 5.0)\n",
        "        else:\n",
        "            return 1.0\n",
        "\n",
        "    if params[\u0027ttur\u0027]:\n",
        "        params[\u0027Adam\u0027][\u0027betas\u0027] \u003d (0, 0.9)\n",
        "\n",
        "    def get_optimizers(g_lr):\n",
        "        d_lr \u003d g_lr\n",
        "        if params[\u0027ttur\u0027]:\n",
        "            d_lr *\u003d 4.0\n",
        "        opt_g \u003d Adam(trainable_params(generator), g_lr, **params[\u0027Adam\u0027])\n",
        "        opt_d \u003d Adam(trainable_params(discriminator), d_lr, **params[\u0027Adam\u0027])\n",
        "        if params[\u0027lr_rampup_kimg\u0027] \u003e 0:\n",
        "            lr_scheduler_g \u003d LambdaLR(opt_g, rampup, -1)\n",
        "            lr_scheduler_d \u003d LambdaLR(opt_d, rampup, -1)\n",
        "            return opt_g, opt_d, lr_scheduler_g, lr_scheduler_d\n",
        "        return opt_g, opt_d, None, None\n",
        "\n",
        "    if params[\u0027resume_network\u0027] !\u003d \u0027\u0027:\n",
        "        logger.log(\u0027resuming networks\u0027)\n",
        "        generator_state, opt_g_state, discriminator_state, opt_d_state, train_cur_img \u003d load_models(\n",
        "            params[\u0027resume_network\u0027], params[\u0027result_dir\u0027], logger)\n",
        "        generator.load_state_dict(generator_state)\n",
        "        discriminator.load_state_dict(discriminator_state)\n",
        "        opt_g, opt_d, _, _ \u003d get_optimizers(params[\u0027lr\u0027])\n",
        "        opt_g.load_state_dict(opt_g_state)\n",
        "        opt_d.load_state_dict(opt_d_state)\n",
        "    else:\n",
        "        opt_g \u003d None\n",
        "        opt_d \u003d None\n",
        "        train_cur_img \u003d 0\n",
        "    latent_size \u003d generator.input_latent_size\n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "    generator \u003d cudize(generator)\n",
        "    discriminator \u003d cudize(discriminator)\n",
        "    if opt_g is not None:\n",
        "        for opt in [opt_g, opt_d]:\n",
        "            for state in opt.state.values():\n",
        "                for k, v in state.items():\n",
        "                    if torch.is_tensor(v):\n",
        "                        state[k] \u003d cudize(v)\n",
        "    d_loss_fun \u003d partial(discriminator_loss, loss_type\u003dparams[\u0027loss_type\u0027], iwass_target\u003dparams[\u0027iwass_target\u0027],\n",
        "                         iwass_drift_epsilon\u003dparams[\u0027iwass_drift_epsilon\u0027], grad_lambda\u003dparams[\u0027grad_lambda\u0027])\n",
        "    g_loss_fun \u003d partial(generator_loss, random_multiply\u003dparams[\u0027random_multiply\u0027], loss_type\u003dparams[\u0027loss_type\u0027],\n",
        "                         feature_matching_lambda\u003dparams[\u0027feature_matching_lambda\u0027])\n",
        "    max_depth \u003d generator.max_depth\n",
        "\n",
        "    logger.log(\u0027exp name: {}\u0027.format(params[\u0027exp_name\u0027]))\n",
        "    try:\n",
        "        logger.log(\u0027commit hash: {}\u0027.format(subprocess.check_output([\u0027git\u0027, \u0027describe\u0027, \u0027--always\u0027]).strip()))\n",
        "    except:\n",
        "        logger.log(\u0027current time: {}\u0027.format(time.time()))\n",
        "    logger.log(\u0027training dataset shape: {}\u0027.format(dataset.shape))\n",
        "    if dataset_params[\u0027validation_ratio\u0027] \u003e 0:\n",
        "        logger.log(\u0027val dataset shape: {}\u0027.format(val_dataset.shape))\n",
        "    logger.log(\u0027Total number of parameters in Generator: {}\u0027.format(num_params(generator)))\n",
        "    logger.log(\u0027Total number of parameters in Discriminator: {}\u0027.format(num_params(discriminator)))\n",
        "\n",
        "    mb_def \u003d params[\u0027DepthManager\u0027][\u0027minibatch_default\u0027]\n",
        "\n",
        "    collate_real \u003d get_collate_real(dataset.end_sampling_freq, dataset.seq_len)\n",
        "    collate_fake \u003d get_collate_fake(latent_size, params[\u0027z_distribution\u0027], collate_real)\n",
        "\n",
        "    def get_dataloader(minibatch_size, is_training\u003dTrue, depth\u003d0, alpha\u003d1, is_real\u003dTrue):\n",
        "        ds \u003d dataset if is_training else val_dataset\n",
        "        shared_dataloader_params \u003d {\u0027dataset\u0027: ds, \u0027batch_size\u0027: minibatch_size, \u0027drop_last\u0027: True,\n",
        "                                    \u0027worker_init_fn\u0027: worker_init, \u0027num_workers\u0027: params[\u0027num_data_workers\u0027],\n",
        "                                    \u0027collate_fn\u0027: collate_real if is_real else collate_fake}\n",
        "        if not is_training:\n",
        "            ds.model_depth \u003d depth\n",
        "            ds.alpha \u003d alpha\n",
        "            # NOTE you must drop last in order to be compatible with D.stats layer\n",
        "            return DataLoader(**shared_dataloader_params, shuffle\u003dTrue)\n",
        "        return DataLoader(**shared_dataloader_params, sampler\u003dInfiniteRandomSampler(list(range(len(ds)))))\n",
        "\n",
        "    # NOTE you can not put the if inside your function (a function should either return or yield)\n",
        "    def get_random_latents(minibatch_size, is_training\u003dTrue, depth\u003d0, alpha\u003d1):\n",
        "        while True:\n",
        "            yield {\u0027z\u0027: cudize(random_latents(minibatch_size, latent_size, params[\u0027z_distribution\u0027]))}\n",
        "\n",
        "    trainer \u003d Trainer(discriminator, generator, d_loss_fun, g_loss_fun, dataset, get_random_latents(mb_def),\n",
        "                      train_cur_img, opt_g, opt_d, **params[\u0027Trainer\u0027])\n",
        "    dm \u003d DepthManager(get_dataloader, get_random_latents, max_depth, params[\u0027Trainer\u0027][\u0027tick_kimg_default\u0027],\n",
        "                      get_optimizers, params[\u0027lr\u0027], **params[\u0027DepthManager\u0027])\n",
        "    trainer.register_plugin(dm)\n",
        "    for i, loss_name in enumerate(losses):\n",
        "        trainer.register_plugin(EfficientLossMonitor(i, loss_name, **params[\u0027EfficientLossMonitor\u0027]))\n",
        "    trainer.register_plugin(SaverPlugin(result_dir, **params[\u0027SaverPlugin\u0027]))\n",
        "    trainer.register_plugin(\n",
        "        OutputGenerator(lambda x: get_random_latents(x), result_dir, dataset.seq_len,\n",
        "                        dataset.end_sampling_freq, **params[\u0027OutputGenerator\u0027]))\n",
        "    if dataset_params[\u0027validation_ratio\u0027] \u003e 0:\n",
        "        trainer.register_plugin(EvalDiscriminator(get_dataloader, params[\u0027SaverPlugin\u0027][\u0027network_snapshot_ticks\u0027]))\n",
        "    if params[\u0027calc_swd\u0027]:\n",
        "        trainer.register_plugin(\n",
        "            SlicedWDistance(dataset.progression_scale, params[\u0027SaverPlugin\u0027][\u0027network_snapshot_ticks\u0027],\n",
        "                            **params[\u0027SlicedWDistance\u0027]))\n",
        "    trainer.register_plugin(AbsoluteTimeMonitor())\n",
        "    if params[\u0027Generator\u0027][\u0027spectral\u0027]:\n",
        "        trainer.register_plugin(WatchSingularValues(generator, **params[\u0027WatchSingularValues\u0027]))\n",
        "    if params[\u0027Discriminator\u0027][\u0027spectral\u0027]:\n",
        "        trainer.register_plugin(WatchSingularValues(discriminator, **params[\u0027WatchSingularValues\u0027]))\n",
        "    trainer.register_plugin(logger)\n",
        "    params[\u0027EEGDataset\u0027][\u0027progression_scale_up\u0027] \u003d dataset.progression_scale_up\n",
        "    params[\u0027EEGDataset\u0027][\u0027progression_scale_down\u0027] \u003d dataset.progression_scale_down\n",
        "    params[\u0027EEGDataset\u0027][\u0027picked_channels\u0027] \u003d dataset.picked_channels\n",
        "    params[\u0027DepthManager\u0027][\u0027minibatch_override\u0027] \u003d dm.minibatch_override\n",
        "    params[\u0027DepthManager\u0027][\u0027tick_kimg_override\u0027] \u003d dm.tick_kimg_override\n",
        "    params[\u0027DepthManager\u0027][\u0027training_kimg_override\u0027] \u003d dm.training_kimg_override\n",
        "    params[\u0027DepthManager\u0027][\u0027transition_kimg_override\u0027] \u003d dm.transition_kimg_override\n",
        "    yaml.dump(params, open(os.path.join(result_dir, \u0027conf.yml\u0027), \u0027w\u0027))\n",
        "    trainer.run(params[\u0027total_kimg\u0027])\n",
        "    del trainer\n",
        "\n",
        "\n",
        "if __name__ \u003d\u003d \"__main__\":\n",
        "    need_arg_classes \u003d [Trainer, Generator, Discriminator, Adam, OutputGenerator, DepthManager, SaverPlugin,\n",
        "                        SlicedWDistance, EfficientLossMonitor, EvalDiscriminator, EEGDataset, WatchSingularValues]\n",
        "    main(parse_config(default_params, need_arg_classes, True, False))\n",
        "    print(\u0027training finished!\u0027)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading train dataset from file\n",
            "loading val dataset from file\n",
            " exp name: \n",
            " current time: 1554705972.6557548\n",
            " training dataset shape: (9501, 5, 1920)\n",
            " val dataset shape: (1155, 5, 1920)\n",
            " Total number of parameters in Generator: 2976360\n",
            " Total number of parameters in Discriminator: 2978497\n",
            " tick:     1\tkimg:    5.120\tdepth: 0\talpha: 1.00\tminibatch_size: 256\ttime: 0:00:12.304734\tsec.tick: 12.3\tsec.kimg: 2.4\tG_loss: 0.0427\tD_loss: 8.7106\tmemorization.val: nan\tmemorization.epoch: 0\n",
            " tick:     2\tkimg:   10.240\tdepth: 0\talpha: 1.00\tminibatch_size: 256\ttime: 0:00:18.866216\tsec.tick: 6.6 \tsec.kimg: 1.3\tG_loss: 0.1024\tD_loss: 6.8374\tmemorization.val: nan\tmemorization.epoch: 0\n",
            " tick:     3\tkimg:   15.360\tdepth: 0\talpha: 1.00\tminibatch_size: 256\ttime: 0:00:25.456018\tsec.tick: 6.6 \tsec.kimg: 1.3\tG_loss: 0.1803\tD_loss: 4.3948\tmemorization.val: nan\tmemorization.epoch: 0\n",
            " tick:     4\tkimg:   20.480\tdepth: 0\talpha: 1.00\tminibatch_size: 256\ttime: 0:00:32.020246\tsec.tick: 6.6 \tsec.kimg: 1.3\tG_loss: 0.3283\tD_loss: 2.3556\tmemorization.val: nan\tmemorization.epoch: 0\n",
            " tick:     5\tkimg:   25.600\tdepth: 0\talpha: 1.00\tminibatch_size: 256\ttime: 0:00:38.607832\tsec.tick: 6.6 \tsec.kimg: 1.3\tG_loss: 0.5276\tD_loss: 1.9203\tmemorization.val: nan\tmemorization.epoch: 0\n",
            " tick:     6\tkimg:   30.720\tdepth: 0\talpha: 1.00\tminibatch_size: 256\ttime: 0:00:45.175789\tsec.tick: 6.6 \tsec.kimg: 1.3\tG_loss: 0.6037\tD_loss: 1.2213\tmemorization.val: nan\tmemorization.epoch: 0\n",
            " tick:     7\tkimg:   35.840\tdepth: 0\talpha: 1.00\tminibatch_size: 256\ttime: 0:00:51.745298\tsec.tick: 6.6 \tsec.kimg: 1.3\tG_loss: 0.5661\tD_loss: 0.8560\tmemorization.val: nan\tmemorization.epoch: 0\n",
            " tick:     8\tkimg:   40.960\tdepth: 0\talpha: 1.00\tminibatch_size: 256\ttime: 0:00:58.305777\tsec.tick: 6.6 \tsec.kimg: 1.3\tG_loss: 1.1040\tD_loss: 0.9078\tmemorization.val: nan\tmemorization.epoch: 0\n",
            " tick:     9\tkimg:   46.080\tdepth: 0\talpha: 1.00\tminibatch_size: 256\ttime: 0:01:04.853075\tsec.tick: 6.5 \tsec.kimg: 1.3\tG_loss: 1.1420\tD_loss: 0.9825\tmemorization.val: nan\tmemorization.epoch: 0\n",
            " tick:    10\tkimg:   51.200\tdepth: 0\talpha: 1.00\tminibatch_size: 256\ttime: 0:01:11.427674\tsec.tick: 6.6 \tsec.kimg: 1.3\tG_loss: 1.2231\tD_loss: 0.8367\tmemorization.val: nan\tmemorization.epoch: 0\n",
            " tick:    11\tkimg:   56.320\tdepth: 0\talpha: 1.00\tminibatch_size: 256\ttime: 0:01:18.018552\tsec.tick: 6.6 \tsec.kimg: 1.3\tG_loss: 1.2851\tD_loss: 0.7418\tmemorization.val: nan\tmemorization.epoch: 0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}