# python train.py --exp_name xxx --config_file test.yml --load_dataset data/512.pkl --cuda_device 0 --test_run True

lr_rampup_kimg: 50
G_lr_max: 0.005
D_lr_max: 0.005
grad_lambda: 10.0
total_kimg: 32
iwass_epsilon: 0.001
loss_type: 'wgan_gp'  # wgan_gp, wgan_ct, hinge, wgan_theirs, wgan_theirs_ct, dcgan
LAMBDA_2: 2
LAMBDA_3: 1
test_run: true
optimizer: 'adam'  # adam, amsgrad, ttur
fmap_base: 128
fmap_max: 16
fmap_min: 8
equalized: true
kernel_size: 3
self_attention_layers: [2, 4] # starts from 0 or null (for G it means putting it after ith layer)
num_classes: 0
monitor_threshold: 10
random_multiply: false
only_one_conv: false
Trainer.D_training_repeats: 1
Trainer.tick_kimg_default: 1
Trainer.memory_friendly: false

Generator.latent_size: 16
Generator.normalize_latents: true
Generator.dropout: 0.1
Generator.do_mode: 'mul' # mul, drop, prop
Generator.param_norm: null # weight, spectral, null
Generator.act_norm: 'pixel' # pixel, layer, batch, null
Generator.is_morph: false
Generator.residual: true
Generator.extend_context_size: 5
Generator.use_extend_padding: true

Discriminator.do_mode: 'mul' # mul, drop, prop
Discriminator.dropout: 0.1
Discriminator.param_norm: null # weight, spectral, null
Discriminator.temporal_stats: false
Discriminator.num_stat_channels: 1
Discriminator.act_norm: null # batch, layer, pixel, null
Discriminator.calc_std: false
Discriminator.residual: true
Discriminator.window_from_first: false

DepthManager.depth_offset: 3 #starts form 0
DepthManager.attention_transition_kimg: 1
DepthManager.minibatch_default: 32
DepthManager.minibatch_overrides: {}
DepthManager.tick_kimg_overrides: {}
DepthManager.lod_training_kimg: 1
DepthManager.lod_training_kimg_overrides: {}
DepthManager.lod_transition_kimg: 1
DepthManager.lod_transition_kimg_overrides: {}
SaverPlugin.network_snapshot_ticks: 3
OutputGenerator.output_snapshot_ticks: 2

EEGDataset.seq_len: 512
EEGDataset.extra_factor: 1
EEGDataset.model_dataset_depth_offset: 2 # starts from 0
