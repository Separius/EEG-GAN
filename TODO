Resize => find the best upsampler, find the best down sampler
I/O => channel by channel output
SpecNorm => test on hinge, test again on wgan
ResidualBlock => Test

PhaseShift => make it work
Validation => use the result in saver and early stopping, add differentiable matlab features, call matlab functions?
SeqLen => play with depth manager's initial depth, play with ds'initial depth
Config => better config reader and also experiment manager and hyper param search, better test_mode(print everything)
Optimizer => what lr and scheduler, num D updates per G, ttur config(b1=0 and b2=0.9, d_lr = 0.0004 and g=d/4)
Sizes => batch size heuristics, feature number of G/D(force their equality), latent size
Training => training and transition size for each stage heuristics
Dropout => what dropout for D, how much dropout for D, what dropout for G(~50!), how much dropout for G
KernelSize => best kernel size for D/G
DataSet => increase to 16 channels, per_user or global, use_abs or max-min
MiscI => d(concatenated parts) == fake, mixed classifiers(abnormal / 6 classes), stats mmd(github.com/ratschlab/RGAN)
Stats => temporal or not, num of channels
Attention => add self attention to D, add self attention to G, where to add them?, has lambda, placing it at higher levels (more channels = lower temporal freq) is better
MiscII => connection network(restore Z from data ||z-C(D(G(z)))||), dp training(github.com/ratschlab/RGAN)
MiscIII => wavenet or samplernn + trained d as semantic loss, test same net on audio(4x the freq at each stage)
progressive dilated RNN or Wavenet/TCN ==> Z(2xFreq)|NewLayer|OldLayer|Fake ; Data(2xFreq)|NewLayer|OldLayer|Pred + AutoRegressive ; what is alpha in here? -> two things, at last layer(repeat vs get output of net) and at first layer, get z directly or from the new layer
Condition ==> on User Embedding or Easy meta-data(age, sex) or all ; ConditionalBatchNorm ; with SemiSupervised InfoGAN(github.com/spurra/ss-infogan) or AcGan(github.com/znxlwm/pytorch-generative-model-collections/blob/master/ACGAN.py) or Activation Maximization(github.com/ZhimingZhou/AM-GAN) or Class Splitting GAN(github.com/CIFASIS/splitting_gan) or Projection(yW . x)(github.com/pfnet-research/sngan_projection)
Recurrent => recurrent layers with a lipschitz regularizers(read the SN paper + orthogonal rnns)
InfiniteLength => G1(z)=o1,o2,o3,o4 so we can have a F(o1,o2,o3)=o4 and H(o2,o3,o4)=o1 + DropOut and also even VAE like arch we can have stochastisity ; or use the connection net to make another gan over sequences of Z! :D
