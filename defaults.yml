result_dir: 'results'
exp_name: ''
lr: 0.001
total_kimg: 6000
resume_network: ''  # 001-test/network-snapshot-{}-000025.dat
num_data_workers: 1
random_seed: 1373
grad_lambda: 10  # must set it to zero to disable gp loss (even for non wgan(10.0) based losses)
iwass_drift_epsilon: 0.001
iwass_target: 1.0
feature_matching_lambda: 0.0
loss_type: 'wgan_gp'  # wgan_gp, hinge, wgan_theirs, rsgan, rasgan, rahinge
#cuda_device: 0
ttur: false
#config_file: null
fmap_base: 1024
fmap_max: 256
fmap_min: 64
equalized: true
kernel_size: 3
self_attention_layers: []  # starts from 0 or null (for G it means putting it after ith layer)
num_classes: 0
random_multiply: false
lr_rampup_kimg: 0
validation_ratio: 0
z_distribution: 'normal' # or censored or bernoulli
init: 'kaiming_normal' # or xavier_uniform or orthogonal

Trainer:
  d_training_repeats: 1
  tick_kimg_default: 5.0
  resume_nimg: 0

Generator:
  to_rgb_mode: 'pggan' # or sngan or sagan
  act_alpha: 0.2
  latent_size: 256
  residual: false
  normalize_latents: true
  dropout: 0.2
  do_mode: 'mul' # or 'drop', 'prop'
  spectral: false
  act_norm: 'pixel' # or batch or layer or null
  no_tanh: false

Discriminator:
  sngan_rgb: false
  dropout: 0.2
  do_mode: 'mul' # or 'drop', 'prop'
  residual: false
  spectral: false
  act_norm: null # or batch or layer or pixel
  group_size: 4
  act_alpha: 0.2

DepthManager:
  reset_optimizer: true
  disable_progression: false
  depth_offset: 0  # starts form 0
  attention_transition_kimg: 400
  minibatch_default: 256
  # all overrides start from depth_offset+1
  minibatch_overrides: {4: 128, 5: 128, 6: 128, 7: 64, 8: 64, 9: 32, 10: 32, 11: 16, 12: 16}
  tick_kimg_overrides: {4: 4, 5: 4, 6: 4, 7: 3, 8: 3, 9: 2, 10: 2, 11: 1, 12: 1}
  lod_training_kimg: 400
  lod_training_kimg_overrides: {1: 200, 2: 200, 3: 200, 4: 200}
  lod_transition_kimg: 400
  lod_transition_kimg_overrides: {1: 200, 2: 200, 3: 200, 4: 200}

SaverPlugin:
  keep_old_checkpoints: true
  network_snapshot_ticks: 50

SlicedWDistance:
  patches_per_item: 16
  patch_size: 49
  number_of_batches: 128
  number_of_projections: 512
  dir_repeats: 4
  dirs_per_repeat: 128

OutputGenerator:
  samples_count: 8
  output_snapshot_ticks: 25

Adam:
  betas: !!python/tuple [0.0, 0.99]
  eps: 0.00000001
  weight_decay: 0

EEGDataset:
  dir_path: './data/prepared_eegs'
  seq_len: 512
  stride: 0.25
  num_channels: 5
  per_user_normalization: true
  dataset_freq: 80
  progression_scale: 2
  num_files: 12518
  per_channel_normalization: false
  model_dataset_depth_offset: 2

EfficientLossMonitor:
  monitor_threshold: 10.0
  monitor_warmup: 50
  monitor_patience: 5

EvalDiscriminator:
  output_snapshot_ticks: 25
